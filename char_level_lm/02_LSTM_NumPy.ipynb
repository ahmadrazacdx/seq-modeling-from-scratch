{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Character-Level LSTM Language Model**\n","\n","## **From Scratch Implementation in NumPy**\n","\n","This notebook implements a **vanilla LSTM** for character-level language modeling.\n","\n","\n","### **What We'll Build:**\n","- **Vanilla LSTM** with three (forget, input, output) gates\n","- **Backpropagation** for gradient computation\n","- **Character-level text generation** from learned patterns\n","---\n","*Notebook by*: Ahmad Raza [@ahmadrazacdx](https://github.com/ahmadrazacdx)<br>\n","*Date: 2025*  \n","*License: MIT*"],"metadata":{"id":"kv99qL16D9Kj"}},{"cell_type":"code","source":["import numpy as np\n","np.random.seed(42)"],"metadata":{"id":"SgSsJcK83DfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goJyoKz2zrmy","outputId":"71cd1ab9-bc56-4c96-c330-0895b75a7e92","executionInfo":{"status":"ok","timestamp":1763542319647,"user_tz":-300,"elapsed":18,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has 677 characters, 33 unique.\n"]}],"source":["#Load Data\n","data = open('../data/thirsty_crow.txt', 'r').read()\n","chars = sorted(list(set(data)))\n","data_size, vocab_size = len(data), len(chars)\n","print(f'Data has {data_size} characters, {vocab_size} unique.')"]},{"cell_type":"code","source":["char_to_ix = {ch:i for i,ch in enumerate(chars)}"],"metadata":{"id":"xc6oODzj3ijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ix_to_char = {i:ch for i,ch in enumerate(chars)}"],"metadata":{"id":"ClysqC-_3wNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __HYPER-PARAMETERS__"],"metadata":{"id":"JhOzRBXz414W"}},{"cell_type":"code","source":["lr = 1e-3 # learning rate\n","seq_len = 25 # times LSTM will be unrolled (Timesteps)\n","hidden_size= 100 # size of hidden units"],"metadata":{"id":"InWw7ZqC3xVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __MODEL PARAM INIT__\n","**Initializing weight matrices for the LSTM.**\n","\n","**Gate Weight Matrices (concatenated input $[\\mathbf{h}_{t-1}; \\mathbf{x}_t]$):**\n","- $\\mathbf{W}_f \\in \\mathbb{R}^{H \\times (H+V)}$ Forget gate weights\n","- $\\mathbf{W}_i \\in \\mathbb{R}^{H \\times (H+V)}$ Input gate weights\n","- $\\mathbf{W}_c \\in \\mathbb{R}^{H \\times (H+V)}$ Candidate cell state weights\n","- $\\mathbf{W}_o \\in \\mathbb{R}^{H \\times (H+V)}$ Output gate weights\n","\n","**Output Layer:**\n","- $\\mathbf{W}_y \\in \\mathbb{R}^{V \\times H}$ Hidden-to-output weights\n","\n","\n","**Biases:**\n","- Gate biases: $\\mathbf{b}_f, \\mathbf{b}_i, \\mathbf{b}_c, \\mathbf{b}_o \\in \\mathbb{R}^{H \\times 1}$\n","- Output bias:  $\\mathbf{b}_y \\in \\mathbb{R}^{V \\times 1}$\n","\n","**Total parameters:** $4H(H+V) + VH + 4H + V$ <br>\n","= **4×100×133 + 33×100 + 5×100 + 33 = $56,933$ parameters**\n","\n","**Where:**   $V$ = vocabulary size (33 unique chars), $H$ = hidden size (100)\n","\n","**Note:** A small but important implementation detail is that initializing the LSTM’s forget-gate bias to **1** helps prevent early forgetting and improves gradient flow. This recommendation was empirically emphasized by *Jozefowicz et al. (2015)*.\n"],"metadata":{"id":"5LRNhINY6CMp"}},{"cell_type":"code","source":["Wf = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Forget Gate weights (H, H+V)\n","Wi = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Input Gate weights (H, H+V)\n","Wc = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Candidate Cell State  weights (H, H+V)\n","Wo = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Output Gate weights (H, H+V)\n","Wy = np.random.randn(vocab_size, hidden_size)*0.01 # Prediction weights (V, H)\n","bf = np.ones((hidden_size, 1)) # Forget Gate bias (H, 1) -Jozefowicz et al (2015).\n","bi = np.zeros((hidden_size, 1)) # Input Gate bias(H, 1)\n","bo = np.zeros((hidden_size, 1)) # Output Gate bias (H, 1)\n","bc = np.zeros((hidden_size, 1)) # CCS bias (H, 1)\n","by = np.zeros((vocab_size, 1)) # prediction bias (V, 1)"],"metadata":{"id":"NPbbN7zi5jjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"\n","Wf: Forget Gate Weights  : {Wf.shape}\n","Wi: Input Gate Weights   : {Wi.shape}\n","Wc: CCS Weights          : {Wc.shape}\n","Wo: Output Gate Weights  : {Wo.shape}\n","Wy: Prediction Weights   : {Wy.shape}\n","bf: Forget Gate bias     : {bf.shape}\n","bi: Input Gate bias      : {bi.shape}\n","bc: CCS bias             : {bc.shape}\n","bo: Output Gate bias     : {bo.shape}\n","by: Prediction bias      : {by.shape}\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HGbspWj7RRE","outputId":"5ac037b6-2162-40f4-89e0-5db583610b3e","executionInfo":{"status":"ok","timestamp":1763542326394,"user_tz":-300,"elapsed":307,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Wf: Forget Gate Weights  : (100, 133)\n","Wi: Input Gate Weights   : (100, 133)\n","Wc: CCS Weights          : (100, 133)\n","Wo: Output Gate Weights  : (100, 133)\n","Wy: Prediction Weights   : (33, 100)\n","bf: Forget Gate bias     : (100, 1)\n","bi: Input Gate bias      : (100, 1)\n","bc: CCS bias             : (100, 1)\n","bo: Output Gate bias     : (100, 1)\n","by: Prediction bias      : (33, 1)\n","\n"]}]},{"cell_type":"markdown","source":["### __ADAM OPTIMIZER INITIALIZATION__"],"metadata":{"id":"xvWhMsGbeRjl"}},{"cell_type":"code","source":["# Adam hyperparameters\n","beta1 = 0.9\n","beta2 = 0.999\n","epsilon = 1e-8\n","\n","# Adam memory variables (first moment)\n","mWf = np.zeros_like(Wf)\n","mWi = np.zeros_like(Wi)\n","mWc = np.zeros_like(Wc)\n","mWo = np.zeros_like(Wo)\n","mWy = np.zeros_like(Wy)\n","mbf = np.zeros_like(bf)\n","mbi = np.zeros_like(bi)\n","mbc = np.zeros_like(bc)\n","mbo = np.zeros_like(bo)\n","mby = np.zeros_like(by)\n","\n","# Adam memory variables (second moment)\n","vWf = np.zeros_like(Wf)\n","vWi = np.zeros_like(Wi)\n","vWc = np.zeros_like(Wc)\n","vWo = np.zeros_like(Wo)\n","vWy = np.zeros_like(Wy)\n","vbf = np.zeros_like(bf)\n","vbi = np.zeros_like(bi)\n","vbc = np.zeros_like(bc)\n","vbo = np.zeros_like(bo)\n","vby = np.zeros_like(by)\n","# Timestep counter for bias correction\n","t_adam = 0"],"metadata":{"id":"tUZFfUeAePrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","def softmax(z):\n","    exp_z = np.exp(z - np.max(z))\n","    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"],"metadata":{"id":"ehegFzIZmOCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SINGLE LSTM CELL__\n","\n","**LSTM Forward Pass Equations:**\n","\n","$$\n","\\begin{align}\n","\\mathbf{z}_t &= [\\mathbf{h}_{t-1}; \\mathbf{x}_t] \\quad &\\text{(concatenated input)} \\\\\n","\\mathbf{f}_t &= \\sigma(\\mathbf{W}_f \\mathbf{z}_t + \\mathbf{b}_f) \\quad &\\text{(forget gate)} \\\\\n","\\mathbf{i}_t &= \\sigma(\\mathbf{W}_i \\mathbf{z}_t + \\mathbf{b}_i) \\quad &\\text{(input gate)} \\\\\n","\\tilde{\\mathbf{c}}_t &= \\tanh(\\mathbf{W}_c \\mathbf{z}_t + \\mathbf{b}_c) \\quad &\\text{(candidate cell state)} \\\\\n","\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t \\quad &\\text{(cell state update)} \\\\\n","\\mathbf{o}_t &= \\sigma(\\mathbf{W}_o \\mathbf{z}_t + \\mathbf{b}_o) \\quad &\\text{(output gate)} \\\\\n","\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t) \\quad &\\text{(hidden state)} \\\\\n","\\mathbf{y}_t &=\\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y \\quad &\\text{(output logits)}\n","\\end{align}\n","$$\n","\n","**Where:** $\\sigma$ = sigmoid, $\\odot$ = element-wise multiplication\n","\n","**Reference:** Hochreiter & Schmidhuber (1997) - *Long Short-Term Memory*"],"metadata":{"id":"VkDtomuF8LFZ"}},{"cell_type":"code","source":["def lstm(c_prev, h_prev, xt):\n","    \"\"\"\n","    Single LSTM cell\n","\n","    Inputs:\n","        - c_prev: Previous cell state (H, 1)\n","        - h_prev: Previous hidden state (H, 1)\n","        - xt: input charater (V, 1)\n","\n","    Returns:\n","        - ft, it, cct, ot: Gate activations\n","        - ct: Current cell state (H, 1)\n","        - ht: Current hidden state (H, 1)\n","        - yt: Output logits (V, 1)\n","    \"\"\"\n","    zt = np.concatenate((h_prev, xt), axis=0) # (H,1):(V,1)= (H+V,1)\n","    # Forget Gate\n","    zf = np.dot(Wf, zt) + bf #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    ft = sigmoid(zf) # (H,1)\n","    #Input Gate\n","    zi = np.dot(Wi, zt) + bi #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    it = sigmoid(zi) # (H,1)\n","    #Candidate Cell State\n","    zc = np.dot(Wc, zt) + bc #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    cct = np.tanh(zc) # (H,1)\n","    #Cell State\n","    ct = ft * c_prev + it * cct # (H,1)*(H,1)+(H,1)*(H,1)=(H,1)\n","    #Output Gate\n","    zo = np.dot(Wo, zt) + bo #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    ot = sigmoid(zo) # (H,1)\n","    #Hidden State\n","    ht = ot * np.tanh(ct) # (H,1)*(H,1)=(H,1)\n","    #Output Logits\n","    yt = np.dot(Wy, ht) + by # #(V,H)@(H,1)->(V,1)+(V,1)=(V,1)\n","    return ft, it, cct, ot, ct, ht, yt"],"metadata":{"id":"AVvRr4GJ7SmG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding the LSTM Cell:**\n","\n","**What's happening here?**\n","- At each timestep $t$, the LSTM takes three inputs:\n","  1. Current character $\\mathbf{x}_t$ (one-hot vector)\n","  2. Previous hidden state $\\mathbf{h}_{t-1}$\n","  3. Previous cell state $\\mathbf{c}_{t-1}$ (long-term memory)\n","  \n","- It outputs:\n","  1. New hidden state $\\mathbf{h}_t$ (short-term working memory)\n","  2. New cell state $\\mathbf{c}_t$ (long-term memory)\n","  3. Output logits $\\mathbf{y}_t$\n","  \n","**Key Advantage over Vanilla RNN:** The cell state $\\mathbf{c}_t$ provides a \"highway\" for gradients to flow backward without vanishing, enabling learning of long-range dependencies (100+ timesteps)!\n","\n","**The LSTM Magic - Three Gates:**\n","\n","1. **Forget gate** ($\\mathbf{f}_t$): Decides what to forget from $\\mathbf{c}_{t-1}$.\n","2. **Output gate** ($\\mathbf{o}_t$): Controls how much of the candidate cell state to add to $\\mathbf{c}_t$\n","3. **Input gate** ($\\mathbf{i}_t$): Decides what new information to store in $\\mathbf{c}_t$"],"metadata":{"id":"4f5wu1Tj9rbm"}},{"cell_type":"markdown","source":["### __FORWARD PASS__\n"],"metadata":{"id":"JSiqg1u1Fcuo"}},{"cell_type":"code","source":["def forward(inputs, targets, c_prev, h_prev):\n","    \"\"\"\n","    Forward pass through LSTM Cell\n","\n","    Inputs:\n","        - inputs: List of character indices, e.g., [5, 8, 12, 12, 15] for \"hello\"\n","        - targets: List of target character indices (inputs shifted by 1)\n","        - c_prev: Initial cell state from previous sequence, shape (H, 1)\n","        - h_prev: Initial hidden state from previous sequence, shape (H, 1)\n","\n","    Returns:\n","        - xt: Dict of one-hot inputs {0: x_0, 1: x_1, ...}\n","        - ht: Dict of hidden states {-1: h_init, 0: h_0, 1: h_1, ...}\n","        - ct: Dict of cell states {-1: c_init, 0: c_0, 1: c_1, ...}\n","        - probt: Dict of probability distributions {0: p_0, 1: p_1, ...}\n","        - loss: Total cross-entropy loss across all timesteps (scalar)\n","    \"\"\"\n","    # Initialize storage dictionaries\n","    xt = {}  # Store one-hot encoded inputs\n","    ft = {}  # Store forget gate values\n","    it = {}  # Store input gate values\n","    cct = {} # Store ccs values\n","    ot = {}  # Store output gate values\n","    ht = {}  # Store hidden states\n","    ct = {}  # Store cell states\n","    yt = {}  # Store Output logits\n","    probt = {}  # Store probability distributions (after softmax)\n","\n","    # Set initial hidden and cell state\n","    ht[-1] = np.copy(h_prev) #(H,1)\n","    ct[-1] = np.copy(c_prev) #(H,1)\n","    loss = 0\n","    # Loop through each timestep in the sequence\n","    for t in range(len(inputs)):\n","        # Step 1: Convert character index to one-hot vector\n","        xt[t] = np.zeros((vocab_size, 1)) #(V,1)\n","        xt[t][inputs[t]] = 1  # Set the position of char to 1, (V,1)\n","        # Step 2: Run LSTM cell (forward computation)\n","        ft[t], it[t], cct[t], ot[t], ct[t], ht[t], yt[t] = lstm(ct[t-1], ht[t-1], xt[t]) # (H,1), (H,1), (V,1)\n","        # Step 3: Apply softmax to get probabilities\n","        probt[t] = softmax(yt[t]) # (V,1)\n","        # Step 3: Compute loss for this timestep\n","        # Cross-entropy: -log(probability of correct next character)\n","        loss += -np.log(probt[t][targets[t], 0]+ epsilon)\n","\n","    cache = (h_prev, c_prev, ft, it, cct, ot)\n","    return xt, ht, ct, probt, loss, cache"],"metadata":{"id":"ErpAgrEEEpld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __BACKWARD PASS (BPTT FOR LSTM)__\n","**Complete LSTM Backpropagation Through Time Equations**\n","#### **Step 1: Output Layer Gradient (Softmax + Cross-Entropy)**\n","\n","$$\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} = \\mathbf{p}_t - \\mathbf{1}_{y^*_t}$$\n","\n","Where $\\mathbf{p}_t$ is the predicted probability distribution and $\\mathbf{1}_{y^*_t}$ is the one-hot vector of the true label.\n","\n","**Output layer weight gradients:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\mathbf{h}_t^T$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t}$$\n","\n","#### **Step 2: Hidden State Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} = \\mathbf{W}_y^T \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}}$$\n","\n","The gradient flows from two sources:\n","- Current timestep's output loss (first term)\n","- Future timestep's hidden state (second term)\n","\n","\n","#### **Step 3: Output Gate Gradients**\n","\n","Recall: $\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)$\n","\n","**Gradient w.r.t. output gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot \\tanh(\\mathbf{c}_t)$$\n","\n","**Gradient w.r.t. output gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_o} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} \\odot \\mathbf{o}_t \\odot (1 - \\mathbf{o}_t)$$\n","\n","(Using sigmoid derivative: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$)\n","\n","#### **Step 4: Cell State Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot \\mathbf{o}_t \\odot (1 - \\tanh^2(\\mathbf{c}_t)) + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t+1}}$$\n","\n","**Key Insight:** The cell state gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$ flows through forget gates across timesteps, allowing long-range gradient propagation without vanishing!\n","\n","#### **Step 5: Candidate Cell State Gradients**\n","\n","Recall: $\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t$\n","**Gradient w.r.t. candidate cell (after tanh):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{c}}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{i}_t$$\n","**Gradient w.r.t. candidate cell pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_c} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{c}}_t} \\odot (1 - \\tilde{\\mathbf{c}}_t^2)$$\n","\n","(Using tanh derivative: $\\tanh'(z) = 1 - \\tanh^2(z)$)\n","#### **Step 6: Input Gate Gradients**\n","**Gradient w.r.t. input gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{i}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\tilde{\\mathbf{c}}_t$$\n","**Gradient w.r.t. input gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{i}_t} \\odot \\mathbf{i}_t \\odot (1 - \\mathbf{i}_t)$$\n","#### **Step 7: Forget Gate Gradients**\n","**Gradient w.r.t. forget gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{f}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{c}_{t-1}$$\n","**Gradient w.r.t. forget gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_f} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{f}_t} \\odot \\mathbf{f}_t \\odot (1 - \\mathbf{f}_t)$$\n","#### **Step 8: Weight Matrix Gradients (All Gates)**\n","For each gate $g \\in \\{f, i, c, o\\}$:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_g} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_g} \\mathbf{z}_t^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_g} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_g}$$\n","Where $\\mathbf{z}_t = [\\mathbf{h}_{t-1}; \\mathbf{x}_t]$ is the concatenated input.\n","#### **Step 9: Gradient to Previous Timestep**\n","**Gradient w.r.t. concatenated input:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t} = \\sum_{g \\in \\{f,i,c,o\\}} \\mathbf{W}_g^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_g}$$\n","**Split to get gradient for previous hidden state:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t}[0:H]$$\n","(First $H$ elements of $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t}$)\n","#### **Step 10: Cell State Gradient to Previous Timestep**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{f}_t$$\n","This gradient flows backward through time via the forget gate, enabling long-term memory.\n","**Implementation Note:** Gradients are accumulated across all timesteps (sums in Step 8), then clipped to prevent exploding gradients before parameter updates.\n","**Notation:**\n","- $\\odot$ = element-wise multiplication\n","- $\\sigma$ = sigmoid function\n","- $T$ = sequence length\n","- $H$ = hidden dimension"],"metadata":{"id":"7CTgWopPOAka"}},{"cell_type":"code","source":["def backward(inputs, targets, xt, ht, ct, probt, cache):\n","    \"\"\"\n","    Backpropagation Through Time (BPTT) for LSTM\n","\n","    Inputs:\n","        - inputs: List of input character indices\n","        - targets: List of target character indices\n","        - xt: Dict of one-hot inputs from forward pass\n","        - ht: Dict of hidden states from forward pass\n","        - ct: Dict of cell states from forward pass\n","        - probt: Dict of probability distributions from forward pass\n","        - cache: Tuple (h_prev, c_prev, ft, it, cct, ot) from forward pass\n","\n","    Returns:\n","        - dWf, dWi, dWc, dWo: Gradients for gate weight matrices\n","        - dWy: Gradient w.r.t. hidden-to-output weights\n","        - dbf, dbi, dbc, dbo: Gradients for gate biases\n","        - dby: Gradient w.r.t. output bias\n","    \"\"\"\n","    h_prev, c_prev, ft, it, cct, ot = cache\n","    dWf = np.zeros_like(Wf)  # (H, H+V)\n","    dWi = np.zeros_like(Wi)  # (H, H+V)\n","    dWc = np.zeros_like(Wc)  # (H, H+V)\n","    dWo = np.zeros_like(Wo)  # (H, H+V)\n","    dWy = np.zeros_like(Wy)  # (V, H)\n","\n","    dbf = np.zeros_like(bf)  # (H, 1)\n","    dbi = np.zeros_like(bi)  # (H, 1)\n","    dbc = np.zeros_like(bc)  # (H, 1)\n","    dbo = np.zeros_like(bo)  # (H, 1)\n","    dby = np.zeros_like(by)  # (V, 1)\n","\n","    # Gradient of hidden state and cell state at next timestep (initially zero)\n","    dh_next = np.zeros_like(ht[0])  # (H, 1)\n","    dc_next = np.zeros_like(ct[0])  # (H, 1)\n","\n","    # Backpropagate through time (from last to first timestep)\n","    for t in reversed(range(len(inputs))):\n","        # Step 1: Gradient of loss w.r.t output probabilities\n","        dy = np.copy(probt[t])  # (V, 1)\n","        dy[targets[t]] -= 1  # Subtract 1 from correct class (cross-entropy gradient)\n","\n","        # Step 2: Gradients for output layer (Wy, by)\n","        dWy += np.dot(dy, ht[t].T)  # (V,1)@(1,H) = (V,H)\n","        dby += dy  # (V, 1)\n","\n","        # Step 3: Gradient w.r.t hidden state\n","        # Comes from two sources: current output and next timestep\n","        dh = np.dot(Wy.T, dy) + dh_next  # (H,V)@(V,1) + (H,1) = (H,1)\n","\n","        # Step 4: Gradient w.r.t output gate\n","        tanh_ct = np.tanh(ct[t])  # (H, 1)\n","        do = dh * tanh_ct  # (H,1) * (H,1) = (H,1)\n","        dzo = do * ot[t] * (1 - ot[t])  # (H,1)*(H,1)*(H,1)=(H,1)\n","\n","        # Step 5: Gradient w.r.t cell state\n","        dc = dh * ot[t] * (1 - tanh_ct**2)  # (H,1)*(H,1)*(H,1) = (H,1)\n","        dc = dc + dc_next  # gradient from next timestep will be added\n","\n","        # Step 6: Gradient w.r.t candidate cell state\n","        dcc = dc * it[t]  # (H,1) * (H,1) = (H,1)\n","        dzcc = dcc * (1 - cct[t]**2)  # (H,1)\n","\n","        # Step 7: Gradient w.r.t input gate\n","        di = dc * cct[t]  # (H,1) * (H,1) = (H,1)\n","        dzi = di * it[t] * (1 - it[t])  # (H,1)\n","\n","        # Step 8: Gradient w.r.t forget gate\n","        df = dc * ct[t-1]  # (H,1) * (H,1) = (H,1)\n","        dzf = df * ft[t] * (1 - ft[t])  # (H,1)\n","\n","        # Step 9: Create concatenated input [h_{t-1}, x_t]\n","        zt = np.concatenate((ht[t-1], xt[t]), axis=0)  # (H+V, 1)\n","\n","        # Step 10: Gradients for gate weight matrices and biases\n","        dWf += np.dot(dzf, zt.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbf += dzf  # (H,1)\n","\n","        dWi += np.dot(dzi, zt.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbi += dzi  # (H,1)\n","\n","        dWc += np.dot(dzcc, zt.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbc += dzcc  # (H,1)\n","\n","        dWo += np.dot(dzo, zt.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbo += dzo  # (H,1)\n","\n","        # Step 11: Gradient w.r.t concatenated input [h_t-1, x_t]\n","        dzt = np.dot(Wf.T, dzf) + np.dot(Wi.T, dzi) + np.dot(Wc.T, dzcc) + np.dot(Wo.T, dzo) #(H+V, 1)\n","\n","        # Step 12: Split gradient and pass to previous timestep\n","        dh_next = dzt[:hidden_size, :]  # First H elements: (H,1)\n","\n","        # Step 13: Gradient w.r.t previous cell state\n","        dc_next = dc * ft[t]  # (H,1) * (H,1) = (H,1)\n","\n","        # Clip gradients to prevent exploding gradients\n","    for dparam in [dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby]:\n","        np.clip(dparam, -5, 5, out=dparam)\n","\n","    return dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby"],"metadata":{"id":"07BaraxIyIst"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding LSTM BPTT: The Gradient Flow**\n","\n","**1. The Softmax-CrossEntropy Gradient**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} = \\mathbf{p} - \\mathbf{1}_{y^*}$$\n","**Example:** If vocab_size=5 and target=2:\n","```python\n","probt[t] = [0.1, 0.2, 0.6, 0.05, 0.05]  # Model's prediction\n","target = 2  # Correct class\n","dy = [0.1, 0.2, -0.4, 0.05, 0.05]  # Gradient (0.6 - 1 = -0.4 at position 2)\n","```\n","**2. LSTM Gradient Flow - The Highway**\n","\n","Unlike vanilla RNNs, LSTM gradients flow through TWO paths:\n","\n","**Path 1 (Hidden State):** $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} \\leftarrow \\mathbf{W}_f, \\mathbf{W}_i, \\mathbf{W}_c, \\mathbf{W}_o$\n","- Subject to multiplication by weights → can still vanish\n","\n","**Path 2 (Cell State Highway):** $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{f}_t$\n","- **Only element-wise multiplication** by forget gate!\n","- If $\\mathbf{f}_t \\approx 1$, gradients flow unimpeded\n","- This is the **\"constant error carousel\"** that solves vanishing gradients!\n","\n","**3. Why LSTM Solves Vanishing Gradients**\n","\n","Vanilla RNN: $\\frac{\\partial \\mathbf{h}_0}{\\partial \\mathbf{h}_T} = \\prod_{t=1}^{T} \\mathbf{W}_{hh}^T \\cdot \\text{diag}(1-\\mathbf{h}_t^2)$\n","- Product of many matrices → exponential decay\n","\n","LSTM Cell State: $\\frac{\\partial \\mathbf{c}_0}{\\partial \\mathbf{c}_T} = \\prod_{t=1}^{T} \\mathbf{f}_t$\n","- Only element-wise products → gradients can flow for 100+ timesteps!\n","\n","**4. Gradient Clipping Still Needed**While LSTM solves vanishing gradients, **exploding gradients** can still occur through the hidden state path. Clipping to [-5, 5] prevents: `loss = nan`, `weights = inf`"],"metadata":{"id":"Spgg8FYXLqKl"}},{"cell_type":"markdown","source":["### __UPDATE PARAMS WITH ADAM__"],"metadata":{"id":"whAHx2WTW03k"}},{"cell_type":"code","source":["def update_parameters(dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby, learning_rate):\n","    \"\"\"\n","    Update LSTM model parameters using Adam optimizer with bias correction\n","\n","    Inputs:\n","        - dWf: Gradient for forget gate weights (H, H+V)\n","        - dWi: Gradient for input gate weights (H, H+V)\n","        - dWc: Gradient for cell candidate weights (H, H+V)\n","        - dWo: Gradient for output gate weights (H, H+V)\n","        - dWy: Gradient for prediction weights (V, H)\n","        - dbf, dbi, dbc, dbo: Gradients for gate biases (H, 1)\n","        - dby: Gradient for output bias (V, 1)\n","        - learning_rate: Step size for parameter updates\n","\n","    Returns:\n","        - None (updates global parameters in-place)\n","    \"\"\"\n","    global Wf, Wi, Wc, Wo, Wy, bf, bi, bc, bo, by\n","    global mWf, mWi, mWc, mWo, mWy, mbf, mbi, mbc, mbo, mby\n","    global vWf, vWi, vWc, vWo, vWy, vbf, vbi, vbc, vbo, vby\n","    global t_adam\n","\n","    t_adam += 1\n","    params = [\n","        (Wf, dWf, mWf, vWf),\n","        (Wi, dWi, mWi, vWi),\n","        (Wc, dWc, mWc, vWc),\n","        (Wo, dWo, mWo, vWo),\n","        (Wy, dWy, mWy, vWy),\n","        (bf, dbf, mbf, vbf),\n","        (bi, dbi, mbi, vbi),\n","        (bc, dbc, mbc, vbc),\n","        (bo, dbo, mbo, vbo),\n","        (by, dby, mby, vby)\n","    ]\n","    updated = []\n","    for param, grad, m, v in params:\n","        m = beta1 * m + (1 - beta1) * grad\n","        v = beta2 * v + (1 - beta2) * (grad ** 2)\n","        # Bias correction\n","        m_corrected = m / (1 - beta1 ** t_adam)\n","        v_corrected = v / (1 - beta2 ** t_adam)\n","        param = param - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n","        updated.append((param, m, v))\n","\n","    (Wf, mWf, vWf), (Wi, mWi, vWi), (Wc, mWc, vWc), (Wo, mWo, vWo), (Wy, mWy, vWy), \\\n","    (bf, mbf, vbf), (bi, mbi, vbi), (bc, mbc, vbc), (bo, mbo, vbo), (by, mby, vby) = updated"],"metadata":{"id":"wSGIUqZsMU0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __TRAIN MODEL__\n"],"metadata":{"id":"hpHLQqh7XFil"}},{"cell_type":"code","source":["def train(data, num_iterations=1000, print_every=100, sample_every=100):\n","    \"\"\"\n","    Train LSTM language model\n","\n","    Inputs:\n","        - data: String of training text\n","        - num_iterations: Number of training iterations (not full data passes)\n","        - print_every: Print loss every N iterations\n","        - sample_every: Generate sample text every N iterations\n","\n","    Returns:\n","        - smooth_loss: Exponentially smoothed loss value\n","\n","    Note: Both hidden state h_prev AND cell state c_prev are maintained across\n","          sequences for temporal continuity. Only reset when reaching end of\n","          data or on first iteration.\n","    \"\"\"\n","    smooth_loss = -np.log(1.0 / vocab_size) * seq_len\n","    n = 0  # Iterations counter\n","    p = 0  # Data pointer (position in text)\n","    h_prev = np.zeros((hidden_size, 1))\n","    c_prev = np.zeros((hidden_size, 1))  # LSTM cell state\n","\n","    while n < num_iterations:\n","        # Reset pointer, hidden state, AND cell state at end of data or first iteration\n","        if p + seq_len + 1 >= len(data) or n == 0:\n","            h_prev = np.zeros((hidden_size, 1))  # Fresh start\n","            c_prev = np.zeros((hidden_size, 1))  # Fresh cell state\n","            p = 0  # Go back to beginning\n","\n","        # Input:  characters at positions [p, p+1, ..., p+seq_len-1]\n","        # Target: characters at positions [p+1, p+2, ..., p+seq_len]\n","        inputs = [char_to_ix[ch] for ch in data[p:p+seq_len]]\n","        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_len+1]]\n","\n","        # Forward pass (pass both hidden state AND cell state!)\n","        xt, ht, ct, probt, loss, cache = forward(inputs, targets, c_prev, h_prev)\n","\n","        # Update hidden state AND cell state for next sequence\n","        h_prev = np.copy(ht[len(inputs) - 1])\n","        c_prev = np.copy(ct[len(inputs) - 1])\n","\n","        # Backward pass\n","        dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby = backward(inputs, targets, xt, ht, ct, probt, cache)\n","\n","        # Update parameters\n","        update_parameters(dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby, lr)\n","\n","        # Update smooth loss\n","        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","\n","        # Print progress\n","        if n % print_every == 0:\n","            print(f\"Iter {n:6d} | Loss: {smooth_loss:.4f}\")\n","        # Generate sample text\n","        if n % sample_every == 0 and n > 0:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"SAMPLE at iteration {n}:\")\n","            print(f\"{'='*60}\")\n","            sample_text = sample(data[0], n_chars=200)\n","            print(sample_text)\n","            print(f\"{'='*60}\\n\")\n","        p += seq_len\n","\n","        n += 1\n","    print(\"\\nTraining complete!\")\n","    return smooth_loss"],"metadata":{"id":"HvrBIPp3XAt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training Deep Dive: What's Really Happening?**\n","\n","**The LSTM Training Loop Flow:**\n","\n","```\n","Initialize: h = zeros(100, 1), c = zeros(100, 1)\n","\n","Iteration 1:\n","  Data: \"Once upon a time, on a ve\"\n","  → Forward: Get loss, store ALL states (h, c, gates)\n","  → h_prev = h[24], c_prev = c[24]  ← Keep BOTH for next iteration!\n","  → Backward: Compute gradients for all 10 parameters\n","  → Adam: Update Wf, Wi, Wc, Wo, Wy, bf, bi, bc, bo, by\n","\n","Iteration 2:\n","  Data: \"ry hot day, a thirsty cro\"\n","  → Forward with h_prev AND c_prev from iteration 1  ← DOUBLE CONTINUITY!\n","  → ...\n","```\n","\n","**Why LSTM Training Works Better:**\n","\n","1. **Dual state continuity**: Both `h_prev` (short-term) and `c_prev` (long-term) maintained\n","2. **Cell state highway**: Enables learning dependencies 100+ timesteps away\n","3. **Gate control**: Forget gate decides what to keep/discard from memory\n","- Result: LSTM learns longer dependencies with more stable gradients\n","\n","**Critical for LSTM:** Must maintain BOTH `h_prev` and `c_prev` across sequences. Resetting either breaks long-term memory!\n","\n","**LSTM vs Vanilla RNN:**\n","- LSTM: 2 states (`h_prev`, `c_prev`), 10 parameters\n","- RNN: 1 state (`h_prev`), 5 parameters"],"metadata":{"id":"xqbJ8Rh_MljU"}},{"cell_type":"markdown","source":["### __SAMPLING FUNCTION (GENERATE TEXT)__"],"metadata":{"id":"j4GOFoOuYzvD"}},{"cell_type":"code","source":["def sample(seed_char, n_chars=100):\n","    \"\"\"\n","    Generate text by sampling from the trained LSTM language model\n","\n","    Inputs:\n","        - seed_char: Starting character for text generation\n","        - n_chars: Number of characters to generate (default 100)\n","\n","    Returns:\n","        - String of generated text starting with seed_char\n","\n","    Note: Uses stochastic sampling from probability distribution (not argmax).\n","          LSTM requires both hidden state h AND cell state c.\n","    \"\"\"\n","    if seed_char not in char_to_ix:\n","        if seed_char.lower() in char_to_ix:\n","            seed_char = seed_char.lower()\n","        else:\n","            seed_char = data[0]\n","            print(f\"Warning: Seed not in vocab. Using '{seed_char}' instead\")\n","\n","    x = np.zeros((vocab_size, 1)) # Convert seed character to one-hot\n","    x[char_to_ix[seed_char]] = 1\n","    h = np.zeros((hidden_size, 1)) # Initialize hidden state\n","    c = np.zeros((hidden_size, 1)) # Initialize cell state\n","    generated_chars = [seed_char] # Store generated characters\n","\n","    for _ in range(n_chars):\n","        _, _, _, _, c, h, y = lstm(c, h, x)\n","        p = softmax(y)\n","        ix = np.random.choice(range(vocab_size), p=p.ravel())\n","        char = ix_to_char[ix]\n","        generated_chars.append(char)\n","        x = np.zeros((vocab_size, 1))\n","\n","        x[ix] = 1\n","\n","    return ''.join(generated_chars)"],"metadata":{"id":"ldl7q4JsYWJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __RUN TRAINING__"],"metadata":{"id":"1y0M3vMIZcjn"}},{"cell_type":"code","source":["final_loss = train(data, num_iterations=10000, print_every=1000, sample_every=2000)\n","print(f\"\\nFinal smooth loss: {final_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjfsldMiZVna","outputId":"fae4c357-eea0-4a1a-d1a7-106e21a7eff8","executionInfo":{"status":"ok","timestamp":1763542421225,"user_tz":-300,"elapsed":82580,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter      0 | Loss: 87.4127\n","Iter   1000 | Loss: 74.9917\n","Iter   2000 | Loss: 59.8194\n","\n","============================================================\n","SAMPLE at iteration 2000:\n","============================================================\n","Onde w unpimthet He tpisiren inungol !odes the krtin, inkine. das acdrori,. ted satee stteein..\n","ca. way sowathy \n","oged tae ater yh and bmiw fletinlinelinc belinged linponpl pnmpiflow s tlorlatcrer and p\n","============================================================\n","\n","Iter   3000 | Loss: 46.6624\n","Iter   4000 | Loss: 33.6497\n","\n","============================================================\n","SAMPLE at iteration 4000:\n","============================================================\n","Once upon a tilm, t mea hot deok doug un tol  hone dow ther. a upilel cher w tore,. the wat erebugdAthe, hot tea peyw hingh yrot, the crow wate Thorg the the coten an wastre coupping tore. deapchir sas\n","============================================================\n","\n","Iter   5000 | Loss: 21.9956\n","Iter   6000 | Loss: 12.8951\n","\n","============================================================\n","SAMPLE at iteration 6000:\n","============================================================\n","Once upon a bistweac oot downs seary, on a time, on a vere hint reb ane soo an a dron iu search iok and flew akly fona crow was fleling stinrin .\n","rireweo wat lling un inminve mimer crow conlying bideno\n","============================================================\n","\n","Iter   7000 | Loss: 6.9401\n","Iter   8000 | Loss: 3.6156\n","\n","============================================================\n","SAMPLE at iteration 8000:\n","============================================================\n","Once upon a time, on a very hot day, a thirsty crow was flyclon ued dropping stones unt las was wea litg un inebded ont. Soored tot, the water began to rise. Thenc ton ho got, beda gat rot eat ras fong\n","============================================================\n","\n","Iter   9000 | Loss: 2.3766\n","\n","Training complete!\n","\n","Final smooth loss: 1.1603\n"]}]},{"cell_type":"markdown","source":["### __TEST DIFFERENT SEEDS__"],"metadata":{"id":"jhx3NWCEtl0J"}},{"cell_type":"code","source":["seed_chars = ['T', 'A', 'H', 'W', 'I']\n","for char in seed_chars:\n","    generated = sample(char, n_chars=150)\n","    print(char, ':', generated)\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_1JWyvyZi7B","outputId":"af9e6c83-3914-4d14-dda4-42e4455fa68e","executionInfo":{"status":"ok","timestamp":1763542421291,"user_tz":-300,"elapsed":49,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["T : Tmlcl upon a timr. TAe was was ining brightly, and the piow tor came up high erbg deak the water the thot, the water began to rise. The crow continued \n","------------------------------------------------------------\n","A : Ance upon a time, on a ver a ver a dim not reshed on ide. There was a little water at the bottom, but his beak could not reacr pickon an to rise. The c\n","------------------------------------------------------------\n","H : Hrce upon a ver a time, a thirsty crow was flying is wrarch of water. The sun was shining brightly, and the poor crow was feeling tired and weak.\n","\n","Afte\n","------------------------------------------------------------\n","W : wgcceupon a tile, on a sisearch owas faychiy heak.\n","\n","At The stirt. the crow thought for a moment. Then he got an idea!\n","\n","He started pickly ly wnming up k\n","------------------------------------------------------------\n","I : iupe wawea hitthe water at the bottom, but his beak could not reach it.\n","\n","The crow thought for a moment. Then he got an idea!\n","\n","He started picking up sma\n","------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## **SUMMARY**\n","### **Key Takeaways:**\n","1. **Cell State = Long-Term Memory**: LSTMs use cell state $\\mathbf{c}_t$ for long-term memory, separate from hidden state $\\mathbf{h}_t$\n","2. **Gating Mechanisms**: Three gates (forget, input, output) control information flow and solve vanishing gradients\n","3. **BPTT with Gradient Highway**: Cell state gradient flows directly through forget gates, enabling learning across 100+ timesteps\n","5. **Gradient Clipping is Essential**: Clipping to [-5, 5] prevents exploding gradients during BPTT\n","6. **State Continuity Matters**: Maintaining both $\\mathbf{h}_t$ and $\\mathbf{c}_t$ across sequences enables long-range learning\n","\n","### **Possible Extensions:**\n","1. **Temperature Sampling**: Add `temperature` parameter to control randomness: `p = softmax(logits / T)`\n","2. **Stacked LSTMs**: Add 2-3 LSTM layers for hierarchical feature learning\n","3. **Bidirectional LSTM**: Process sequences both forward and backward\n","4. **Different Datasets**: Shakespeare, Python code, Linux kernel, Wikipedia\n","5. **Dropout Regularization**: Add dropout between LSTM layers to prevent overfitting\n","6. **Perplexity Metric**: Calculate `exp(avg_loss)` for better interpretability\n","\n","### **Comparison: Vanilla RNN vs LSTM:**\n","| Aspect | Vanilla RNN | LSTM (This Implementation) |\n","|--------|-------------|----------------------------|\n","| **Memory** | Hidden state only | Cell state + hidden state |\n","| **Long-range** | ~20 timesteps | 100-200 timesteps |\n","| **Gradients** | Vanishing/exploding | Stable via gates |\n","| **Parameters** | ~15K (same size) | 57K (4x weight matrices) |\n","| **Training** | Faster (simpler) | Slower (more computation) |\n","### **References:**\n","1. Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735-1780. [**Original LSTM Paper**]\n","2. Gers, F. A., et al. (2000). *Learning to Forget: Continual Prediction with LSTM*. Neural Computation, 12(10), 2451-2471.\n","3. Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). *An Empirical Exploration of Recurrent Network Architectures*. In Proceedings of the 32nd International Conference on Machine Learning (ICML) (pp. 2342–2350). PMLR.\n","\n","### **Further Reading:**\n","1. Hochreiter & Schmidhuber (1997). [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) — Original LSTM paper\n","2. Olah, C. (2015). [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) — Visual explanations\n","3. Cho et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for SMT — Introduced GRUs\n","4. Vaswani et al. (2017). Attention Is All You Need. NeurIPS — Transformers replacing LSTMs\n","5. Goodfellow et al. (2016). Deep Learning. Chapter 10: Sequence Modeling — Comprehensive theory\n","6. Graves, A. (2013). Generating Sequences With Recurrent Neural Networks — Advanced LSTM techniques"],"metadata":{"id":"HhZHDwtPMo4E"}}]}