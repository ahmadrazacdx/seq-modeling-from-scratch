{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Character-Level GRU Language Model**\n","\n","## **From Scratch Implementation in NumPy**\n","\n","This notebook implements a **vanilla GRU** for character-level language modeling.\n","\n","\n","### **What We'll Build:**\n","- **Vanilla GRU** with two (update, reset) gates\n","- **Backpropagation** for gradient computation\n","- **Character-level text generation** from learned patterns\n","---\n","*Notebook by*: Ahmad Raza [@ahmadrazacdx](https://github.com/ahmadrazacdx)<br>\n","*Date: 2025*  \n","*License: MIT*"],"metadata":{"id":"kv99qL16D9Kj"}},{"cell_type":"code","source":["import numpy as np\n","np.random.seed(42)"],"metadata":{"id":"SgSsJcK83DfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goJyoKz2zrmy","outputId":"4c82756b-bc64-4d6c-f2a2-c7a2f1d624cc","executionInfo":{"status":"ok","timestamp":1763524955625,"user_tz":-300,"elapsed":11,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has 677 characters, 33 unique.\n"]}],"source":["#Load Data\n","data = open('../data/thirsty_crow.txt', 'r').read()\n","chars = sorted(list(set(data)))\n","data_size, vocab_size = len(data), len(chars)\n","print(f'Data has {data_size} characters, {vocab_size} unique.')"]},{"cell_type":"code","source":["char_to_ix = {ch:i for i,ch in enumerate(chars)}"],"metadata":{"id":"xc6oODzj3ijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ix_to_char = {i:ch for i,ch in enumerate(chars)}"],"metadata":{"id":"ClysqC-_3wNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __HYPER-PARAMETERS__"],"metadata":{"id":"JhOzRBXz414W"}},{"cell_type":"code","source":["lr = 1e-3 # learning rate\n","seq_len = 25 # times GRU will be unrolled (Timesteps)\n","hidden_size= 100 # size of hidden units"],"metadata":{"id":"InWw7ZqC3xVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __MODEL PARAM INIT__\n","**Initializing weight matrices for the GRU.**\n","\n","**Gate Weight Matrices (concatenated input $[\\mathbf{h}_{t-1}; \\mathbf{x}_t]$):**\n","- $\\mathbf{W}_u \\in \\mathbb{R}^{H \\times (H+V)}$ Update gate weights\n","- $\\mathbf{W}_r \\in \\mathbb{R}^{H \\times (H+V)}$ Reset gate weights\n","- $\\mathbf{W}_h \\in \\mathbb{R}^{H \\times (H+V)}$ Candidate hidden state weights\n","\n","**Output Layer:**\n","- $\\mathbf{W}_y \\in \\mathbb{R}^{V \\times H}$ Hidden-to-output weights\n","\n","**Biases:**\n","- Gate biases: $\\mathbf{b}_u, \\mathbf{b}_r, \\mathbf{b}_h \\in \\mathbb{R}^{H \\times 1}$\n","- Output bias: $\\mathbf{b}_y \\in \\mathbb{R}^{V \\times 1}$\n","\n","**Total parameters:** $3H(H+V) + VH + 3H + V$\n","\n","**Where:** $V$ = vocabulary size (33 unique chars), $H$ = hidden size (100)\n","\n","**Note:** GRU has ~25% fewer parameters than LSTM due to simpler architecture (2 gates vs 3, single hidden state vs separate cell state)."],"metadata":{"id":"5LRNhINY6CMp"}},{"cell_type":"code","source":["Wu = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Update Gate weights (H, H+V)\n","Wr = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Reset Gate weights (H, H+V)\n","Wh = np.random.randn(hidden_size, hidden_size+vocab_size)*0.01 # Candidate Hidden State weights (H, H+V)\n","Wy = np.random.randn(vocab_size, hidden_size)*0.01 # Prediction weights (V, H)\n","bu = np.zeros((hidden_size, 1)) # Update Gate bias (H, 1)\n","br = np.zeros((hidden_size, 1)) # Reset Gate bias(H, 1)\n","bh = np.zeros((hidden_size, 1)) # CHS bias (H, 1)\n","by = np.zeros((vocab_size, 1)) # prediction bias (V, 1)"],"metadata":{"id":"NPbbN7zi5jjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"\n","Wu: Update Gate Weights  : {Wu.shape}\n","Wr: Reset Gate Weights   : {Wr.shape}\n","Wh: CHS Weights          : {Wh.shape}\n","Wy: Prediction Weights   : {Wy.shape}\n","bu: Update Gate bias     : {bu.shape}\n","br: Reset Gate bias      : {br.shape}\n","bh: CHS bias             : {bh.shape}\n","by: Prediction bias      : {by.shape}\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HGbspWj7RRE","outputId":"24c7a8f8-bbe6-4759-e910-372905466fcb","executionInfo":{"status":"ok","timestamp":1763524958877,"user_tz":-300,"elapsed":184,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Wu: Update Gate Weights  : (100, 133)\n","Wr: Reset Gate Weights   : (100, 133)\n","Wh: CHS Weights          : (100, 133)\n","Wy: Prediction Weights   : (33, 100)\n","bu: Update Gate bias     : (100, 1)\n","br: Reset Gate bias      : (100, 1)\n","bh: CHS bias             : (100, 1)\n","by: Prediction bias      : (33, 1)\n","\n"]}]},{"cell_type":"markdown","source":["### __ADAM OPTIMIZER INITIALIZATION__"],"metadata":{"id":"xvWhMsGbeRjl"}},{"cell_type":"code","source":["# Adam hyperparameters\n","beta1 = 0.9\n","beta2 = 0.999\n","epsilon = 1e-8\n","\n","# Adam memory variables (first moment)\n","mWu = np.zeros_like(Wu)\n","mWr = np.zeros_like(Wr)\n","mWh = np.zeros_like(Wh)\n","mWy = np.zeros_like(Wy)\n","mbu = np.zeros_like(bu)\n","mbr = np.zeros_like(br)\n","mbh = np.zeros_like(bh)\n","mby = np.zeros_like(by)\n","\n","# Adam memory variables (second moment)\n","vWu = np.zeros_like(Wu)\n","vWr = np.zeros_like(Wr)\n","vWh = np.zeros_like(Wh)\n","vWy = np.zeros_like(Wy)\n","vbu = np.zeros_like(bu)\n","vbr = np.zeros_like(br)\n","vbh = np.zeros_like(bh)\n","vby = np.zeros_like(by)\n","# Timestep counter for bias correction\n","t_adam = 0"],"metadata":{"id":"tUZFfUeAePrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","def softmax(z):\n","    exp_z = np.exp(z - np.max(z))\n","    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"],"metadata":{"id":"ehegFzIZmOCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SINGLE GRU CELL__\n","**GRU Forward Pass Equations:**\n","**Note:** This implementation follows paper's (Cho et al. 2014) equations. The frameworks like PyTorch and others follow slightly different equations for efficiency.\n","\n","**Update Gate:**\n","$$\\mathbf{z}_u = \\mathbf{W}_u[\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_u$$\n","$$\\mathbf{u}_t = \\sigma(\\mathbf{z}_u)$$\n","**Reset Gate:**\n","$$\\mathbf{z}_r = \\mathbf{W}_r[\\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_r$$\n","$$\\mathbf{r}_t = \\sigma(\\mathbf{z}_r)$$\n","**Candidate Hidden State:**\n","$$\\mathbf{z}_h = \\mathbf{W}_h[\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_h$$\n","$$\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{z}_h)$$\n","**Hidden State (Linear Interpolation):**\n","$$\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{u}_t \\odot \\tilde{\\mathbf{h}}_t$$\n","**Output:**\n","$$\\mathbf{y}_t = \\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y$$\n","**Where:** $\\sigma$ = sigmoid, $\\odot$ = element-wise multiplication\n","\n","**Reference:** Cho et al. (2014) - *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*"],"metadata":{"id":"VkDtomuF8LFZ"}},{"cell_type":"code","source":["def gru(h_prev, xt):\n","    zt = np.concatenate((h_prev, xt), axis=0) #(H+V,1)\n","    # Update Gate\n","    zu = np.dot(Wu, zt) + bu #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    ut = sigmoid(zu) # (H,1)\n","    #Reset Gate\n","    zr = np.dot(Wr, zt) + br #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    rt = sigmoid(zr) # (H,1)\n","    #Candidate Hidden State\n","    zt_h = np.concatenate((rt * h_prev, xt), axis=0) # (H,1)*(H,1)=(H,1):(V,1)= (H+V,1)\n","    zh = np.dot(Wh, zt_h) + bh #(H,H+V)@(H+V,1)->(H,1)+(H,1)=(H,1)\n","    cht = np.tanh(zh) # (H,1)\n","    #Hidden State\n","    ht = ((1-ut) * h_prev) + (ut * cht) # (H,1)+(H,1)=(H,1)\n","    #Output logits\n","    yt = np.dot(Wy, ht) + by # #(V,H)@(H,1)->(V,1)+(V,1)=(V,1)\n","    return ut, rt, cht, ht, yt"],"metadata":{"id":"AVvRr4GJ7SmG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding the GRU Cell:**\n","\n","**What's happening here?**\n","- At each timestep $t$, the GRU takes two inputs:\n","  1. Current character $\\mathbf{x}_t$ (one-hot vector)\n","  2. Previous hidden state $\\mathbf{h}_{t-1}$\n","  \n","- It outputs:\n","  1. New hidden state $\\mathbf{h}_t$ (combines short and long-term memory)\n","  2. Prediction $\\mathbf{y}_t$ (probability distribution)\n","\n","**The GRU Magic - Two Gates:**\n","\n","1. **Update gate** ($\\mathbf{u}_t$): Decides how much of the past to keep vs new candidate\n","   - Acts like combined forget + input gate in LSTM\n","   - When $\\mathbf{u}_t \\approx 0$: Keep old hidden state (remember long-term)\n","   - When $\\mathbf{u}_t \\approx 1$: Use new candidate (incorporate new info)\n","\n","2. **Reset gate** ($\\mathbf{r}_t$): Decides how much past information to use when computing candidate\n","   - Controls access to previous hidden state\n","   - When $\\mathbf{r}_t \\approx 0$: Ignore past, compute fresh candidate\n","   - When $\\mathbf{r}_t \\approx 1$: Use full previous hidden state\n"],"metadata":{"id":"4f5wu1Tj9rbm"}},{"cell_type":"markdown","source":["### __FORWARD PASS__\n"],"metadata":{"id":"JSiqg1u1Fcuo"}},{"cell_type":"code","source":["def forward(inputs, targets, h_prev):\n","    \"\"\"\n","    Forward pass through GRU Cell\n","\n","    Inputs:\n","        - inputs: List of character indices, e.g., [5, 8, 12, 12, 15] for \"hello\"\n","        - targets: List of target character indices (inputs shifted by 1)\n","        - h_prev: Initial hidden state from previous sequence, shape (H, 1)\n","\n","    Returns:\n","        - xt: Dict of one-hot inputs {0: x_0, 1: x_1, ...}\n","        - ht: Dict of hidden states {-1: h_init, 0: h_0, 1: h_1, ...}\n","        - probt: Dict of probability distributions {0: p_0, 1: p_1, ...}\n","        - loss: Total cross-entropy loss across all timesteps (scalar)\n","    \"\"\"\n","    # Initialize storage dictionaries\n","    xt = {}  # Store one-hot encoded inputs\n","    ut = {}  # Store update gate values\n","    rt = {}  # Store reset gate values\n","    cht = {} # Store chs values\n","    ht = {}  # Store hidden states\n","    yt = {}  # Store logits\n","    probt = {}  # Store probability distributions (after softmax)\n","\n","    # Set initial hidden and cell state\n","    ht[-1] = np.copy(h_prev) #(H,1)\n","    loss = 0\n","    # Loop through each timestep in the sequence\n","    for t in range(len(inputs)):\n","        # Step 1: Convert character index to one-hot vector\n","        xt[t] = np.zeros((vocab_size, 1)) #(V,1)\n","        xt[t][inputs[t]] = 1  # Set the position of char to 1, (V,1)\n","        # Step 2: Run GRU cell (forward computation)\n","        ut[t], rt[t], cht[t], ht[t], yt[t] = gru(ht[t-1], xt[t])\n","        # Step 3: Apply softmax\n","        probt[t] = softmax(yt[t])\n","        # Step 3: Compute loss for this timestep\n","        # Cross-entropy: -log(probability of correct next character)\n","        loss += -np.log(probt[t][targets[t], 0]+ epsilon)\n","\n","    cache = (h_prev, ut, rt, cht)\n","    return xt, ht, probt, loss, cache"],"metadata":{"id":"ErpAgrEEEpld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __BACKWARD PASS (BPTT)__\n","**Complete GRU Backpropagation Through Time Equations**\n","\n","#### **Step 1: Output Layer Gradient (Softmax + Cross-Entropy)**\n","\n","$$\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} = \\mathbf{p}_t - \\mathbf{1}_{y^*_t}$$\n","\n","Where $\\mathbf{p}_t$ is the predicted probability distribution and $\\mathbf{1}_{y^*_t}$ is the one-hot vector of the true label.\n","\n","**Output layer weight gradients:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\mathbf{h}_t^T$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t}$$\n","\n","#### **Step 2: Hidden State Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} = \\mathbf{W}_y^T \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}}$$\n","\n","The gradient flows from two sources:\n","- Current timestep's output loss (first term)\n","- Future timestep's hidden state (second term, initialized as zeros for the last timestep)\n","\n","#### **Step 3: Candidate Hidden State Gradients**\n","\n","Recall: $\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{u}_t \\odot \\tilde{\\mathbf{h}}_t$\n","\n","**Gradient w.r.t. candidate hidden state (after tanh):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{h}}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot \\mathbf{u}_t$$\n","\n","**Key Insight:** Only the portion gated by $\\mathbf{u}_t$ contributes to $\\tilde{\\mathbf{h}}_t$.\n","\n","**Gradient w.r.t. candidate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{h}}_t} \\odot (1 - \\tilde{\\mathbf{h}}_t^2)$$\n","\n","(Using tanh derivative: $\\tanh'(z) = 1 - \\tanh^2(z)$)\n","\n","#### **Step 4: Update Gate Gradients**\n","\n","Recall: $\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{u}_t \\odot \\tilde{\\mathbf{h}}_t$\n","\n","**Gradient w.r.t. update gate (after sigmoid):**\n","\n","Taking the derivative:\n","$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{u}_t} = -\\mathbf{h}_{t-1} + \\tilde{\\mathbf{h}}_t = \\tilde{\\mathbf{h}}_t - \\mathbf{h}_{t-1}$$\n","\n","Therefore:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (\\tilde{\\mathbf{h}}_t - \\mathbf{h}_{t-1})$$\n","\n","**Gradient w.r.t. update gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_t} \\odot \\mathbf{u}_t \\odot (1 - \\mathbf{u}_t)$$\n","\n","(Using sigmoid derivative: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$)\n","\n","#### **Step 5: Reset Gate Gradients**\n","\n","Recall: $\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_h[\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}; \\mathbf{x}_t] + \\mathbf{b}_h)$\n","\n","The reset gate affects the candidate computation through the concatenated input.\n","\n","**Gradient w.r.t. concatenated input for candidate:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial [\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}; \\mathbf{x}_t]} = \\mathbf{W}_h^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}$$\n","\n","**Extract gradient for reset-gated hidden state (first $H$ elements):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})} = \\left[\\mathbf{W}_h^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}\\right]_{[0:H]}$$\n","\n","**Gradient w.r.t. reset gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{r}_t} = \\frac{\\partial \\mathcal{L}}{\\partial (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})} \\odot \\mathbf{h}_{t-1}$$\n","\n","**Gradient w.r.t. reset gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{r}_t} \\odot \\mathbf{r}_t \\odot (1 - \\mathbf{r}_t)$$\n","\n","#### **Step 6: Weight Matrix Gradients (All Gates)**\n","\n","For update gate, reset gate, and candidate computation:\n","\n","**Update Gate:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_u} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u} [\\mathbf{h}_{t-1}; \\mathbf{x}_t]^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_u} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}$$\n","\n","**Reset Gate:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_r} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r} [\\mathbf{h}_{t-1}; \\mathbf{x}_t]^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_r} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}$$\n","\n","**Candidate Hidden State:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_h} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} [\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}; \\mathbf{x}_t]^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_h} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}$$\n","\n","#### **Step 7: Gradient to Previous Hidden State**\n","\n","The gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}$ flows through **four pathways**:\n","\n","**1. Through Update Gate Input:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(u)} = \\left[\\mathbf{W}_u^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}\\right]_{[0:H]}$$\n","\n","**2. Through Reset Gate Input:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(r)} = \\left[\\mathbf{W}_r^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}\\right]_{[0:H]}$$\n","\n","**3. Through Candidate Computation (gated by reset):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(h)} = \\left[\\mathbf{W}_h^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}\\right]_{[0:H]} \\odot \\mathbf{r}_t$$\n","\n","**4. Direct path through hidden state update:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(\\text{direct})} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (1 - \\mathbf{u}_t)$$\n","\n","**Total gradient to previous hidden state:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(u)} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(r)} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(h)} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(\\text{direct})}$$\n","\n","**Key Insight:** The direct pathway $(1 - \\mathbf{u}_t) \\odot \\mathbf{h}_{t-1}$ allows gradients to flow unimpeded when the update gate is closed ($\\mathbf{u}_t \\approx 0$), mitigating vanishing gradients!\n","\n","#### **Step 8: Gradient Clipping**\n","\n","To prevent exploding gradients, clip all parameter gradients:\n","\n","$$\\text{clip}(\\nabla \\theta, -\\tau, \\tau)$$\n","\n","Common clipping threshold: $\\tau = 5$\n","\n","**Notation:**\n","- $\\odot$ = element-wise multiplication\n","- $\\sigma$ = sigmoid function\n","- $T$ = sequence length\n","- $H$ = hidden dimension\n","- $V$ = vocabulary size\n","- $[\\cdot; \\cdot]$ = concatenation along dimension 0"],"metadata":{"id":"7CTgWopPOAka"}},{"cell_type":"code","source":["def backward(inputs, targets, xt, ht, probt, cache):\n","    \"\"\"\n","    Backpropagation Through Time (BPTT) for GRU\n","\n","    Inputs:\n","        - inputs: List of input character indices\n","        - targets: List of target character indices\n","        - xt: Dict of one-hot inputs from forward pass\n","        - ht: Dict of hidden states from forward pass\n","        - probt: Dict of probability distributions from forward pass\n","        - cache: Tuple (h_prev, ut, rt, cht) from forward pass\n","\n","    Returns:\n","        - dWu, dWr, dWh: Gradients for gate weight matrices\n","        - dWy: Gradient w.r.t. hidden-to-output weights\n","        - dbu, dbr, dbh: Gradients for gate biases\n","        - dby: Gradient w.r.t. output bias\n","    \"\"\"\n","    h_prev, ut, rt, cht = cache\n","    dWu = np.zeros_like(Wu)  # (H, H+V)\n","    dWr = np.zeros_like(Wr)  # (H, H+V)\n","    dWh = np.zeros_like(Wh)  # (H, H+V)\n","    dWy = np.zeros_like(Wy)  # (V, H)\n","\n","    dbu = np.zeros_like(bu)  # (H, 1)\n","    dbr = np.zeros_like(br)  # (H, 1)\n","    dbh = np.zeros_like(bh)  # (H, 1)\n","    dby = np.zeros_like(by)  # (V, 1)\n","\n","    # Gradient of hidden state at next timestep (initially zero)\n","    dh_next = np.zeros_like(ht[0])  # (H, 1)\n","\n","    # Backpropagate through time (from last to first timestep)\n","    for t in reversed(range(len(inputs))):\n","        # Step 1: Gradient of loss w.r.t output probabilities\n","        dy = np.copy(probt[t])  # (V, 1)\n","        dy[targets[t]] -= 1  # Subtract 1 from correct class (cross-entropy gradient)\n","\n","        # Step 2: Gradients for output layer\n","        dWy += np.dot(dy, ht[t].T)  # (V,1)@(1,H) = (V,H)\n","        dby += dy  # (V, 1)\n","\n","        # Step 3: Gradient w.r.t hidden state\n","        # Comes from two sources: current output and next timestep\n","        dh = np.dot(Wy.T, dy) + dh_next  # (H,V)@(V,1) + (H,1) = (H,1)\n","\n","        # Step 4: Gradient w.r.t candidate hidden state\n","        dcht = dh * ut[t]  # (H,1) * (H,1) = (H,1)\n","        dzh = dcht * (1 - cht[t]**2)  # (H,1)\n","\n","\n","        # Step 5: Gradient w.r.t update gate\n","        du = dh * (cht[t] - ht[t-1])  # (H,1) * (H,1) = (H,1)\n","        dzu = du * ut[t] * (1 - ut[t])  # (H,1)\n","\n","        # Step 6: Gradient w.r.t reset gate\n","        dzt_h = np.dot(Wh.T, dzh)  # (H+V,H)@(H,1) = (H+V,1)\n","        dr_times_hprev = dzt_h[:hidden_size, :]  # First H elements: (H,1)\n","        dr = dr_times_hprev * ht[t-1]  # (H,1) * (H,1) = (H,1)\n","        dzr = dr * rt[t] * (1 - rt[t])  # (H,1)\n","\n","        # Step 7: Create concatenated inputs for gradient computation\n","        zt = np.concatenate((ht[t-1], xt[t]), axis=0)  # (H+V, 1)\n","        zt_h = np.concatenate((rt[t] * ht[t-1], xt[t]), axis=0)  # (H+V, 1)\n","\n","        # Step 8: Gradients for weight matrices and biases\n","        dWu += np.dot(dzu, zt.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbu += dzu  # (H,1)\n","\n","        dWr += np.dot(dzr, zt.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbr += dzr  # (H,1)\n","\n","        dWh += np.dot(dzh, zt_h.T)  # (H,1)@(1,H+V) = (H,H+V)\n","        dbh += dzh  # (H,1)\n","\n","        # Step 9: Gradient w.r.t previous hidden state computation\n","        dh_from_zu = np.dot(Wu.T, dzu)[:hidden_size, :]  # (H+V,H)@(H,1)->(H+V,1)->(H,1)\n","        dh_from_zr = np.dot(Wr.T, dzr)[:hidden_size, :]  # (H,1)\n","        dh_from_zh = np.dot(Wh.T, dzh)[:hidden_size, :] * rt[t]  # (H,1) * (H,1)\n","        dh_from_ht = dh * (1 - ut[t])  # from ht = (1-ut)*h_prev + ut*cht\n","\n","        dh_next = dh_from_zu + dh_from_zr + dh_from_zh + dh_from_ht  # (H,1)\n","\n","    for dparam in [dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby]:\n","        np.clip(dparam, -5, 5, out=dparam)\n","\n","    return dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby"],"metadata":{"id":"07BaraxIyIst"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding GRU BPTT: The Gradient Flow**\n","\n","**1. The Softmax-CrossEntropy Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} = \\mathbf{p} - \\mathbf{1}_{y^*}$$\n","\n","**2. GRU Gradient Flow - Four Pathways**\n","\n","Unlike vanilla RNNs, GRU gradients flow through **FOUR paths** to previous hidden state:\n","\n","**Path 1 (Update Gate Input):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(u)} = \\left[\\mathbf{W}_u^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}\\right]_{[0:H]}$$\n","\n","**Path 2 (Reset Gate Input):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(r)} = \\left[\\mathbf{W}_r^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}\\right]_{[0:H]}$$\n","\n","**Path 3 (Candidate Computation - Gated by Reset):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(h)} = \\left[\\mathbf{W}_h^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}\\right]_{[0:H]} \\odot \\mathbf{r}_t$$\n","\n","**Path 4 (Direct Highway):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(\\text{direct})} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (1 - \\mathbf{u}_t)$$\n","\n","**The Magic of Path 4:**\n","- **Only element-wise multiplication** by $(1-\\mathbf{u}_t)$!\n","- If $\\mathbf{u}_t \\approx 0$, gradients flow unimpeded (gradient = 1)\n","- This is GRU's \"shortcut connection\" - similar to LSTM's cell state highway!\n","\n","**3. Why GRU Solves Vanishing Gradients**\n","\n","**Vanilla RNN:**\n","$$\\frac{\\partial \\mathbf{h}_0}{\\partial \\mathbf{h}_T} = \\prod_{t=1}^{T} \\mathbf{W}_{hh}^T \\cdot \\text{diag}(1-\\mathbf{h}_t^2)$$\n","- Product of many matrices → exponential decay\n","\n","**GRU Direct Path:**\n","$$\\frac{\\partial \\mathbf{h}_0}{\\partial \\mathbf{h}_T}^{(\\text{direct})} = \\prod_{t=1}^{T} (1 - \\mathbf{u}_t)$$\n","- Only element-wise products → gradients can flow for 100+ timesteps!"],"metadata":{"id":"Spgg8FYXLqKl"}},{"cell_type":"markdown","source":["### __UPDATE PARAMS WITH ADAM__"],"metadata":{"id":"whAHx2WTW03k"}},{"cell_type":"code","source":["def update_parameters(dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby, learning_rate):\n","    \"\"\"\n","    Update GRU model parameters using Adam optimizer with bias correction\n","\n","    Inputs:\n","        - dWu: Gradient for Update gate weights (H, H+V)\n","        - dWr: Gradient for Reset gate weights (H, H+V)\n","        - dWh: Gradient for cell Hidden weights (H, H+V)\n","        - dWy: Gradient for prediction weights (V, H)\n","        - dbu, dbr, dbh: Gradients for gate biases (H, 1)\n","        - dby: Gradient for output bias (V, 1)\n","        - learning_rate: Step size for parameter updates\n","\n","    Returns:\n","        - None (updates global parameters in-place)\n","    \"\"\"\n","    global Wu, Wr, Wh, Wy, bu, br, bh, by\n","    global mWu, mWr, mWh, mWy, mbu, mbr, mbh, mby\n","    global vWu, vWr, vWh, vWy, vbu, vbr, vbh, vby\n","    global t_adam\n","\n","    t_adam += 1\n","    params = [\n","        (Wu, dWu, mWu, vWu),\n","        (Wr, dWr, mWr, vWr),\n","        (Wh, dWh, mWh, vWh),\n","        (Wy, dWy, mWy, vWy),\n","        (bu, dbu, mbu, vbu),\n","        (br, dbr, mbr, vbr),\n","        (bh, dbh, mbh, vbh),\n","        (by, dby, mby, vby)\n","    ]\n","    updated = []\n","    for param, grad, m, v in params:\n","        m = beta1 * m + (1 - beta1) * grad\n","        v = beta2 * v + (1 - beta2) * (grad ** 2)\n","        # Bias correction\n","        m_corrected = m / (1 - beta1 ** t_adam)\n","        v_corrected = v / (1 - beta2 ** t_adam)\n","        param = param - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n","        updated.append((param, m, v))\n","\n","    (Wu, mWu, vWu), (Wr, mWr, vWr), (Wh, mWh, vWh), (Wy, mWy, vWy),\\\n","    (bu, mbu, vbu), (br, mbr, vbr), (bh, mbh, vbh), (by, mby, vby) = updated"],"metadata":{"id":"wSGIUqZsMU0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __TRAIN MODEL__\n"],"metadata":{"id":"hpHLQqh7XFil"}},{"cell_type":"code","source":["def train(data, num_iterations=1000, print_every=100, sample_every=100):\n","    \"\"\"\n","    Train GRU language model\n","\n","    Inputs:\n","        - data: String of training text\n","        - num_iterations: Number of training iterations (not full data passes)\n","        - print_every: Print loss every N iterations\n","        - sample_every: Generate sample text every N iterations\n","\n","    Returns:\n","        - smooth_loss: Exponentially smoothed loss value\n","\n","    Note: Hidden state h_prev is maintained across sequences for temporal\n","        continuity. Only reset when reaching end of data or on first iteration.\n","    \"\"\"\n","    smooth_loss = -np.log(1.0 / vocab_size) * seq_len\n","    n = 0  # Iterations counter\n","    p = 0  # Data pointer (position in text)\n","    h_prev = np.zeros((hidden_size, 1))\n","\n","    while n < num_iterations:\n","        # Reset pointer, hidden state, AND cell state at end of data or first iteration\n","        if p + seq_len + 1 >= len(data) or n == 0:\n","            h_prev = np.zeros((hidden_size, 1))  # Fresh start\n","            p = 0  # Go back to beginning\n","\n","        # Input:  characters at positions [p, p+1, ..., p+seq_len-1]\n","        # Target: characters at positions [p+1, p+2, ..., p+seq_len]\n","        inputs = [char_to_ix[ch] for ch in data[p:p+seq_len]]\n","        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_len+1]]\n","        # Forward pass\n","        xt, ht, probt, loss, cache = forward(inputs, targets, h_prev)\n","        # Update hidden state for next sequence\n","        h_prev = np.copy(ht[len(inputs) - 1])\n","        # Backward pass\n","        dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby = backward(inputs, targets, xt, ht, probt, cache)\n","        # Update parameters\n","        update_parameters(dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby, lr)\n","        # Update smooth loss\n","        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","        # Print progress\n","        if n % print_every == 0:\n","            print(f\"Iter {n:6d} | Loss: {smooth_loss:.4f}\")\n","        # Generate sample text\n","        if n % sample_every == 0 and n > 0:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"SAMPLE at iteration {n}:\")\n","            print(f\"{'='*60}\")\n","            sample_text = sample(data[0], n_chars=200)\n","            print(sample_text)\n","            print(f\"{'='*60}\\n\")\n","        p += seq_len\n","        n += 1\n","\n","    print(\"\\nTraining complete!\")\n","    return smooth_loss"],"metadata":{"id":"HvrBIPp3XAt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training Deep Dive: What's Really Happening?**\n","\n","**The GRU Training Loop Flow:**\n","\n","```\n","Initialize: h = zeros(100, 1)\n","Iteration 1:\n","Data: \"Once upon a time, on a ve\"\n","    → Forward: Get loss, store ALL states (h, u_t, r_t, cht)\n","    → h_prev = h[24]  ← Keep hidden state for next iteration!\n","    → Backward: Compute gradients for 8 parameters (Wu, Wr, Wh, Wy, bu, br, bh, by)\n","    → Adam: Update all parameters\n","Iteration 2:\n","Data: \"ry hot day, a thirsty cro\"\n","    → Forward with h_prev from iteration 1  ← STATE CONTINUITY!\n","    → ...\n","```\n","\n","**Why GRU Training Works:**\n","\n","1. **Single state continuity**: `h_prev` maintained across sequences (simpler than LSTM's dual state)\n","2. **Direct gradient highway**: $(1-\\mathbf{u}_t)$ path enables learning dependencies 100+ timesteps away\n","3. **Adaptive gating**: Update gate decides keep vs update; reset gate controls candidate computation\n","4. **Efficient computation**: 25% fewer parameters than LSTM, faster forward/backward passes\n","5. **Comparable performance**: Empirically matches LSTM on most tasks despite simpler architecture\n","\n","**Critical for GRU:** Must maintain `h_prev` across sequences. Resetting breaks temporal continuity and long-term learning!"],"metadata":{"id":"xqbJ8Rh_MljU"}},{"cell_type":"markdown","source":["### __SAMPLING FUNCTION (GENERATE TEXT)__"],"metadata":{"id":"j4GOFoOuYzvD"}},{"cell_type":"code","source":["def sample(seed_char, n_chars=100):\n","    \"\"\"\n","    Generate text by sampling from the trained GRU language model\n","\n","    Inputs:\n","        - seed_char: Starting character for text generation\n","        - n_chars: Number of characters to generate (default 100)\n","\n","    Returns:\n","        - String of generated text starting with seed_char\n","\n","    Note: Uses stochastic sampling from probability distribution (not argmax).\n","        GRU uses only hidden state h (no cell state needed).\n","    \"\"\"\n","    if seed_char not in char_to_ix:\n","        if seed_char.lower() in char_to_ix:\n","            seed_char = seed_char.lower()\n","        else:\n","            seed_char = data[0]\n","            print(f\"Warning: Seed not in vocab. Using '{seed_char}' instead\")\n","\n","    x = np.zeros((vocab_size, 1)) # Convert seed character to one-hot\n","    x[char_to_ix[seed_char]] = 1\n","    h = np.zeros((hidden_size, 1)) # Initialize hidden state\n","    generated_chars = [seed_char] # Store generated characters\n","\n","    for _ in range(n_chars):\n","        _, _, _, h, y = gru(h, x)\n","        p = softmax(y)\n","        ix = np.random.choice(range(vocab_size), p=p.ravel())\n","        char = ix_to_char[ix]\n","        generated_chars.append(char)\n","        x = np.zeros((vocab_size, 1))\n","        x[ix] = 1\n","\n","    return ''.join(generated_chars)"],"metadata":{"id":"ldl7q4JsYWJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __RUN TRAINING__"],"metadata":{"id":"1y0M3vMIZcjn"}},{"cell_type":"code","source":["final_loss = train(data, num_iterations=10000, print_every=500, sample_every=1000)\n","print(f\"\\nFinal smooth loss: {final_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjfsldMiZVna","outputId":"7f78e228-4d94-4dc4-8e40-954445d9fec4","executionInfo":{"status":"ok","timestamp":1763525038027,"user_tz":-300,"elapsed":69500,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter      0 | Loss: 87.4127\n","Iter    500 | Loss: 80.8953\n","Iter   1000 | Loss: 68.8993\n","\n","============================================================\n","SAMPLE at iteration 1000:\n","============================================================\n","Othe, ntitk..THe Thma!H the troup The pitey ingot he wat ar fole yiwle ther inle, fot row doy,als totel cimend thel. SoiiknS Aledr muas.eh ant yimk linker wate !iqmveitg wator tiikgh fnew an he ahe she\n","============================================================\n","\n","Iter   1500 | Loss: 54.6856\n","Iter   2000 | Loss: 40.7877\n","\n","============================================================\n","SAMPLE at iteration 2000:\n","============================================================\n","Opthe crow was feea tiye, and stintil cringh  pont. chow thorght dot anling the pioc, a vere bywan ar a tireig.\n","The soor intos the water.\n","He uldine. Then ed atly piokly flewndad on a ver. Thins uig sea\n","============================================================\n","\n","Iter   2500 | Loss: 28.5968\n","Iter   3000 | Loss: 19.2507\n","\n","============================================================\n","SAMPLE at iteration 3000:\n","============================================================\n","Ong ur idea!\n","\n","He was ane yeach it.\n","\n","The criwly low was feeling tired and ftly naw a thom, tid hear ther ant meghad one ap hig ferw a bomtil ided fllwl ines into therted anued dropping bnga waten an tre\n","============================================================\n","\n","Iter   3500 | Loss: 13.2632\n","Iter   4000 | Loss: 8.5683\n","\n","============================================================\n","SAMPLE at iteration 4000:\n","============================================================\n","Ong w ar w on ary hag das hew oo anme. \n","Atll w a vere tot and the orow and arlthe how a thee. He criw though dhin leswasth frew aidhit he crow thought for a moment. Then height hte und s ore low ooe aa\n","============================================================\n","\n","Iter   4500 | Loss: 5.5114\n","Iter   5000 | Loss: 5.8293\n","\n","============================================================\n","SAMPLE at iteration 5000:\n","============================================================\n","Once upon a time, on a vely hot day, a thirsty crow thought for a moment. Then he got and the pooresuig sonea thime, the crow fingil ig a lypld shinst ued andide, the crow finelying forea ceast, the cr\n","============================================================\n","\n","Iter   5500 | Loss: 3.9463\n","Iter   6000 | Loss: 2.5934\n","\n","============================================================\n","SAMPLE at iteration 6000:\n","============================================================\n","Once upon a time, und wed pighiegho poter began to rise. The crow comtinde pnder a tree. He quickly flew down and looked inside. There was a little water an ther ad a mon the water at the bottom, but h\n","============================================================\n","\n","Iter   6500 | Loss: 1.7052\n","Iter   7000 | Loss: 1.1299\n","\n","============================================================\n","SAMPLE at iteration 7000:\n","============================================================\n","Oncr upithe water.\n","He felt refreshed and flew away happily hit heach ith.\n","\n","At laststthe bottom, drok Hirea cAot flying in search of water. TAe sun one untid flyw gaw a the water.\n","He felt refreshed and \n","============================================================\n","\n","Iter   7500 | Loss: 0.7566\n","Iter   8000 | Loss: 0.5121\n","\n","============================================================\n","SAMPLE at iteration 8000:\n","============================================================\n","Once upon a very, Ha was flying srow w a lomtily hot hiscbe, in a me crow was flying stones until the water came up high enough.\n","\n","At last, the crow could drink the water.\n","He felt refreshed and flew awa\n","============================================================\n","\n","Iter   8500 | Loss: 0.3504\n","Iter   9000 | Loss: 2.3911\n","\n","============================================================\n","SAMPLE at iteration 9000:\n","============================================================\n","Once upon filin sas flying stang tere how and spintme pitcher. Slowly, the cris could dring time, the crow themsidefly hot hes eastime. The crow thoughe water began tome and stone up hishe was an s anl\n","============================================================\n","\n","Iter   9500 | Loss: 2.0182\n","\n","Training complete!\n","\n","Final smooth loss: 1.3406\n"]}]},{"cell_type":"markdown","source":["### __TEST DIFFERENT SEEDS__"],"metadata":{"id":"jhx3NWCEtl0J"}},{"cell_type":"code","source":["seed_chars = ['T', 'A', 'H', 'W', 'I']\n","for char in seed_chars:\n","    generated = sample(char, n_chars=150)\n","    print(char, ':', generated)\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_1JWyvyZi7B","outputId":"2b316653-6698-4d67-bea6-8181b2219e65","executionInfo":{"status":"ok","timestamp":1763525038065,"user_tz":-300,"elapsed":65,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["T : The water began to rise. The crow continued dropping weared into  he sun was shining brightly, and the pior was flying in search of water. The sun was \n","------------------------------------------------------------\n","A : Aw w a wase. The crow continued dropping stones until the water came up high enough.\n","\n","At last, the crow could drink the water.\n","He felt refreshed and fl\n","------------------------------------------------------------\n","H : He was a little water came up high enough.\n","\n","At last, the crow could drink the water.\n","He felt refreshed and flew away happily hot daw and the poor crow \n","------------------------------------------------------------\n","W : wat f ung nit beak doul sha tim , srar he water begtreshof ang the poor crow was feeling tired and weak.\n","\n","After flying fortiryed and flew away happily \n","------------------------------------------------------------\n","I : ing stones until the water came up hish enough.\n","\n","At latt, the poor crow was feeling tired and weak.\n","\n","After flying for a long time, the crow finally saw\n","------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## **SUMMARY**\n","\n","### **Key Takeaways:**\n","1. **Single Hidden State**: GRUs use a single hidden state $\\mathbf{h}_t$ that combines short and long-term memory (unlike LSTM's separate $\\mathbf{c}_t$ and $\\mathbf{h}_t$)\n","2. **Two-Gate Mechanism**: Update and reset gates control information flow efficiently\n","3. **Direct Gradient Highway**: $(1-\\mathbf{u}_t)$ path solves vanishing gradients, enabling learning across 100+ timesteps\n","4. **Parameter Efficiency**: ~25% fewer parameters than LSTM with comparable performance\n","5. **Gradient Clipping Essential**: Clipping to [-5, 5] prevents exploding gradients during BPTT\n","6. **State Continuity Matters**: Maintaining $\\mathbf{h}_t$ across sequences enables long-range learning\n","\n","\n","### **Implementation Highlights:**\n","- **43,533 Parameters**: 3 weight matrices (update, reset, candidate) + prediction layer\n","- **Character-Level Modeling**: One-hot encoding with vocab_size=33\n","- **Sequence Length**: 25 timesteps per training batch\n","- **Hidden Size**: 100 units for memory capacity\n","- **Training Strategy**: Exponential smoothing (0.999) for stable loss monitoring\n","- **Adam Optimizer**: Adaptive learning rates with momentum for each parameter\n","\n","### **GRU vs LSTM Comparison:**\n","\n","| Aspect | GRU | LSTM |\n","|--------|-----|------|\n","| **Gates** | 2 (update, reset) | 3 (forget, input, output) |\n","| **Memory** | Single $\\mathbf{h}_t$ | Dual $\\mathbf{c}_t$ and $\\mathbf{h}_t$ |\n","| **Parameters** | ~43K (25% fewer) | ~57K |\n","| **Computation** | Faster (fewer ops) | Slower (more gates) |\n","| **Gradient Highway** | $(1-\\mathbf{u}_t)$ on hidden | $\\mathbf{f}_t$ on cell state |\n","| **Performance** | Comparable on most tasks | Slightly better on very long sequences |\n","| **Use Case** | General purpose, resource-constrained | Maximum expressiveness needed |\n","\n","### **References:**\n","1. Cho, K., et al. (2014). *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*. EMNLP. [**Original GRU Paper**]\n","2. Chung, J., et al. (2014). *Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling*. arXiv:1412.3555.\n","3. Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation.\n","\n","### **Further Reading:**\n","1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) — Olah (2015) - Visual explanations (covers GRU too)\n","2. [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555) — Chung et al. (2014) - GRU vs LSTM comparison\n","3. [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) — Karpathy (2015) - Character-level modeling\n","4. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) — Vaswani et al. (2017) - Transformers replacing RNNs\n","5. Deep Learning Book, Chapter 10 — Goodfellow et al. (2016) - Comprehensive RNN theory\n","6. [GRU vs LSTM: A Comparison](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) — Practical guide with visualizations\n"],"metadata":{"id":"HhZHDwtPMo4E"}}]}