{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Seq2Seq: Encoder-Decoder with NumPy**\n",
        "\n",
        "## **From Scratch Implementation in NumPy**\n",
        "\n",
        "This notebook implements a **Sequence-to-Sequence (Seq2Seq)** model using the **Encoder-Decoder** architecture following **Cho et el., 2014**. Unlike a standard language model that predicts the next word in a continuous stream, this architecture predicts next sentence given a context sentence.\n",
        "\n",
        "### **Architecture Overview:**\n",
        "1.  **Encoder:** A GRU that digests the input sentence and compresses it into a fixed-size **Context Vector** (Hidden State).\n",
        "2.  **The Bridge:** A learned non-linear layer that transforms the Encoder's final representation into the Decoder's initial state.\n",
        "3.  **Decoder:** A second GRU that generates the target sentence one word at a time, initialized by the Bridge.\n",
        "\n",
        "### **Key Features:**\n",
        "- **Teacher Forcing:** Used during training to stabilize convergence.\n",
        "- **Autoregressive Inference:** Used during prediction (feeding the model's own output back in).\n",
        "- **Parameter Sharing:** Encoder and Decoder share the same Embedding Matrix.\n",
        "\n",
        "---\n",
        "*Notebook by*: Ahmad Raza [@ahmadrazacdx](https://github.com/ahmadrazacdx)<br>\n",
        "*Date: 2025* *License: MIT*"
      ],
      "metadata": {
        "id": "9bhi1NwuXB6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "SgSsJcK83DfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goJyoKz2zrmy",
        "outputId": "239e2802-d3c9-4c1c-883b-ffe57163ae69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 sentences.\n",
            "Example sentences: ['once upon a time, on a very hot day, a thirsty crow was flying in search of water.', 'the sun was shining brightly, and the poor crow was feeling tired and weak.']\n"
          ]
        }
      ],
      "source": [
        "data = open('../data/thirsty_crow.txt', 'r').read().lower()\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', data)\n",
        "sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
        "print(f\"Found {len(sentences)} sentences.\")\n",
        "print(f\"Example sentences: {sentences[:2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Vocab\n",
        "words = []\n",
        "for sent in sentences:\n",
        "    word = re.findall(r\"\\w+|[.,!?'\\\";:]\", sent)\n",
        "    words.extend(word)\n",
        "\n",
        "SOS_TOKEN = '<SOS>'  # Start of Sequence\n",
        "EOS_TOKEN = '<EOS>'  # End of Sequence\n",
        "UNK_TOKEN = '<UNK>'  # Unknown word\n",
        "\n",
        "vocab = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(list(set(words)))\n",
        "vocab_size = len(vocab)\n",
        "word_to_ix = {w: i for i, w in enumerate(vocab)}\n",
        "ix_to_word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "print(f\"Vocab Size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH03BeIm72IZ",
        "outputId": "cb4747c0-014f-45f9-dab4-2038ecf46b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Training Pairs (Encoder(Input), Decoder(Target))  with Teacher Forcing\n",
        "training_pairs = []\n",
        "\n",
        "for i in range(len(sentences) - 1):\n",
        "    src_text = sentences[i]\n",
        "    src_words = re.findall(r\"\\w+|[.,!?'\\\";:]\", src_text)\n",
        "    src_indices = [word_to_ix[w] for w in src_words]\n",
        "\n",
        "    trg_text = sentences[i+1]\n",
        "    trg_words = re.findall(r\"\\w+|[.,!?'\\\";:]\", trg_text)\n",
        "\n",
        "    dec_input = [word_to_ix[SOS_TOKEN]] + [word_to_ix[w] for w in trg_words] # Decoder Input: <SOS> + sentence\n",
        "    dec_target = [word_to_ix[w] for w in trg_words] + [word_to_ix[EOS_TOKEN]] # Decoder Target: sentence + <EOS>\n",
        "\n",
        "    training_pairs.append({\n",
        "        'src': src_indices,\n",
        "        'dec_input': dec_input,\n",
        "        'dec_target': dec_target\n",
        "    })\n",
        "\n",
        "print(f\"\\nExample Pair 0:\")\n",
        "print(f\"Encoder Input (Indices): {training_pairs[0]['src']}\")\n",
        "print(f\"Decoder Input (Indices): {training_pairs[0]['dec_input']}\")\n",
        "print(f\"Decoder Target (Indices): {training_pairs[0]['dec_target']}\")\n",
        "print(f\"Original Source: {sentences[0]}\")\n",
        "print(f\"Original Target: {sentences[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo_joxDV8diN",
        "outputId": "f0032f09-e5e0-4406-c685-b9afa9e867cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Pair 0:\n",
            "Encoder Input (Indices): [54, 85, 6, 78, 4, 53, 6, 86, 39, 22, 4, 6, 76, 21, 87, 32, 41, 65, 52, 88, 5]\n",
            "Decoder Input (Indices): [0, 72, 71, 87, 66, 15, 4, 9, 72, 59, 21, 87, 28, 79, 9, 89, 5]\n",
            "Decoder Target (Indices): [72, 71, 87, 66, 15, 4, 9, 72, 59, 21, 87, 28, 79, 9, 89, 5, 1]\n",
            "Original Source: once upon a time, on a very hot day, a thirsty crow was flying in search of water.\n",
            "Original Target: the sun was shining brightly, and the poor crow was feeling tired and weak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __HYPER-PARAMETERS__"
      ],
      "metadata": {
        "id": "JhOzRBXz414W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3            # Learning rate\n",
        "hidden_size = 100    # Size of hidden state (h)\n",
        "embed_size = 100     # Size of embedding vector (e)\n",
        "MAX_LEN = 25         # Max length for generation\n",
        "clip_value = 5.0     # Gradient clipping threshold"
      ],
      "metadata": {
        "id": "InWw7ZqC3xVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __MODEL PARAMETER INITIALIZATION__\n",
        "\n",
        "**1. Shared Embeddings:**\n",
        "- $\\mathbf{W}_{emb} \\in \\mathbb{R}^{V \\times E}$: Shared lookup table for both source and target words.\n",
        "-  Concatenated input $[\\mathbf{h}_{t-1}; \\mathbf{x}_t]\\in \\mathbb{R}^{H+E}$\n",
        "\n",
        "**2. Encoder Parameters:**\n",
        "- $\\mathbf{W}^{enc}_u, \\mathbf{W}^{enc}_r, \\mathbf{W}^{enc}_h \\in \\mathbb{R}^{H \\times H+E}$: Weights for Update, Reset, and Candidate gates.\n",
        "- $\\mathbf{b}^{enc}_u, \\mathbf{b}^{enc}_r, \\mathbf{b}^{enc}_h \\in \\mathbb{R}^{H \\times 1}$: Biases for the Encoder.\n",
        "\n",
        "**3. The Bridge:**\n",
        "- $\\mathbf{W}_{bridge} \\in \\mathbb{R}^{H \\times H}$: Learns how to translate the Encoder's final thought into the Decoder's starting thought.\n",
        "- $\\mathbf{b}_{bridge} \\in \\mathbb{R}^{H \\times 1}$: Bias for the bridge.\n",
        "\n",
        "**4. Decoder Parameters:**\n",
        "- $\\mathbf{W}^{dec}_u, \\mathbf{W}^{dec}_r, \\mathbf{W}^{dec}_h \\in \\mathbb{R}^{H \\times H+E}$: Weights for the Decoder GRU.\n",
        "- $\\mathbf{b}^{dec}_u, \\mathbf{b}^{dec}_r, \\mathbf{b}^{dec}_h \\in \\mathbb{R}^{H \\times 1}$: Biases for the Decoder.\n",
        "- $\\mathbf{W}_y\\in \\mathbb{R}^{V \\times H}$: Hidden-to-Output weight matrix (Vocabulary projection).\n",
        "- $\\mathbf{b}_y\\in \\mathbb{R}^{V \\times 1}$: Output bias.\n",
        "\n",
        "**Where:**  \n",
        "- $V$ = vocabulary size  \n",
        "- $E$ = embedding dimension (100)  \n",
        "- $H$ = hidden size (100)"
      ],
      "metadata": {
        "id": "eJLFnn5MXpGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Shared Embeddings\n",
        "Wemb = np.random.randn(vocab_size, embed_size) * 0.01 # Word Embeddings (V,E)\n",
        "# 2. Encoder Parameters\n",
        "Wu_enc = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01 # Update Gate weights (H, H+E)\n",
        "Wr_enc = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01 # Reset Gate weights (H, H+E)\n",
        "Wh_enc = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01 # Candidate Hidden weights (H, H+E)\n",
        "\n",
        "bu_enc = np.zeros((hidden_size, 1)) # Update Gate bias (H, 1)\n",
        "br_enc = np.zeros((hidden_size, 1)) # Reset Gate bias (H, 1)\n",
        "bh_enc = np.zeros((hidden_size, 1)) # Candidate Hidden bias (H, 1)\n",
        "\n",
        "# 3. Bridge Paramters\n",
        "W_bridge = np.random.randn(hidden_size, hidden_size) * 0.01 # Bridge Weights (H,H)\n",
        "b_bridge = np.zeros((hidden_size, 1)) # Bridge bias (H,1)\n",
        "\n",
        "# 4. Decoder Parameters\n",
        "Wu_dec = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01 # Update Gate weights (H, H+E)\n",
        "Wr_dec = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01 # Reset Gate weights (H, H+E)\n",
        "Wh_dec = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01 # Candidate Hidden weights (H, H+E)\n",
        "\n",
        "bu_dec = np.zeros((hidden_size, 1)) # Update Gate bias (H, 1)\n",
        "br_dec = np.zeros((hidden_size, 1)) # Reset Gate bias (H, 1)\n",
        "bh_dec = np.zeros((hidden_size, 1)) # Candidate Hidden bias (H, 1)\n",
        "\n",
        "# 5. Output Layer (Only the Decoder makes predictions)\n",
        "Wy = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "by = np.zeros((vocab_size, 1))"
      ],
      "metadata": {
        "id": "e4aMeebN_1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "Wemb: Word Embeddings        : {Wemb.shape}\n",
        "=========================================\n",
        "         ENCODER PARAMS\n",
        "=========================================\n",
        "Wu_enc: Update Gate Weights  : {Wu_enc.shape}\n",
        "Wr_enc: Reset Gate Weights   : {Wr_enc.shape}\n",
        "Wh_enc: CHS Weights          : {Wh_enc.shape}\n",
        "bu_enc: Update Gate bias     : {bu_enc.shape}\n",
        "br_enc: Reset Gate bias      : {br_enc.shape}\n",
        "bh_enc: CHS bias             : {bh_enc.shape}\n",
        "=========================================\n",
        "         BRIDGE PARAMS\n",
        "=========================================\n",
        "W_bridge: Bridge Weights    : {W_bridge.shape}\n",
        "b_bridge: Bridge bias       : {b_bridge.shape}\n",
        "=========================================\n",
        "         DECODER PARAMS\n",
        "=========================================\n",
        "Wu_dec: Update Gate Weights  : {Wu_dec.shape}\n",
        "Wr_dec: Reset Gate Weights   : {Wr_dec.shape}\n",
        "Wh_dec: CHS Weights          : {Wh_dec.shape}\n",
        "bu_dec: Update Gate bias     : {bu_dec.shape}\n",
        "br_dec: Reset Gate bias      : {br_dec.shape}\n",
        "bh_dec: CHS bias             : {bh_dec.shape}\n",
        "=========================================\n",
        "Wy: Prediction Weights       : {Wy.shape}\n",
        "by: Prediction bias          : {by.shape}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HGbspWj7RRE",
        "outputId": "81e8c75e-d458-41f9-a4ff-400d2fbde0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wemb: Word Embeddings        : (90, 100)\n",
            "=========================================\n",
            "         ENCODER PARAMS\n",
            "=========================================\n",
            "Wu_enc: Update Gate Weights  : (100, 200)\n",
            "Wr_enc: Reset Gate Weights   : (100, 200)\n",
            "Wh_enc: CHS Weights          : (100, 200)\n",
            "bu_enc: Update Gate bias     : (100, 1)\n",
            "br_enc: Reset Gate bias      : (100, 1)\n",
            "bh_enc: CHS bias             : (100, 1)\n",
            "=========================================\n",
            "         BRIDGE PARAMS\n",
            "=========================================\n",
            "W_bridge: Bridge Weights    : (100, 100)\n",
            "b_bridge: Bridge bias       : (100, 1)\n",
            "=========================================\n",
            "         DECODER PARAMS\n",
            "=========================================\n",
            "Wu_dec: Update Gate Weights  : (100, 200)\n",
            "Wr_dec: Reset Gate Weights   : (100, 200)\n",
            "Wh_dec: CHS Weights          : (100, 200)\n",
            "bu_dec: Update Gate bias     : (100, 1)\n",
            "br_dec: Reset Gate bias      : (100, 1)\n",
            "bh_dec: CHS bias             : (100, 1)\n",
            "=========================================\n",
            "Wy: Prediction Weights       : (90, 100)\n",
            "by: Prediction bias          : (90, 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __ADAM OPTIMIZER INITIALIZATION__"
      ],
      "metadata": {
        "id": "xvWhMsGbeRjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam hyperparameters\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 1. Embeddings\n",
        "mWemb = np.zeros_like(Wemb); vWemb = np.zeros_like(Wemb)\n",
        "\n",
        "# 2. Encoder\n",
        "mWu_enc = np.zeros_like(Wu_enc); vWu_enc = np.zeros_like(Wu_enc)\n",
        "mWr_enc = np.zeros_like(Wr_enc); vWr_enc = np.zeros_like(Wr_enc)\n",
        "mWh_enc = np.zeros_like(Wh_enc); vWh_enc = np.zeros_like(Wh_enc)\n",
        "mbu_enc = np.zeros_like(bu_enc); vbu_enc = np.zeros_like(bu_enc)\n",
        "mbr_enc = np.zeros_like(br_enc); vbr_enc = np.zeros_like(br_enc)\n",
        "mbh_enc = np.zeros_like(bh_enc); vbh_enc = np.zeros_like(bh_enc)\n",
        "\n",
        "# 3. Bridge\n",
        "mW_bridge = np.zeros_like(W_bridge); vW_bridge = np.zeros_like(W_bridge)\n",
        "mb_bridge = np.zeros_like(b_bridge); vb_bridge = np.zeros_like(b_bridge)\n",
        "\n",
        "# 4. Decoder\n",
        "mWu_dec = np.zeros_like(Wu_dec); vWu_dec = np.zeros_like(Wu_dec)\n",
        "mWr_dec = np.zeros_like(Wr_dec); vWr_dec = np.zeros_like(Wr_dec)\n",
        "mWh_dec = np.zeros_like(Wh_dec); vWh_dec = np.zeros_like(Wh_dec)\n",
        "mbu_dec = np.zeros_like(bu_dec); vbu_dec = np.zeros_like(bu_dec)\n",
        "mbr_dec = np.zeros_like(br_dec); vbr_dec = np.zeros_like(br_dec)\n",
        "mbh_dec = np.zeros_like(bh_dec); vbh_dec = np.zeros_like(bh_dec)\n",
        "\n",
        "# 5. Output Layer\n",
        "mWy = np.zeros_like(Wy); vWy = np.zeros_like(Wy)\n",
        "mby = np.zeros_like(by); vby = np.zeros_like(by)\n",
        "\n",
        "# Timestep counter\n",
        "t_adam = 0"
      ],
      "metadata": {
        "id": "kkRRAXAOCEH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "ehegFzIZmOCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __ENCODER & DECODER GRU CELLS__\n",
        "**Note:** I use two separate GRU cells with distinct weights for the Encoder and Decoder. Both follow standard GRU dynamics, but only the Decoder projects to the vocabulary.\n",
        "\n",
        "**1. Shared Embeddings (Both Cells):**\n",
        "$$\\mathbf{e}_t = \\mathbf{W}_{emb}[word\\_idx]$$\n",
        "\n",
        "\n",
        "#### **A. Encoder GRU**\n",
        "The Encoder updates its hidden state to consume information but does not make predictions.\n",
        "\n",
        "**Update Gate:**\n",
        "$$\\mathbf{z}_u = \\mathbf{W}^{enc}_u[\\mathbf{h}_{t-1}; \\mathbf{e}_t] + \\mathbf{b}^{enc}_u$$\n",
        "$$\\mathbf{u}_t = \\sigma(\\mathbf{z}_u)$$\n",
        "\n",
        "**Reset Gate:**\n",
        "$$\\mathbf{z}_r = \\mathbf{W}^{enc}_r[\\mathbf{h}_{t-1}; \\mathbf{e}_t] + \\mathbf{b}^{enc}_r$$\n",
        "$$\\mathbf{r}_t = \\sigma(\\mathbf{z}_r)$$\n",
        "\n",
        "**Candidate Hidden State:**\n",
        "$$\\mathbf{z}_h =  \\mathbf{W}^{enc}_{h,x} \\mathbf{e}_t  + \\mathbf{r}_t \\odot ( \\mathbf{W}^{enc}_{h,h} \\mathbf{h}_{t-1}) + \\mathbf{b}^{enc}_{h}$$\n",
        "$$\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{z}_h)$$\n",
        "\n",
        "**Final Hidden State:**\n",
        "$$\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\tilde{\\mathbf{h}}_t  + \\mathbf{u}_t \\odot \\mathbf{h}_{t-1}$$\n",
        "\n",
        "\n",
        "#### **B. Decoder GRU**\n",
        "The Decoder updates its hidden state *and* projects it to the vocabulary size to predict the next word.\n",
        "\n",
        "**Update & Reset Gates:**\n",
        "$$\\mathbf{z}_u = \\mathbf{W}^{dec}_u[\\mathbf{h}_{t-1}; \\mathbf{e}_t] + \\mathbf{b}^{dec}_u$$\n",
        "$$\\mathbf{z}_r = \\mathbf{W}^{dec}_r[\\mathbf{h}_{t-1}; \\mathbf{e}_t] + \\mathbf{b}^{dec}_r$$\n",
        "\n",
        "**Candidate & Final Hidden State:**\n",
        "$$\\mathbf{z}_h =  \\mathbf{W}^{dec}_{h,x} \\mathbf{e}_t  + \\mathbf{r}_t \\odot ( \\mathbf{W}^{dec}_{h,h} \\mathbf{h}_{t-1}) + \\mathbf{b}^{dec}_{h}$$\n",
        "$$\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\tilde{\\mathbf{h}}_t  + \\mathbf{u}_t \\odot \\mathbf{h}_{t-1}$$\n",
        "\n",
        "**Output Layer (Projection):**\n",
        "$$\\mathbf{z}_y = \\mathbf{W}_y\\mathbf{h}_t + \\mathbf{b}_y$$\n",
        "$$\\mathbf{p}_t = \\text{softmax}(\\mathbf{z}_y)$$\n",
        "\n",
        "**Note:**\n",
        "This implementation follows encoder-decoder from original paper **(Cho et al., 2014)** but the hidden state equation is according to PyTorch's modified equation that is slightly different.\n",
        "\n",
        "**References**\n",
        "\n",
        "- Cho et al. *Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation*, EMNLP 2014. [https://aclanthology.org/D14-1179/](https://aclanthology.org/D14-1179/)\n",
        "\n",
        "- PyTorch GRUCell documentation. [https://docs.pytorch.org/stable/generated/torch.nn.GRUCell.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html)"
      ],
      "metadata": {
        "id": "tPLcP__rdFLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODER GRU CELL\n",
        "def encoder(h_prev, word_idx):\n",
        "    \"\"\"\n",
        "    Single GRU step for the Encoder. Uses _enc weights and DOES NOT compute output yt.\n",
        "    \"\"\"\n",
        "    et = Wemb[word_idx].reshape(-1, 1)  # (E, 1)\n",
        "    zt = np.concatenate((h_prev, et), axis=0) # (H+E, 1)\n",
        "    zu = np.dot(Wu_enc, zt) + bu_enc #(H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n",
        "    ut = sigmoid(zu) # (H,1)\n",
        "    zr = np.dot(Wr_enc, zt) + br_enc ##(H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n",
        "    rt = sigmoid(zr) # (H,1)\n",
        "    Wh_h = Wh_enc[:, :hidden_size] # (H,H)\n",
        "    Wh_x = Wh_enc[:, hidden_size:] # (H,E)\n",
        "    zcht = np.dot(Wh_x, et) + rt * np.dot(Wh_h, h_prev) + bh_enc #(H,1)\n",
        "    cht = np.tanh(zcht) # (H,1)\n",
        "    ht = (1 - ut) * cht + ut * h_prev # (H,1)*(H,1) + (H,1)*(H,1)=(H,1)\n",
        "    return et, ut, rt, cht, ht\n",
        "\n",
        "# DECODER GRU CELL\n",
        "def decoder(h_prev, word_idx):\n",
        "    \"\"\"\n",
        "    Single GRU step for the Decoder.Uses _dec weights and COMPUTES output yt.\n",
        "    \"\"\"\n",
        "    et = Wemb[word_idx].reshape(-1, 1)\n",
        "    zt = np.concatenate((h_prev, et), axis=0)\n",
        "    zu = np.dot(Wu_dec, zt) + bu_dec\n",
        "    ut = sigmoid(zu)\n",
        "    zr = np.dot(Wr_dec, zt) + br_dec\n",
        "    rt = sigmoid(zr)\n",
        "    Wh_h = Wh_dec[:, :hidden_size]\n",
        "    Wh_x = Wh_dec[:, hidden_size:]\n",
        "    zcht = np.dot(Wh_x, et) + rt * np.dot(Wh_h, h_prev) + bh_dec\n",
        "    cht = np.tanh(zcht)\n",
        "    ht = (1 - ut) * cht + ut * h_prev\n",
        "    yt = np.dot(Wy, ht) + by # #(V,H)@(H,1)->(V,1)+(V,1)=(V,1)\n",
        "    return et, ut, rt, cht, ht, yt"
      ],
      "metadata": {
        "id": "FDO_yCnzC5Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __UNDERSTANDING THE GRU CELLS__\n",
        "\n",
        "Since the architecture is splitted, the two GRU cells behave differently:\n",
        "\n",
        "**1. Encoder GRU (The Reader)**\n",
        "- **Inputs:** Current word embedding $\\mathbf{e}_t$ and Previous hidden state $\\mathbf{h}_{t-1}$.\n",
        "- **Action:** Updates its internal memory to \"understand\" the sequence.\n",
        "- **Outputs:**\n",
        "  1. New hidden state $\\mathbf{h}_t$.\n",
        "  2. Cache values $(\\mathbf{e}_t, \\mathbf{u}_t, \\mathbf{r}_t, \\tilde{\\mathbf{h}}_t)$ for backprop.\n",
        "- **Note:** It does **not** produce output logits ($\\mathbf{y}_t$).\n",
        "\n",
        "**2. Decoder GRU (The Writer)**\n",
        "- **Inputs:** Current word embedding $\\mathbf{e}_t$ (from Teacher Forcing or Prediction) and Previous hidden state $\\mathbf{h}_{t-1}$.\n",
        "- **Action:** Updates memory *and* predicts the next word.\n",
        "- **Outputs:**\n",
        "  1. New hidden state $\\mathbf{h}_t$.\n",
        "  2. **Logits $\\mathbf{y}_t$** (Projected to Vocabulary size).\n",
        "  3. Cache values $(\\mathbf{e}_t, \\mathbf{u}_t, \\mathbf{r}_t, \\tilde{\\mathbf{h}}_t)$."
      ],
      "metadata": {
        "id": "4f5wu1Tj9rbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __FORWARD PASS__\n"
      ],
      "metadata": {
        "id": "JSiqg1u1Fcuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(src_inputs, dec_inputs, dec_targets):\n",
        "    \"\"\"\n",
        "    Forward pass for the Encoder-Decoder Architecture.\n",
        "\n",
        "    Inputs:\n",
        "        - src_inputs: List of indices for Source Sentence (Encoder)\n",
        "        - dec_inputs: List of indices for Decoder Inputs (Teacher Forcing: <SOS> + words)\n",
        "        - dec_targets: List of indices for Decoder Targets (words + <EOS>)\n",
        "\n",
        "    Returns:\n",
        "        - loss: Scalar Cross-Entropy Loss\n",
        "        - enc_cache: Tuple of dictionaries needed for Encoder Backward\n",
        "        - dec_cache: Tuple of dictionaries needed for Decoder Backward\n",
        "    \"\"\"\n",
        "\n",
        "    # ENCODER\n",
        "    enc_et, enc_ut, enc_rt, enc_cht, enc_ht = {}, {}, {}, {}, {}\n",
        "    h_enc_init = np.zeros((hidden_size, 1))\n",
        "    enc_ht[-1] = h_enc_init\n",
        "    # Run Encoder Loop\n",
        "    for t in range(len(src_inputs)):\n",
        "        word_idx = src_inputs[t]\n",
        "        # Run single step\n",
        "        enc_et[t], enc_ut[t], enc_rt[t], enc_cht[t], enc_ht[t] = encoder(enc_ht[t-1], word_idx)\n",
        "\n",
        "    # BRIDGE\n",
        "    h_enc_final = enc_ht[len(src_inputs) - 1] # (H,1)\n",
        "    bridge_z = np.dot(W_bridge, h_enc_final) + b_bridge # (H,H)@(H,1)+(H,1)=(H,1)\n",
        "    h_dec_init = np.tanh(bridge_z) #(H,1)\n",
        "    bridge_cache = (h_enc_final, bridge_z, h_dec_init)\n",
        "\n",
        "    # DECODER\n",
        "    dec_et, dec_ut, dec_rt, dec_cht, dec_ht, dec_yt = {}, {}, {}, {}, {}, {}\n",
        "    dec_probt = {}\n",
        "    dec_ht[-1] = np.copy(h_dec_init)\n",
        "    loss = 0\n",
        "    # Run Decoder Loop\n",
        "    for t in range(len(dec_inputs)):\n",
        "        word_idx = dec_inputs[t]   # Current input (from Teacher Forcing)\n",
        "        target_idx = dec_targets[t] # True label\n",
        "        dec_et[t], dec_ut[t], dec_rt[t], dec_cht[t], dec_ht[t], dec_yt[t] = decoder(dec_ht[t-1], word_idx)\n",
        "        dec_probt[t] = softmax(dec_yt[t])\n",
        "        loss += -np.log(dec_probt[t][target_idx, 0] + epsilon)\n",
        "\n",
        "    enc_cache = (enc_et, enc_ut, enc_rt, enc_cht, enc_ht)\n",
        "    dec_cache = (dec_et, dec_ut, dec_rt, dec_cht, dec_ht, dec_yt, dec_probt)\n",
        "\n",
        "    return loss, enc_cache, bridge_cache,  dec_cache"
      ],
      "metadata": {
        "id": "zLGSwkThGX1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __BACKWARD PASS (BPTT)__\n",
        "\n",
        "**Backpropagation Through Time Equations for Encoder-Decoder Architecture**\n",
        "\n",
        "\n",
        "### **DECODER BACKWARD PASS**\n",
        "\n",
        "#### **Step 1: Output Layer Gradient (Softmax + Cross-Entropy)**\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} = \\mathbf{p}_t - \\mathbf{1}_{y^*_t}$$\n",
        "\n",
        "**Output layer weight gradients:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_y} = \\sum_{t=0}^{T_{dec}-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\mathbf{h}_t^{dec, T}$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T_{dec}-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t}$$\n",
        "\n",
        "#### **Step 2: Decoder Hidden State Gradient**\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t^{dec}} = \\mathbf{W}_y^T \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}^{dec}}$$\n",
        "\n",
        "The gradient flows from two sources:\n",
        "- Current timestep's output loss (first term)\n",
        "- Future timestep's hidden state (second term, initialized as zeros for the last timestep)\n",
        "\n",
        "\n",
        "### **GRU GATE GRADIENTS (Applied to both Encoder & Decoder)**\n",
        "\n",
        "**Note:** The following gradient computations (Steps 3-7) apply to both the Encoder and Decoder GRUs. For clarity:\n",
        "- **Decoder**: Use $\\mathbf{h}^{dec}$, $\\mathbf{u}^{dec}$, $\\mathbf{r}^{dec}$, $\\tilde{\\mathbf{h}}^{dec}$, and corresponding weight matrices $\\mathbf{W}_u^{dec}$, $\\mathbf{W}_r^{dec}$, $\\mathbf{W}_h^{dec}$\n",
        "- **Encoder**: Use $\\mathbf{h}^{enc}$, $\\mathbf{u}^{enc}$, $\\mathbf{r}^{enc}$, $\\tilde{\\mathbf{h}}^{enc}$, and corresponding weight matrices $\\mathbf{W}_u^{enc}$, $\\mathbf{W}_r^{enc}$, $\\mathbf{W}_h^{enc}$\n",
        "\n",
        "For brevity, I write equations without superscripts below, understanding they apply to whichever RNN is being processed.\n",
        "\n",
        "#### **Step 3: Update Gate Gradients**\n",
        "\n",
        "**Gradient w.r.t. update gate (after sigmoid):**\n",
        "$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{u}_t} = -\\tilde{\\mathbf{h}}_t + \\mathbf{h}_{t-1} = \\mathbf{h}_{t-1} - \\tilde{\\mathbf{h}}_t$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (\\mathbf{h}_{t-1} - \\tilde{\\mathbf{h}}_t)$$\n",
        "\n",
        "**Gradient w.r.t. update gate pre-activation:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_t} \\odot \\mathbf{u}_t \\odot (1 - \\mathbf{u}_t)$$\n",
        "\n",
        "#### **Step 4: Candidate Hidden State Gradients**\n",
        "\n",
        "**Gradient w.r.t. candidate hidden state (after tanh):**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{h}}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (1 - \\mathbf{u}_t)$$\n",
        "\n",
        "**Key Insight:** The coefficient of $\\tilde{\\mathbf{h}}_t$ in the hidden state equation is $(1 - \\mathbf{u}_t)$.\n",
        "\n",
        "**Gradient w.r.t. candidate pre-activation:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{h}}_t} \\odot (1 - \\tilde{\\mathbf{h}}_t^2)$$\n",
        "\n",
        "#### **Step 5: Reset Gate Gradients**\n",
        "**Gradient w.r.t. reset gate (after sigmoid):**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{r}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\odot (\\mathbf{W}_{h,h} \\mathbf{h}_{t-1})$$\n",
        "\n",
        "**Key Insight:** The reset gate multiplies the hidden contribution $\\mathbf{W}_{h,h} \\mathbf{h}_{t-1}$, so its gradient is the element-wise product with this term.\n",
        "\n",
        "**Gradient w.r.t. reset gate pre-activation:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{r}_t} \\odot \\mathbf{r}_t \\odot (1 - \\mathbf{r}_t)$$\n",
        "\n",
        "#### **Step 6: Weight Matrix Gradients**\n",
        "\n",
        "**Update Gate:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_u} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u} [\\mathbf{h}_{t-1}; \\mathbf{e}_t]^T$$\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_u} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}$$\n",
        "\n",
        "**Reset Gate:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_r} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r} [\\mathbf{h}_{t-1}; \\mathbf{e}_t]^T$$\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_r} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}$$\n",
        "\n",
        "**Candidate Hidden State (Split Computation):**\n",
        "\n",
        "For the hidden part $\\mathbf{W}_{h,h}$:\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{h,h}} = \\sum_{t=0}^{T-1} (\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\odot \\mathbf{r}_t) \\mathbf{h}_{t-1}^T$$\n",
        "\n",
        "For the input part $\\mathbf{W}_{h,x}$:\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{h,x}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\mathbf{e}_t^T$$\n",
        "\n",
        "Bias gradient:\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_h} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}$$\n",
        "\n",
        "**Embedding Matrix:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{emb}[i]} \\mathrel{+}= \\sum_{t: w_t=i} \\left[\\mathbf{W}_u^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}\\right]_{[H:]} + \\left[\\mathbf{W}_r^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}\\right]_{[H:]} + \\mathbf{W}_{h,x}^T\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}$$\n",
        "\n",
        "\n",
        "#### **Step 7: Gradient to Previous Hidden State**\n",
        "\n",
        "The gradient flows back to $t-1$ via four paths (same for both Encoder and Decoder logic):\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} =\n",
        "\\underbrace{\\left[\\mathbf{W}_u^T \\delta_u\\right]_{H}}_{\\text{Update}} +\n",
        "\\underbrace{\\left[\\mathbf{W}_r^T \\delta_r\\right]_{H}}_{\\text{Reset}} +\n",
        "\\underbrace{\\mathbf{W}_{h,h}^T (\\delta_h \\odot \\mathbf{r}_t)}_{\\text{Candidate}} +\n",
        "\\underbrace{(\\delta_{\\text{next}} \\odot \\mathbf{u}_t)}_{\\text{Direct}}\n",
        "$$\n",
        "**Where:**\n",
        "$$\n",
        "\\delta_k = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_k} \\quad (\\text{for } k \\in \\{u, r, h\\})\n",
        "$$\n",
        "$$\n",
        "\\delta_{\\text{next}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{\\text{next}}}\n",
        "$$\n",
        "\n",
        "\n",
        "### **BRIDGE BACKWARD PASS**\n",
        "\n",
        "#### **Step 8: Bridge Layer Gradients**\n",
        "\n",
        "The final gradient from the decoder $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{-1}^{dec}}$ (gradient w.r.t. decoder's initial state) flows through the bridge. Recall: $\\mathbf{h}_{-1}^{dec} = \\tanh(\\mathbf{W}_{bridge} \\mathbf{h}_{final}^{enc} + \\mathbf{b}_{bridge})$\n",
        "\n",
        "**Gradient w.r.t. bridge pre-activation:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{bridge}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{-1}^{dec}} \\odot (1 - (\\mathbf{h}_{-1}^{dec})^2)$$\n",
        "\n",
        "**Bridge weight gradients:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{bridge}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{bridge}} (\\mathbf{h}_{final}^{enc})^T$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_{bridge}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{bridge}}$$\n",
        "\n",
        "**Gradient to encoder's final hidden state:**\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{final}^{enc}} = \\mathbf{W}_{bridge}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_{bridge}}$$\n",
        "\n",
        "\n",
        "### **ENCODER BACKWARD PASS**\n",
        "\n",
        "#### **Step 9: Encoder Hidden State Gradient**\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t^{enc}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}^{enc}}$$\n",
        "\n",
        "For the final timestep $T_{enc}-1$, initialize with gradient from bridge:\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{T_{enc}-1}^{enc}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{final}^{enc}}$$\n",
        "\n",
        "**Steps 3-7 from the GRU Gate Gradients section above are applied here for the Encoder**, using $\\mathbf{h}^{enc}$, $\\mathbf{W}_u^{enc}$, $\\mathbf{W}_r^{enc}$, $\\mathbf{W}_h^{enc}$, and accumulating gradients over $t = 0$ to $T_{enc}-1$.\n",
        "\n",
        "\n",
        "#### **Step 10: Gradient Clipping**\n",
        "\n",
        "To prevent exploding gradients, clip all parameter gradients:\n",
        "\n",
        "$$\\text{clip}(\\nabla \\theta, -\\tau, \\tau)$$\n",
        "\n",
        "Common clipping threshold: $\\tau = 5$\n",
        "\n",
        "\n",
        "**Notation:**\n",
        "- $T_{enc}$ = encoder sequence length\n",
        "- $T_{dec}$ = decoder sequence length\n",
        "- $T$ = sequence length (generic, when referring to either encoder or decoder)\n",
        "- $H$ = hidden dimension\n",
        "- $E$ = embedding dimension\n",
        "- $V$ = vocabulary size\n",
        "- $\\mathbf{W}_{h,h}$ = hidden part of $\\mathbf{W}_h$ (first $H$ columns)\n",
        "- $\\mathbf{W}_{h,x}$ = input part of $\\mathbf{W}_h$ (last $E$ columns)\n",
        "- $[\\cdot; \\cdot]$ = concatenation along dimension 0\n",
        "- Superscripts $enc$ and $dec$ denote encoder and decoder components respectively when disambiguation is needed"
      ],
      "metadata": {
        "id": "DjyNlnLGZPlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(src_inputs, dec_inputs, dec_targets, enc_cache,\n",
        "            bridge_cache, dec_cache):\n",
        "    \"\"\"\n",
        "    Backpropagation through Time BPTT for Encoder-Decoder Architecture (Decoder -> Bridge -> Encoder)\n",
        "    \"\"\"\n",
        "    # Unpack caches\n",
        "    enc_et, enc_ut, enc_rt, enc_cht, enc_ht = enc_cache\n",
        "    h_enc_final, bridge_z, h_dec_init = bridge_cache\n",
        "    dec_et, dec_ut, dec_rt, dec_cht, dec_ht, dec_yt, dec_probt = dec_cache\n",
        "\n",
        "    # Initialize Gradients\n",
        "    dWemb = np.zeros_like(Wemb) # Shared embeddings\n",
        "    # Encoder\n",
        "    dWu_enc, dWr_enc, dWh_enc = np.zeros_like(Wu_enc), np.zeros_like(Wr_enc), np.zeros_like(Wh_enc)\n",
        "    dbu_enc, dbr_enc, dbh_enc = np.zeros_like(bu_enc), np.zeros_like(br_enc), np.zeros_like(bh_enc)\n",
        "    # Bridge\n",
        "    dW_bridge = np.zeros_like(W_bridge)\n",
        "    db_bridge = np.zeros_like(b_bridge)\n",
        "    # Decoder\n",
        "    dWu_dec, dWr_dec, dWh_dec = np.zeros_like(Wu_dec), np.zeros_like(Wr_dec), np.zeros_like(Wh_dec)\n",
        "    dbu_dec, dbr_dec, dbh_dec = np.zeros_like(bu_dec), np.zeros_like(br_dec), np.zeros_like(bh_dec)\n",
        "    dWy, dby = np.zeros_like(Wy), np.zeros_like(by)\n",
        "\n",
        "    # DECODER BACKWARD PASS\n",
        "    dh_next = np.zeros_like(dec_ht[0]) # Gradient from future timestep (0 for last step)- (H,1)\n",
        "\n",
        "    for t in reversed(range(len(dec_inputs))):\n",
        "        # A. Gradient of Loss w.r.t Output\n",
        "        dy = np.copy(dec_probt[t]) # (V,1)\n",
        "        dy[dec_targets[t]] -= 1  # (V,1)\n",
        "        dWy += np.dot(dy, dec_ht[t].T) # Accumulate output layer gradients (V,1)@(1,H)=(V,H)\n",
        "        dby += dy #(V,1)\n",
        "\n",
        "        # B. Gradient w.r.t Hidden State\n",
        "        dh = np.dot(Wy.T, dy) + dh_next #(H,V)@(V,1):(H,1)+(H,1)=(H,1)\n",
        "\n",
        "        # C. Compute all gate gradients (GRU)\n",
        "        du = dh * (dec_ht[t-1] - dec_cht[t]) #(H,1)*(H,1)=(H,1)\n",
        "        dzu = du * dec_ut[t] * (1 - dec_ut[t]) # (H,1)*(H,1)=(H,1)\n",
        "\n",
        "        dcht = dh * (1 - dec_ut[t]) # (H,1)*(H,1)=(H,1)\n",
        "        dzh = dcht * (1 - dec_cht[t]**2) #(H,1)*(H,1)=(H,1)\n",
        "\n",
        "        Wh_h_dec = Wh_dec[:, :hidden_size] #(H,H)\n",
        "        Wh_x_dec = Wh_dec[:, hidden_size:] #(H,E)\n",
        "\n",
        "        dr = dzh * np.dot(Wh_h_dec, dec_ht[t-1]) #(H,H)@(H,1):(H,1)*(H,1)=(H,1)\n",
        "        dzr = dr * dec_rt[t] * (1 - dec_rt[t]) #(H,1)*(H,1)8(H,1)=(H,1)\n",
        "\n",
        "        # Accumulate Weight Gradients\n",
        "        z_cat = np.concatenate((dec_ht[t-1], dec_et[t]), axis=0) #(H,1)+(E,1)=(H+E,1)\n",
        "        dWu_dec += np.dot(dzu, z_cat.T) #(H,1)@(1,H+E)=(H,H+E)\n",
        "        dbu_dec += dzu #(H,1)\n",
        "\n",
        "        dWr_dec += np.dot(dzr, z_cat.T) #(H,1)@(1,H+E)=(H,H+E)\n",
        "        dbr_dec += dzr #(H,1)\n",
        "\n",
        "        # Candidate State Weights\n",
        "        dWh_dec[:, :hidden_size] += np.dot(dzh * dec_rt[t], dec_ht[t-1].T) # Hidden part (H,1)*(H,1):(H,1)@(1,H)=(H,H)\n",
        "        dWh_dec[:, hidden_size:] += np.dot(dzh, dec_et[t].T)               # Input part (H,1)@(1,E)=(H,E)\n",
        "        dbh_dec += dzh #(H,1)\n",
        "\n",
        "        # Embeddings Gradient\n",
        "        de = (np.dot(Wu_dec.T, dzu) + np.dot(Wr_dec.T, dzr) + np.dot(Wh_dec.T, dzh))[hidden_size:, :] #(H+E,1)+(H+E,1)+(H+E,1)=(H+E,1):[(H+E)-H,1]=(E,1)\n",
        "        dWemb[dec_inputs[t]] += de.ravel() #(E,)\n",
        "\n",
        "        # Compute dh_next for the previous timestep\n",
        "        dh_from_zu = np.dot(Wu_dec.T, dzu)[:hidden_size, :] #(H+E,H)@(H,1):(H+E,1)=>(H,1)\n",
        "        dh_from_zr = np.dot(Wr_dec.T, dzr)[:hidden_size, :] #(H,1)\n",
        "        dh_from_zh = np.dot(Wh_h_dec.T, dzh * dec_rt[t]) #(H,H)@(H,1):(H,1)\n",
        "        dh_from_direct = dh * dec_ut[t] #(H,1)*(H,1)=(H,1)\n",
        "        dh_next = dh_from_zu + dh_from_zr + dh_from_zh + dh_from_direct #(H,1)\n",
        "\n",
        "    # The final `dh_next` here is the gradient w.r.t the Decoder's Initial State!\n",
        "    d_h_dec_init = dh_next #(H,1)\n",
        "\n",
        "    # BRIDGE BACKWARD PASS\n",
        "    d_bridge_z = d_h_dec_init * (1 - h_dec_init**2) # (H,1)*(H,1)=(H,1)\n",
        "    dW_bridge = np.dot(d_bridge_z, h_enc_final.T) # (H,1)@(1,H)=(H,H)\n",
        "    db_bridge = d_bridge_z #(H,1)\n",
        "    d_h_enc_final = np.dot(W_bridge.T, d_bridge_z) #(H,H)@(H,1)=(H,1)\n",
        "\n",
        "    # ENCODER BACKWARD PASS\n",
        "    dh_next = d_h_enc_final # Initialize with gradient from bridge\n",
        "\n",
        "    for t in reversed(range(len(src_inputs))):\n",
        "        dh = dh_next #(H,1)\n",
        "        # Compute all gate gradients (GRU)\n",
        "        du = dh * (enc_ht[t-1] - enc_cht[t]) #(H,1)\n",
        "        dzu = du * enc_ut[t] * (1 - enc_ut[t]) #(H,1)\n",
        "\n",
        "        dcht = dh * (1 - enc_ut[t]) #(H,1)\n",
        "        dzh = dcht * (1 - enc_cht[t]**2) #(H,1)\n",
        "\n",
        "        Wh_h_enc = Wh_enc[:, :hidden_size] #(H,H)\n",
        "\n",
        "        dr = dzh * np.dot(Wh_h_enc, enc_ht[t-1]) #(H,H)@(H,1):(H,1)*(H,1)=(H,1)\n",
        "        dzr = dr * enc_rt[t] * (1 - enc_rt[t]) #(H,1)\n",
        "\n",
        "        # Accumulate Weight Gradients\n",
        "        z_cat = np.concatenate((enc_ht[t-1], enc_et[t]), axis=0) #(H+E,1)\n",
        "        dWu_enc += np.dot(dzu, z_cat.T) #(H,1)@(1,H+E)=(H,H+E)\n",
        "        dbu_enc += dzu #(H,1)\n",
        "\n",
        "        dWr_enc += np.dot(dzr, z_cat.T) #(H,H+E)\n",
        "        dbr_enc += dzr #(H,1)\n",
        "\n",
        "        dWh_enc[:, :hidden_size] += np.dot(dzh * enc_rt[t], enc_ht[t-1].T) #(H,1)*(H,1):(H,1)@(1,H)=(H,H)\n",
        "        dWh_enc[:, hidden_size:] += np.dot(dzh, enc_et[t].T) #(H,1)@(1,E)=(H,E)\n",
        "        dbh_enc += dzh #(H,1)\n",
        "\n",
        "        # Embeddings Gradient\n",
        "        de = (np.dot(Wu_enc.T, dzu) + np.dot(Wr_enc.T, dzr) + np.dot(Wh_enc.T, dzh))[hidden_size:, :] # (E,1)\n",
        "        dWemb[src_inputs[t]] += de.ravel() #(E,)\n",
        "\n",
        "        # Compute dh_next for previous timestep\n",
        "        dh_from_zu = np.dot(Wu_enc.T, dzu)[:hidden_size, :] #(H,1)\n",
        "        dh_from_zr = np.dot(Wr_enc.T, dzr)[:hidden_size, :] #(H,1)\n",
        "        dh_from_zh = np.dot(Wh_h_enc.T, dzh * enc_rt[t]) #(H,1)\n",
        "        dh_from_direct = dh * enc_ut[t] #(H,1)\n",
        "\n",
        "        dh_next = dh_from_zu + dh_from_zr + dh_from_zh + dh_from_direct #(H,1)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    grads = [dWemb, dWu_enc, dWr_enc, dWh_enc, dWu_dec, dWr_dec, dWh_dec, dWy, dW_bridge, dbu_enc, dbr_enc, dbh_enc, dbu_dec, dbr_dec, dbh_dec, dby, db_bridge]\n",
        "\n",
        "    for grad in grads:\n",
        "        np.clip(grad, -clip_value, clip_value, out=grad)\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "fLl0rNfAecI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __UPDATE PARAMS WITH ADAM__"
      ],
      "metadata": {
        "id": "whAHx2WTW03k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update all Encoder, Bridge, and Decoder parameters using Adam.\n",
        "\n",
        "    Inputs:\n",
        "        - grads: List of gradients returned by backward()\n",
        "        - learning_rate: Scalar float\n",
        "    \"\"\"\n",
        "    # Model weights\n",
        "    global Wemb\n",
        "    global Wu_enc, Wr_enc, Wh_enc, bu_enc, br_enc, bh_enc\n",
        "    global Wu_dec, Wr_dec, Wh_dec, bu_dec, br_dec, bh_dec, Wy, by\n",
        "    global W_bridge, b_bridge\n",
        "\n",
        "    # Adam memory variables (m and v)\n",
        "    global mWemb, vWemb\n",
        "    global mWu_enc, vWu_enc, mWr_enc, vWr_enc, mWh_enc, vWh_enc, mbu_enc, vbu_enc, mbr_enc, vbr_enc, mbh_enc, vbh_enc\n",
        "    global mWu_dec, vWu_dec, mWr_dec, vWr_dec, mWh_dec, vWh_dec, mbu_dec, vbu_dec, mbr_dec, vbr_dec, mbh_dec, vbh_dec, mWy, vWy, mby, vby\n",
        "    global mW_bridge, vW_bridge, mb_bridge, vb_bridge\n",
        "    global t_adam\n",
        "\n",
        "    t_adam += 1\n",
        "    (dWemb,\n",
        "     dWu_enc, dWr_enc, dWh_enc,\n",
        "     dWu_dec, dWr_dec, dWh_dec, dWy,\n",
        "     dW_bridge,\n",
        "     dbu_enc, dbr_enc, dbh_enc,\n",
        "     dbu_dec, dbr_dec, dbh_dec, dby,\n",
        "     db_bridge) = grads\n",
        "\n",
        "    params = [\n",
        "        (Wemb, dWemb, mWemb, vWemb),\n",
        "\n",
        "        (Wu_enc, dWu_enc, mWu_enc, vWu_enc),\n",
        "        (Wr_enc, dWr_enc, mWr_enc, vWr_enc),\n",
        "        (Wh_enc, dWh_enc, mWh_enc, vWh_enc),\n",
        "\n",
        "        (Wu_dec, dWu_dec, mWu_dec, vWu_dec),\n",
        "        (Wr_dec, dWr_dec, mWr_dec, vWr_dec),\n",
        "        (Wh_dec, dWh_dec, mWh_dec, vWh_dec),\n",
        "        (Wy, dWy, mWy, vWy),\n",
        "\n",
        "        (W_bridge, dW_bridge, mW_bridge, vW_bridge),\n",
        "\n",
        "        (bu_enc, dbu_enc, mbu_enc, vbu_enc),\n",
        "        (br_enc, dbr_enc, mbr_enc, vbr_enc),\n",
        "        (bh_enc, dbh_enc, mbh_enc, vbh_enc),\n",
        "\n",
        "        (bu_dec, dbu_dec, mbu_dec, vbu_dec),\n",
        "        (br_dec, dbr_dec, mbr_dec, vbr_dec),\n",
        "        (bh_dec, dbh_dec, mbh_dec, vbh_dec),\n",
        "        (by, dby, mby, vby),\n",
        "\n",
        "        (b_bridge, db_bridge, mb_bridge, vb_bridge)\n",
        "    ]\n",
        "\n",
        "    updated_params = []\n",
        "\n",
        "    for param, grad, m, v in params:\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "\n",
        "        # Bias correction\n",
        "        m_corrected = m / (1 - beta1 ** t_adam)\n",
        "        v_corrected = v / (1 - beta2 ** t_adam)\n",
        "\n",
        "        param = param - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n",
        "        updated_params.append((param, m, v))\n",
        "\n",
        "    (Wemb, mWemb, vWemb) = updated_params[0]\n",
        "\n",
        "    (Wu_enc, mWu_enc, vWu_enc) = updated_params[1]\n",
        "    (Wr_enc, mWr_enc, vWr_enc) = updated_params[2]\n",
        "    (Wh_enc, mWh_enc, vWh_enc) = updated_params[3]\n",
        "\n",
        "    (Wu_dec, mWu_dec, vWu_dec) = updated_params[4]\n",
        "    (Wr_dec, mWr_dec, vWr_dec) = updated_params[5]\n",
        "    (Wh_dec, mWh_dec, vWh_dec) = updated_params[6]\n",
        "    (Wy, mWy, vWy) = updated_params[7]\n",
        "\n",
        "    (W_bridge, mW_bridge, vW_bridge) = updated_params[8]\n",
        "\n",
        "    (bu_enc, mbu_enc, vbu_enc) = updated_params[9]\n",
        "    (br_enc, mbr_enc, vbr_enc) = updated_params[10]\n",
        "    (bh_enc, mbh_enc, vbh_enc) = updated_params[11]\n",
        "\n",
        "    (bu_dec, mbu_dec, vbu_dec) = updated_params[12]\n",
        "    (br_dec, mbr_dec, vbr_dec) = updated_params[13]\n",
        "    (bh_dec, mbh_dec, vbh_dec) = updated_params[14]\n",
        "    (by, mby, vby) = updated_params[15]\n",
        "\n",
        "    (b_bridge, mb_bridge, vb_bridge) = updated_params[16]"
      ],
      "metadata": {
        "id": "wSGIUqZsMU0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __TRAIN MODEL__\n"
      ],
      "metadata": {
        "id": "hpHLQqh7XFil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(pairs, num_iterations=1000, print_every=100):\n",
        "    \"\"\"\n",
        "    Train the Encoder-Decoder Model\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    print_loss_total = 0\n",
        "\n",
        "    for iter in range(1, num_iterations + 1):\n",
        "        # 1. Pick a random pair from the dataset\n",
        "        training_pair = random.choice(pairs)\n",
        "        src_inputs = training_pair['src']\n",
        "        dec_inputs = training_pair['dec_input']\n",
        "        dec_targets = training_pair['dec_target']\n",
        "\n",
        "        # 2. Forward Pass\n",
        "        loss, enc_cache, bridge_cache, dec_cache = forward(src_inputs, dec_inputs, dec_targets)\n",
        "\n",
        "        # 3. Backward Pass\n",
        "        grads = backward(src_inputs, dec_inputs, dec_targets, enc_cache, bridge_cache, dec_cache)\n",
        "\n",
        "        # 4. Update Parameters\n",
        "        update_parameters(grads, lr)\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(f\"Iteration {iter} | Loss: {print_loss_avg:.4f}\")\n"
      ],
      "metadata": {
        "id": "HvrBIPp3XAt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __GENERATE TEXT__"
      ],
      "metadata": {
        "id": "j4GOFoOuYzvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(source_sentence, max_len=25):\n",
        "    \"\"\"\n",
        "    Generate a response/next sentence given a source sentence.\n",
        "    Pipeline: Source -> Encoder -> Bridge -> Decoder -> Generation\n",
        "    \"\"\"\n",
        "    # PREPROCESS\n",
        "    words = re.findall(r\"\\w+|[.,!?'\\\";:]\", source_sentence.lower())\n",
        "    src_indices = [word_to_ix.get(w, word_to_ix[UNK_TOKEN]) for w in words]\n",
        "\n",
        "    # ENCODER PASS\n",
        "    h_enc = np.zeros((hidden_size, 1))\n",
        "    for word_idx in src_indices:\n",
        "        _, _, _, _, h_enc = encoder(h_enc, word_idx)\n",
        "\n",
        "    # BRIDGE\n",
        "    # Transform Encoder Final State -> Decoder Initial State\n",
        "    bridge_z = np.dot(W_bridge, h_enc) + b_bridge\n",
        "    h_dec = np.tanh(bridge_z)\n",
        "\n",
        "    # DECODER GENERATION\n",
        "    # Start with <SOS>\n",
        "    curr_word_idx = word_to_ix[SOS_TOKEN]\n",
        "    generated_words = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        _, _, _, _, h_dec, yt = decoder(h_dec, curr_word_idx)\n",
        "        prob = softmax(yt)\n",
        "        next_word_idx = np.argmax(prob)\n",
        "        # next_word_idx = np.random.choice(range(vocab_size), p=prob.ravel())\n",
        "        next_word = ix_to_word[next_word_idx]\n",
        "        if next_word == EOS_TOKEN:\n",
        "            break\n",
        "        generated_words.append(next_word)\n",
        "        curr_word_idx = next_word_idx\n",
        "\n",
        "    return ' '.join(generated_words)"
      ],
      "metadata": {
        "id": "ldl7q4JsYWJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __RUN TRAINING__"
      ],
      "metadata": {
        "id": "1y0M3vMIZcjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "train(training_pairs, num_iterations=5000, print_every=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjfsldMiZVna",
        "outputId": "6edfe492-6dd4-4245-b718-aaf6ace2fbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500 | Loss: 39.0605\n",
            "Iteration 1000 | Loss: 12.1057\n",
            "Iteration 1500 | Loss: 3.9874\n",
            "Iteration 2000 | Loss: 2.4961\n",
            "Iteration 2500 | Loss: 2.1274\n",
            "Iteration 3000 | Loss: 1.8678\n",
            "Iteration 3500 | Loss: 1.8496\n",
            "Iteration 4000 | Loss: 1.1618\n",
            "Iteration 4500 | Loss: 0.7839\n",
            "Iteration 5000 | Loss: 0.4819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __TEST__"
      ],
      "metadata": {
        "id": "jhx3NWCEtl0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"The crow was thirsty.\",\n",
        "    \"He found a pitcher.\",\n",
        "]\n",
        "for sent in test_sentences:\n",
        "    response = predict(sent)\n",
        "    print(f\"Input:  {sent}\")\n",
        "    print(f\"Output: {response}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_1JWyvyZi7B",
        "outputId": "78c8b3e7-2062-4d80-bd64-23606683d9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  The crow was thirsty.\n",
            "Output: there was a little water at the bottom , but his beak could not reach it .\n",
            "------------------------------\n",
            "Input:  He found a pitcher.\n",
            "Output: then he got an idea !\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SUMMARY**\n",
        "\n",
        "This implementation provides a deep, mathematical understanding of how neural networks process sequences, paving the way for understanding more complex architectures like Attention mechanisms and Transformers.\n",
        "\n",
        "\n",
        "### **Further Reading:**\n",
        "1. [Learning Phrase Representations using RNN Encoderâ€“Decoder](https://arxiv.org/abs/1406.1078) â€” Cho et al. (2014) - *The specific architecture implemented in this notebook.*\n",
        "2. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) â€” Sutskever et al. (2014) - *The LSTM-based parallel to this work.*\n",
        "\n",
        "3. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) â€” Olah (2015) - *Visual explanations of gating mechanisms.*\n",
        "4. [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555) â€” Chung et al. (2014) - *Rigorous comparison of GRU vs. LSTM.*\n",
        "\n",
        "**Next Steps (Modern Architectures):**\n",
        "\n",
        "5. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) â€” Bahdanau et al. (2014) - *The introduction of the **Attention Mechanism** (the logical next step for this model).*\n",
        "6. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Vaswani et al. (2017) - *Transformers, which replaced RNNs.*"
      ],
      "metadata": {
        "id": "IOmDMFKYuHno"
      }
    }
  ]
}