{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Word-Level Recurrent Neural Network Language Model**\n","\n","## **From Scratch Implementation in NumPy**\n","\n","This notebook implements a **vanilla RNN** for word-level language modeling.\n","### **What We'll Build:**\n","- **Vanilla RNN** with tanh activation\n","- **Word embeddings** for efficient vocabulary representation\n","- **Backpropagation Through Time (BPTT)** for gradient computation\n","- **Adam optimizer** for parameter updates\n","- **Word-level text generation** from learned patterns\n","\n","---\n","*Notebook by*: Ahmad Raza [@ahmadrazacdx](https://github.com/ahmadrazacdx)<br>\n","*Date: 2025*  \n","*License: MIT*"],"metadata":{"id":"kv99qL16D9Kj"}},{"cell_type":"code","source":["import re\n","import numpy as np\n","np.random.seed(42)"],"metadata":{"id":"SgSsJcK83DfA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __DATA I/O__\n","We will use `Thirsty Crow` story as our small training data, but now tokenized at the **word level**."],"metadata":{"id":"nbAO0JsH4o00"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goJyoKz2zrmy","outputId":"3fc7f126-1111-4031-8218-95ba71a4e76e","executionInfo":{"status":"ok","timestamp":1763525114327,"user_tz":-300,"elapsed":25,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has 148 words, 90 unique (including special tokens).\n","Special tokens: ['<SOS>', '<EOS>', '<UNK>']\n","Sample words: ['!', ',', '.', 'a', 'after', 'an', 'and', 'at', 'away', 'beak']\n"]}],"source":["data = open('../data/thirsty_crow.txt', 'r').read()\n","words = re.findall(r\"\\w+|[.,!?'\\\";:]\", data.lower())\n","SOS_TOKEN = '<SOS>'  # Start of Sequence\n","EOS_TOKEN = '<EOS>'  # End of Sequence\n","UNK_TOKEN = '<UNK>'  # Unknown word\n","vocab = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(list(set(words)))\n","data_size, vocab_size = len(words), len(vocab)\n","print(f'Data has {data_size} words, {vocab_size} unique (including special tokens).')\n","print(f'Special tokens: {vocab[:3]}')\n","print(f'Sample words: {vocab[3:13]}')"]},{"cell_type":"code","source":["word_to_ix = {w: i for i, w in enumerate(vocab)}"],"metadata":{"id":"xc6oODzj3ijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ix_to_word = {i: w for i, w in enumerate(vocab)}"],"metadata":{"id":"ClysqC-_3wNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding the Mappings:**\n","- `word_to_ix`: Maps ```{'<SOS>': 0, '<EOS>': 1, '<UNK>': 2, 'a': 3, 'crow': 4, ...}```\n","- `ix_to_word`: Reverse mapping: ```{0: '<SOS>', 1: '<EOS>', 2: '<UNK>', 3: 'a', ...}```\n","\n","**Special Tokens:**\n","- **`<SOS>`**: Marks the beginning of a sequence (useful for generation)\n","- **`<EOS>`**: Marks the end of a sequence (helps model learn when to stop)\n","- **`<UNK>`**: Represents out-of-vocabulary words at test time\n","\n","**Example:** The phrase \"the cat sat\" becomes `[45, 12, 38]` if ```{'the': 45, 'cat': 12, 'sat': 38}``` in our vocabulary."],"metadata":{"id":"6bXsJGvT_a7-"}},{"cell_type":"markdown","source":["### __HYPER-PARAMETERS__"],"metadata":{"id":"JhOzRBXz414W"}},{"cell_type":"code","source":["lr = 1e-3 # learning rate\n","seq_len = 25 # times RNN will be unrolled (Timesteps) -words in one seq\n","hidden_size = 100 # size of hidden units\n","embed_size = 100 # size of word embedding vector"],"metadata":{"id":"InWw7ZqC3xVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Hyperparameter Guide:**\n","- **`embed_size`**: Dimension of word embedding vectors\n","  - Maps words to dense continuous vectors\n","  - Typical range: 50-300 (smaller vocab → smaller embeddings)\n","  - Much more efficient than one-hot encoding for large vocabularies!"],"metadata":{"id":"7jj_Hua5_9ig"}},{"cell_type":"markdown","source":["### __MODEL PARAM INIT__\n","**Initializing weight matrices for the RNN with word embeddings.**\n","\n","- $\\mathbf{W}_{emb} \\in \\mathbb{R}^{V \\times E}$ Word embedding matrix (lookup table)\n","- $\\mathbf{W}_{eh} \\in \\mathbb{R}^{H \\times E}$ Embedding-to-hidden weights\n","- $\\mathbf{W}_{hh} \\in \\mathbb{R}^{H \\times H}$ Hidden-to-hidden (recurrent) weights\n","- $\\mathbf{W}_{hy} \\in \\mathbb{R}^{V \\times H}$ Hidden-to-output weights\n","- $\\mathbf{b}_h \\in \\mathbb{R}^{H \\times 1}$ Hidden bias\n","- $\\mathbf{b}_y \\in \\mathbb{R}^{V \\times 1}$ Output bias\n","\n","**Where:**  \n","- $V$ = vocabulary size  \n","- $E$ = embedding dimension  \n","- $H$ = hidden size"],"metadata":{"id":"5LRNhINY6CMp"}},{"cell_type":"code","source":["Wemb = np.random.randn(vocab_size, embed_size) * 0.01 # word embeddings (V, E)\n","Weh = np.random.randn(hidden_size, embed_size) * 0.01 # embedding to hidden (H, E)\n","Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden (H, H)\n","Why = np.random.randn(vocab_size, hidden_size) * 0.01 # hidden to output (V, H)\n","bh = np.zeros((hidden_size, 1)) # hidden bias (H, 1)\n","by = np.zeros((vocab_size, 1)) # output bias (V, 1)"],"metadata":{"id":"NPbbN7zi5jjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"\n","Wemb: Word embeddings  : {Wemb.shape}\n","Weh: Embedding to hidden: {Weh.shape}\n","Whh: Hidden to hidden : {Whh.shape}\n","Why: Hidden to output : {Why.shape}\n","bh: Hidden bias       : {bh.shape}\n","by: Output bias       : {by.shape}\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HGbspWj7RRE","outputId":"1c8733fd-0396-4a73-9866-f6779dec1250","executionInfo":{"status":"ok","timestamp":1763525117852,"user_tz":-300,"elapsed":19,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Wemb: Word embeddings  : (90, 100)\n","Wxh: Embedding to hidden: (100, 100)\n","Whh: Hidden to hidden : (100, 100)\n","Why: Hidden to output : (90, 100)\n","bh: Hidden bias       : (100, 1)\n","by: Output bias       : (90, 1)\n","\n"]}]},{"cell_type":"markdown","source":["### __ADAM OPTIMIZER INITIALIZATION__"],"metadata":{"id":"xvWhMsGbeRjl"}},{"cell_type":"code","source":["# Adam hyperparameters\n","beta1 = 0.9\n","beta2 = 0.999\n","epsilon = 1e-8\n","\n","# Adam memory variables (first moment)\n","mWemb = np.zeros_like(Wemb)\n","mWeh = np.zeros_like(Weh)\n","mWhh = np.zeros_like(Whh)\n","mWhy = np.zeros_like(Why)\n","mbh = np.zeros_like(bh)\n","mby = np.zeros_like(by)\n","\n","# Adam memory variables (second moment)\n","vWemb = np.zeros_like(Wemb)\n","vWeh = np.zeros_like(Weh)\n","vWhh = np.zeros_like(Whh)\n","vWhy = np.zeros_like(Why)\n","vbh = np.zeros_like(bh)\n","vby = np.zeros_like(by)\n","\n","# Timestep counter for bias correction\n","t_adam = 0"],"metadata":{"id":"tUZFfUeAePrY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SINGLE VANILLA RNN CELL WITH WORD EMBEDDINGS__\n","\n","**Forward pass equations:**\n","\n","$$\n","\\begin{align}\n","\\mathbf{e}_t &= \\mathbf{W}_{emb}[word\\_idx] \\quad &\\text{(word embedding lookup)} \\\\\n","\\mathbf{z}_t &= \\mathbf{W}_{eh} \\mathbf{e}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_h \\quad &\\text{(pre-activation)} \\\\\n","\\mathbf{h}_t &= \\tanh(\\mathbf{z}_t) \\quad &\\text{(hidden state)} \\\\\n","\\mathbf{y}_t &= \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y \\quad &\\text{(output logits)}\n","\\end{align}\n","$$\n","\n","**Key difference from char-level:** Instead of one-hot input, we lookup a dense embedding vector!"],"metadata":{"id":"VkDtomuF8LFZ"}},{"cell_type":"code","source":["def rnn(word_idx, h_prev):\n","    \"\"\"\n","    Single RNN cell with embedding lookup\n","\n","    Inputs:\n","        - word_idx: Integer index of word in vocabulary\n","        - h_prev: Previous hidden state (H, 1)\n","\n","    Returns:\n","        - ht: Current hidden state (H, 1)\n","        - yt: Output logits (V, 1)\n","        - et: Word embedding (E, 1)\n","    \"\"\"\n","    et = Wemb[word_idx].reshape(-1, 1)  # (E, 1)\n","    zt = np.dot(Weh, et) + np.dot(Whh, h_prev) + bh  # (H, E)@(E,1):(H,1),(H,H)@(H,1):(H,1) + (H,1) = (H,1)\n","    ht = np.tanh(zt)  # (H, 1)\n","    yt = np.dot(Why, ht) + by  # (V,H)@(H,1):(V, 1)+(V,1)=(V,1)\n","    return ht, yt, et"],"metadata":{"id":"AVvRr4GJ7SmG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding the RNN Cell with Embeddings:**\n","\n","**What's happening here?**\n","- At each timestep $t$, the RNN takes two inputs:\n","  1. Current word index → lookup in $\\mathbf{W}_{emb}$ to get dense vector $\\mathbf{e}_t$\n","  2. Previous memory $\\mathbf{h}_{t-1}$ (hidden state from last step)\n","  \n","- It outputs:\n","  1. New memory $\\mathbf{h}_t$ (encodes everything seen so far)\n","  2. Prediction $\\mathbf{y}_t$ (logits for next word)\n","  3. Embedding $\\mathbf{e}_t$ (needed for backprop!)\n","\n","**Why embeddings?**\n","- **Char-level**: vocab_size = 50-100 → one-hot is fine\n","- **Word-level**: vocab_size = 10,000+ → one-hot is wasteful!\n","- Embeddings learn semantic relationships: \"king\" - \"man\" + \"woman\" ≈ \"queen\"\n","\n","**The \"recurrent\" part:** $\\mathbf{h}_t$ depends on $\\mathbf{h}_{t-1}$, creating a feedback loop through time."],"metadata":{"id":"4f5wu1Tj9rbm"}},{"cell_type":"markdown","source":["### __FORWARD PASS__\n"],"metadata":{"id":"JSiqg1u1Fcuo"}},{"cell_type":"code","source":["def forward(inputs, targets, h_prev):\n","    \"\"\"\n","    Forward pass through unrolled RNN with word embeddings\n","\n","    Inputs:\n","        - inputs: List of word indices, e.g., [5, 8, 12, 12, 15] for \"the cat sat sat on\"\n","        - targets: List of target word indices (inputs shifted by 1)\n","        - h_prev: Initial hidden state from previous sequence, shape (H, 1)\n","\n","    Returns:\n","        - word_indices: Dict of word indices {0: idx_0, 1: idx_1, ...}\n","        - et: Dict of embeddings {0: e_0, 1: e_1, ...}\n","        - ht: Dict of hidden states {-1: h_init, 0: h_0, 1: h_1, ...}\n","        - yt: Dict of output logits {0: y_0, 1: y_1, ...}\n","        - probt: Dict of probability distributions {0: p_0, 1: p_1, ...}\n","        - loss: Total cross-entropy loss across all timesteps (scalar)\n","    \"\"\"\n","    # Initialize storage dictionaries\n","    word_indices = {}  # Store word indices\n","    et = {}  # Store embeddings\n","    ht = {}  # Store hidden states\n","    yt = {}  # Store raw outputs (before softmax)\n","    probt = {}  # Store probability distributions (after softmax)\n","\n","    # Set initial hidden state\n","    ht[-1] = np.copy(h_prev)  # (H, 1)\n","    loss = 0\n","\n","    # Loop through each timestep in the sequence\n","    for t in range(len(inputs)):\n","        # Step 1: Store word index\n","        word_indices[t] = inputs[t]\n","\n","        # Step 2: Run RNN cell (forward computation with embedding)\n","        ht[t], yt[t], et[t] = rnn(word_indices[t], ht[t-1])  # (H, 1), (V, 1), (E, 1)\n","\n","        # Step 3: Apply softmax to get probabilities\n","        exp_scores = np.exp(yt[t] - np.max(yt[t]))  # (V, 1)\n","        probt[t] = exp_scores / np.sum(exp_scores)  # (V, 1)\n","\n","        # Step 4: Compute loss for this timestep\n","        # Cross-entropy: -log(probability of correct next word)\n","        loss += -np.log(probt[t][targets[t], 0] + epsilon)\n","\n","    return word_indices, et, ht, probt, loss"],"metadata":{"id":"ErpAgrEEEpld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Deep Dive: Understanding the Forward Pass with Embeddings**\n","\n","**1. From one-hot to embeddings**\n","```python\n","# Char-level (old):\n","xt[t] = [0, 0, 1, 0, 0, ..., 0]  # vocab_size = 50, wasteful but okay\n","\n","# Word-level (new):\n","word_idx = 2047  # \"cat\" in vocabulary\n","et[t] = Wemb[2047]  # Lookup row 2047 → dense vector [0.23, -0.45, 0.67, ...]\n","# embed_size = 50, much smaller than vocab_size = 10000!\n","```\n","\n","**2. Cross-Entropy Loss Formula** (same as before)\n","\n","$$\\mathcal{L}_t = -\\log p_t[y^*_t]$$\n","\n","Where:\n","- $p_t$: Probability distribution at time $t$ (output of softmax)\n","- $y^*_t$: Correct next word (target)\n","- Lower loss → model assigns higher probability to correct word\n","\n","**Total loss:** $\\mathcal{L} = \\sum_{t=0}^{T-1} \\mathcal{L}_t$\n","\n","**3. Why maintain hidden state?**\n","\n","The hidden state $\\mathbf{h}_t$ acts as the network's \"memory\":\n","- Encodes information about all words seen so far\n","- Enables the model to learn long-term dependencies\n","- Without it, predictions would be independent at each position!\n","\n","**Example:** To predict the next word in \"The cat sat on the ___\", the model needs to remember \"cat\" from earlier in the sequence to predict \"mat\" or \"floor\"."],"metadata":{"id":"WnY_dRbDIWDs"}},{"cell_type":"markdown","source":["### __BACKWARD PASS WITH EMBEDDING GRADIENTS__\n","**Complete BPTT Equations (With Embeddings):**\n","\n","**Step 1: Output Gradient (Softmax + Cross-Entropy)**\n","$$\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} = \\mathbf{p}_t - \\mathbf{1}_{y^*_t}$$\n","\n","**Step 2: Output Weight Gradients**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hy}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\mathbf{h}_t^T, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t}$$\n","\n","**Step 3: Hidden State Gradient (Two Sources!)**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} = \\mathbf{W}_{hy}^T \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}}$$\n","\n","**Step 4: Pre-Activation Gradient (Tanh)**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (1 - \\mathbf{h}_t^2)$$\n","\n","**Step 5: Hidden Weight & Embedding Gradients (NEW!)**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{eh}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t} \\mathbf{e}_t^T, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hh}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t} \\mathbf{h}_{t-1}^T$$\n","\n","**Step 6: Embedding Gradient (Backprop to embedding)**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{e}_t} = \\mathbf{W}_{eh}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t}$$\n","\n","**Step 7: Update specific embedding row**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{emb}[word\\_idx]} += \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{e}_t}$$\n","\n","**Step 8: Gradient to Previous Timestep**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} = \\mathbf{W}_{hh}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t}$$\n","\n","**Visual:** Gradients flow backward from $t=T-1 \\rightarrow 0$, accumulating contributions from all timesteps.\n"],"metadata":{"id":"7CTgWopPOAka"}},{"cell_type":"code","source":["def backward(inputs, targets, word_indices, et, ht, probt):\n","    \"\"\"\n","    Backpropagation Through Time (BPTT) with embedding gradients\n","\n","    Inputs:\n","        - inputs: List of input word indices\n","        - targets: List of target word indices\n","        - word_indices: Dict of word indices from forward pass\n","        - et: Dict of embeddings from forward pass\n","        - ht: Dict of hidden states from forward pass\n","        - probt: Dict of probability distributions from forward pass\n","\n","    Returns:\n","        - dWemb: Gradient w.r.t. word embeddings, shape (V, E)\n","        - dWeh: Gradient w.r.t. embedding-to-hidden weights, shape (H, E)\n","        - dWhh: Gradient w.r.t. hidden-to-hidden weights, shape (H, H)\n","        - dWhy: Gradient w.r.t. hidden-to-output weights, shape (V, H)\n","        - dbh: Gradient w.r.t. hidden bias, shape (H, 1)\n","        - dby: Gradient w.r.t. output bias, shape (V, 1)\n","    \"\"\"\n","    # Initialize gradients to zero\n","    dWemb = np.zeros_like(Wemb)\n","    dWeh, dWhh, dWhy = np.zeros_like(Weh), np.zeros_like(Whh), np.zeros_like(Why)\n","    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n","\n","    # Gradient of hidden state at next timestep (initially zero)\n","    dh_next = np.zeros_like(ht[0]) # (H,1)\n","\n","    # Backpropagate through time (from last to first timestep)\n","    for t in reversed(range(len(inputs))):\n","        # Step 1: Gradient of loss w.r.t output probabilities\n","        dy = np.copy(probt[t])  # (V, 1)\n","        dy[targets[t]] -= 1\n","\n","        # Step 2: Gradients for output layer (Why, by)\n","        dWhy += np.dot(dy, ht[t].T) # (V,1)@(H,1)->(1,H)=(V,H)\n","        dby += dy # (V,1)\n","\n","        # Step 3: Gradient w.r.t hidden state\n","        # Comes from two sources: current output and next timestep\n","        dh = np.dot(Why.T, dy) + dh_next # (V,H)->(H,V)@(V,1)=(H,1)+(H,1)=(H,1)\n","\n","        # Step 4: Gradient through tanh activation\n","        dzt = (1 - ht[t]**2) * dh  #(H,1)*(H,1)=(H,1)\n","\n","        # Step 5: Gradients for hidden layer (Weh, Whh, bh)\n","        dWeh += np.dot(dzt, et[t].T) #(H,1)@(E,1)->(1.E)=(H,E)\n","        dWhh += np.dot(dzt, ht[t-1].T) #(H,1)@(H,1)->(1,H)=H,H\n","        dbh += dzt #(H,1)\n","\n","        # Step 6: Gradient w.r.t embedding\n","        de = np.dot(Weh.T, dzt)  # (E, H)@(H, 1)=(E, 1)\n","\n","        # Step 7: Accumulate gradient for specific word in embedding matrix\n","        dWemb[word_indices[t]] += de.flatten()  # Update row corresponding to word_idx\n","\n","        # Step 8: Pass gradient to previous timestep\n","        dh_next = np.dot(Whh.T, dzt)  # (H, H)@(H, 1)=(H, 1)\n","\n","    # Clip gradients to prevent exploding gradients\n","    for dparam in [dWemb, dWeh, dWhh, dWhy, dbh, dby]:\n","        np.clip(dparam, -5, 5, out=dparam)\n","\n","    return dWemb, dWeh, dWhh, dWhy, dbh, dby"],"metadata":{"id":"uX93liyrK-as"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding BPTT with Embeddings: The Gradient Flow**\n","\n","**1. The Softmax-CrossEntropy Gradient** (same as before)\n","\n","For softmax activation with cross-entropy loss, the gradient simplifies to:\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} = \\mathbf{p} - \\mathbf{1}_{y^*}$$\n","\n","Where $\\mathbf{1}_{y^*}$ is the one-hot vector of the target.\n","\n","**2. NEW: Embedding Gradient Flow**\n","\n","```python\n","# Gradient flows: loss → output → hidden → embedding\n","dzt = ...  # Gradient at pre-activation\n","de = Weh.T @ dzt  # Gradient w.r.t embedding vector (E, 1)\n","dWemb[word_idx] += de  # Update ONLY the row for this word!\n","```\n","\n","**Key insight:** At each timestep, only the embedding row for the current word receives a gradient update, not the entire matrix. However, if the same word appears multiple times in a sequence, its embedding row accumulates gradients from all occurrences. This is more efficient than dense matrix updates for large vocabularies.\n","\n","**3. Gradient Accumulation**\n","\n","Notice `+=` instead of `=`:\n","```python\n","dWxh += np.dot(dzt, et[t].T)  # Accumulate across timesteps\n","dWemb[word_idx] += de  # Accumulate for same word appearing multiple times\n","```\n","We sum gradients from all timesteps because the same weights are shared across time!\n"],"metadata":{"id":"Spgg8FYXLqKl"}},{"cell_type":"markdown","source":["### __UPDATE PARAMS WITH ADAM__"],"metadata":{"id":"whAHx2WTW03k"}},{"cell_type":"code","source":["def update_parameters(dWemb, dWeh, dWhh, dWhy, dbh, dby, learning_rate):\n","    \"\"\"\n","    Update model parameters using Adam optimizer\n","\n","    Inputs:\n","        - dWemb: Gradient for word embeddings (V, E)\n","        - dWxh: Gradient for embedding-to-hidden weights (H, E)\n","        - dWhh: Gradient for hidden-to-hidden weights (H, H)\n","        - dWhy: Gradient for hidden-to-output weights (V, H)\n","        - dbh: Gradient for hidden bias (H, 1)\n","        - dby: Gradient for output bias (V, 1)\n","        - learning_rate: Step size for parameter updates\n","\n","    Returns:\n","        - None (updates global parameters in-place)\n","    \"\"\"\n","    global Wemb, Weh, Whh, Why, bh, by\n","    global mWemb, mWeh, mWhh, mWhy, mbh, mby\n","    global vWemb, vWeh, vWhh, vWhy, vbh, vby\n","    global t_adam\n","\n","    # Increment timestep\n","    t_adam += 1\n","\n","    # Update Wemb\n","    mWemb = beta1 * mWemb + (1 - beta1) * dWemb\n","    vWemb = beta2 * vWemb + (1 - beta2) * (dWemb ** 2)\n","    mWemb_corrected = mWemb / (1 - beta1 ** t_adam)\n","    vWemb_corrected = vWemb / (1 - beta2 ** t_adam)\n","    Wemb -= learning_rate * mWemb_corrected / (np.sqrt(vWemb_corrected) + epsilon)\n","\n","    # Update Weh\n","    mWeh = beta1 * mWeh + (1 - beta1) * dWeh\n","    vWeh = beta2 * vWeh + (1 - beta2) * (dWeh ** 2)\n","    mWeh_corrected = mWeh / (1 - beta1 ** t_adam)\n","    vWeh_corrected = vWeh / (1 - beta2 ** t_adam)\n","    Weh -= learning_rate * mWeh_corrected / (np.sqrt(vWeh_corrected) + epsilon)\n","    # Update Whh\n","    mWhh = beta1 * mWhh + (1 - beta1) * dWhh\n","    vWhh = beta2 * vWhh + (1 - beta2) * (dWhh ** 2)\n","    mWhh_corrected = mWhh / (1 - beta1 ** t_adam)\n","    vWhh_corrected = vWhh / (1 - beta2 ** t_adam)\n","    Whh -= learning_rate * mWhh_corrected / (np.sqrt(vWhh_corrected) + epsilon)\n","\n","    # Update Why\n","    mWhy = beta1 * mWhy + (1 - beta1) * dWhy\n","    vWhy = beta2 * vWhy + (1 - beta2) * (dWhy ** 2)\n","    mWhy_corrected = mWhy / (1 - beta1 ** t_adam)\n","    vWhy_corrected = vWhy / (1 - beta2 ** t_adam)\n","    Why -= learning_rate * mWhy_corrected / (np.sqrt(vWhy_corrected) + epsilon)\n","\n","    # Update bh\n","    mbh = beta1 * mbh + (1 - beta1) * dbh\n","    vbh = beta2 * vbh + (1 - beta2) * (dbh ** 2)\n","    mbh_corrected = mbh / (1 - beta1 ** t_adam)\n","    vbh_corrected = vbh / (1 - beta2 ** t_adam)\n","    bh -= learning_rate * mbh_corrected / (np.sqrt(vbh_corrected) + epsilon)\n","\n","    # Update by\n","    mby = beta1 * mby + (1 - beta1) * dby\n","    vby = beta2 * vby + (1 - beta2) * (dby ** 2)\n","    mby_corrected = mby / (1 - beta1 ** t_adam)\n","    vby_corrected = vby / (1 - beta2 ** t_adam)\n","    by -= learning_rate * mby_corrected / (np.sqrt(vby_corrected) + epsilon)"],"metadata":{"id":"b5aaGQ1ELBbx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __TRAIN MODEL__\n"],"metadata":{"id":"hpHLQqh7XFil"}},{"cell_type":"code","source":["def train(words_list, num_iterations=1000, print_every=100, sample_every=100):\n","    \"\"\"\n","    Train RNN language model using Adam optimizer\n","\n","    Inputs:\n","        - words_list: List of words in training text\n","        - iterations: Number of training iterations (not full data passes)\n","        - print_every: Print loss every N iterations\n","        - sample_every: Generate sample text every N iterations\n","\n","    Returns:\n","        - smooth_loss: Exponentially smoothed loss value\n","\n","    Note: Hidden state h_prev is maintained across sequences for temporal continuity.\n","        Only reset when reaching end of data or on first iteration.\n","    \"\"\"\n","    smooth_loss = -np.log(1.0 / vocab_size) * seq_len\n","    n = 0  # Iteration counter\n","    p = 0  # Data pointer (position in word list)\n","    h_prev = np.zeros((hidden_size, 1))\n","\n","    while n < num_iterations:\n","        # Reset pointer and hidden state at end of data or first iteration\n","        if p + seq_len + 1 >= len(words_list) or n == 0:\n","            h_prev = np.zeros((hidden_size, 1))  # Fresh start\n","            p = 0  # Go back to beginning\n","\n","        # Input:  words at positions [p, p+1, ..., p+seq_len-1]\n","        # Target: words at positions [p+1, p+2, ..., p+seq_len]\n","        inputs = [word_to_ix[w] for w in words_list[p:p+seq_len]]\n","        targets = [word_to_ix[w] for w in words_list[p+1:p+seq_len+1]]\n","\n","        # Forward pass (pass hidden state!)\n","        word_indices, et, ht, probt, loss = forward(inputs, targets, h_prev)\n","        # Update hidden state for next sequence\n","        h_prev = np.copy(ht[len(inputs) - 1])\n","        # Backward pass\n","        dWemb, dWeh, dWhh, dWhy, dbh, dby = backward(inputs, targets, word_indices, et, ht, probt)\n","        # Update parameters\n","        update_parameters(dWemb, dWeh, dWhh, dWhy, dbh, dby, lr)\n","        # Update smooth loss\n","        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","        # Print progress\n","        if n % print_every == 0:\n","            print(f\"Iter {n:6d} | Loss: {smooth_loss:.4f}\")\n","        # Generate sample text\n","        if n % sample_every == 0 and n > 0:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"SAMPLE at iteration {n}:\")\n","            print(f\"{'='*60}\")\n","            sample_text = sample(seed_word=None, n_words=50, use_sos=True, stop_at_eos=True)\n","            print(sample_text)\n","            print(f\"{'='*60}\\n\")\n","        p += seq_len\n","        n += 1\n","\n","    print(\"\\nTraining complete!\")\n","    return smooth_loss"],"metadata":{"id":"HvrBIPp3XAt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SAMPLING FUNCTION (GENERATE TEXT)__"],"metadata":{"id":"j4GOFoOuYzvD"}},{"cell_type":"code","source":["def sample(seed_word=None, n_words=50, use_sos=True, stop_at_eos=True):\n","    \"\"\"\n","    Generate text by sampling from the trained RNN language model\n","\n","    Inputs:\n","        - seed_word: Starting word for text generation (if None, uses <SOS>)\n","        - n_words: Maximum number of words to generate (default 50)\n","        - use_sos: If True and seed_word is None, start with <SOS> token\n","        - stop_at_eos: If True, stop generation when <EOS> token is sampled\n","\n","    Returns:\n","        - String of generated text\n","    \"\"\"\n","    # Initialize with seed word or SOS token\n","    if seed_word is None:\n","        if use_sos:\n","            word_idx = word_to_ix[SOS_TOKEN]\n","            generated_words = []  # Don't include SOS in output\n","        else:\n","            word_idx = word_to_ix[vocab[3]]\n","            generated_words = [ix_to_word[word_idx]]\n","    else:\n","        # Handle seed word not in vocabulary - use UNK token\n","        word_idx = word_to_ix.get(seed_word.lower(), word_to_ix[UNK_TOKEN])\n","        if word_idx == word_to_ix[UNK_TOKEN]:\n","            print(f\"Warning: '{seed_word}' not in vocab. Using <UNK> token.\")\n","        generated_words = [ix_to_word[word_idx]]\n","    h = np.zeros((hidden_size, 1))  # Initialize hidden state\n","\n","    for _ in range(n_words):\n","        h, y, e = rnn(word_idx, h)\n","        p = np.exp(y - np.max(y)) / np.sum(np.exp(y - np.max(y)))\n","        ix = np.random.choice(range(vocab_size), p=p.ravel())\n","        word = ix_to_word[ix]\n","        # Check for EOS token\n","        if stop_at_eos and word == EOS_TOKEN:\n","            break\n","        # Skip adding special tokens to output\n","        if word not in [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]:\n","            generated_words.append(word)\n","        word_idx = ix\n","\n","    return ' '.join(generated_words)"],"metadata":{"id":"ldl7q4JsYWJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding Text Generation (Word-Level with Special Tokens)**\n","\n","**Example:** Suppose after \"Once upon a\", the model outputs probabilities:\n","\n","```python\n","Words: ['time', 'day', 'hill', '<EOS>', 'morning', ...]\n","Probabilities: [0.65, 0.20, 0.10, 0.03, 0.01, ...]\n","```\n","\n","**Sampling:**\n","- 65% chance pick 'time' → \"Once upon a time there\"\n","- 20% chance pick 'day' → \"Once upon a day when\"  \n","- 10% chance pick 'hill' → \"Once upon a hill sat\"\n","- 3% chance pick '<EOS>' → Stop generation (if `stop_at_eos=True`)\n","- ...more diverse output!\n","\n","**Special Tokens in Generation:**\n","- **`<SOS>`**: Used to initialize generation from scratch\n","- **`<EOS>`**: Model learns when sequences naturally end\n","- **`<UNK>`**: Handles unknown words gracefully at test time\n","\n","**Word-level vs Char-level:**\n","- **Char-level**: Can generate any word (including typos) → \"teh cat\"  \n","- **Word-level**: Only generates known words → more coherent but less creative\n","- **Word-level**: Faster generation (fewer timesteps)\n","\n"],"metadata":{"id":"lADZxKmeNAXE"}},{"cell_type":"markdown","source":["### __RUN TRAINING__"],"metadata":{"id":"1y0M3vMIZcjn"}},{"cell_type":"code","source":["final_loss = train(words, 10000, print_every=500, sample_every=2000)\n","print(f\"\\nFinal smooth loss: {final_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjfsldMiZVna","outputId":"a2a55ca1-a9bb-4c08-c856-0bd6355b37eb","executionInfo":{"status":"ok","timestamp":1763525156355,"user_tz":-300,"elapsed":25246,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter      0 | Loss: 112.4952\n","Iter    500 | Loss: 99.0222\n","Iter   1000 | Loss: 67.6377\n","Iter   1500 | Loss: 42.5509\n","Iter   2000 | Loss: 26.2755\n","\n","============================================================\n","SAMPLE at iteration 2000:\n","============================================================\n","then he very flew down and looked inside . there was a little water at the bottom , but his brightly , and looked them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came slowly a moment crow\n","============================================================\n","\n","Iter   2500 | Loss: 16.1413\n","Iter   3000 | Loss: 9.9051\n","Iter   3500 | Loss: 6.0800\n","Iter   4000 | Loss: 3.7361\n","\n","============================================================\n","SAMPLE at iteration 4000:\n","============================================================\n","and brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his\n","============================================================\n","\n","Iter   4500 | Loss: 2.2999\n","Iter   5000 | Loss: 1.4192\n","Iter   5500 | Loss: 0.8785\n","Iter   6000 | Loss: 0.5459\n","\n","============================================================\n","SAMPLE at iteration 6000:\n","============================================================\n","flew upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher\n","============================================================\n","\n","Iter   6500 | Loss: 0.3409\n","Iter   7000 | Loss: 0.2141\n","Iter   7500 | Loss: 0.1355\n","Iter   8000 | Loss: 0.0864\n","\n","============================================================\n","SAMPLE at iteration 8000:\n","============================================================\n","water water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came thought for a long time , the crow finally saw a\n","============================================================\n","\n","Iter   8500 | Loss: 0.0557\n","Iter   9000 | Loss: 0.0363\n","Iter   9500 | Loss: 0.0239\n","\n","Training complete!\n","\n","Final smooth loss: 0.0160\n"]}]},{"cell_type":"markdown","source":["### __TEST DIFFERENT SEEDS__"],"metadata":{"id":"jhx3NWCEtl0J"}},{"cell_type":"code","source":["seed_words = ['The', 'he', 'thirsty', 'wood', 'shiny']\n","for word in seed_words:\n","    generated_word = sample(word, n_words=150)\n","    print(word, ':', generated_word)\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_1JWyvyZi7B","outputId":"1915f146-86b0-4c5c-b50c-6bdd94d0d2bc","executionInfo":{"status":"ok","timestamp":1763525156449,"user_tz":-300,"elapsed":86,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The : the water began to rise then he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came thought for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one\n","------------------------------------------------------------\n","he : he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow\n","------------------------------------------------------------\n","thirsty : thirsty . the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones\n","------------------------------------------------------------\n","Warning: 'wood' not in vocab. Using <UNK> token.\n","wood : <UNK> dropped pitcher . on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise\n","------------------------------------------------------------\n","Warning: 'shiny' not in vocab. Using <UNK> token.\n","shiny : <UNK> . the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came slowly , the water began to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the crow continued dropping stones until the water came to rise . the\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Different seed words from vocabulary\n","seed_words = ['the', 'a', 'crow', 'water', 'once']\n","for word in seed_words:\n","    if word in word_to_ix:\n","        generated = sample(word, n_words=30, stop_at_eos=True)\n","        print(f\"{word}: {generated}\")\n","        print(\"-\" * 60)\n","    else:\n","        print(f\"'{word}' not in vocabulary, skipping...\")\n","        print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arXs_luUNjra","executionInfo":{"status":"ok","timestamp":1763525156475,"user_tz":-300,"elapsed":22,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"34a3f2a5-fc97-48dc-d8ae-840dae44247d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the: the water . slowly , the water began to rise . the crow continued dropping stones until the water came very dropping stones until the water came to rise . the\n","------------------------------------------------------------\n","a: a time , and the inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment .\n","------------------------------------------------------------\n","crow: crow stones until the water came thought for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside .\n","------------------------------------------------------------\n","water: water water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a\n","------------------------------------------------------------\n","once: once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Test with SOS token (no seed word)\n","for i in range(3):\n","    generated = sample(seed_word=None, n_words=30, use_sos=True, stop_at_eos=True)\n","    print(f\"Sample {i+1}: {generated}\")\n","    print(\"-\" * 60)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-Jw5DwWQ0mF","executionInfo":{"status":"ok","timestamp":1763525156504,"user_tz":-300,"elapsed":24,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"fd7852e7-0dc2-4235-aaf2-34ef4d659305"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample 1: flew one a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying\n","------------------------------------------------------------\n","Sample 2: a time , and the inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment\n","------------------------------------------------------------\n","Sample 3: tree . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Test with unknown word\n","generated = sample('elephant', n_words=30, stop_at_eos=True)\n","print(f\"elephant: {generated}\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XjaMtHyQ2aW","executionInfo":{"status":"ok","timestamp":1763525156572,"user_tz":-300,"elapsed":65,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"899e55c0-121e-4f19-a0da-01b404da57a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: 'elephant' not in vocab. Using <UNK> token.\n","elephant: <UNK> quickly beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and\n","============================================================\n"]}]},{"cell_type":"markdown","source":["### __UNDERSTANDING SPECIAL TOKENS IN ACTION__\n","\n","**Why Special Tokens Matter:**\n","\n","1. **`<SOS>` (Start of Sequence)**\n","   - Signals the beginning of text generation\n","   - Helps model learn how stories/sentences typically begin\n","   - Used when generating from scratch: `sample(seed_word=None, use_sos=True)`\n","\n","2. **`<EOS>` (End of Sequence)**\n","   - Model learns natural stopping points\n","   - Prevents endless generation\n","   - Can control when generation stops: `sample(stop_at_eos=True)`\n","   - Example: \"The crow drank the water <EOS>\" → generation stops\n","\n","3. **`<UNK>` (Unknown)**\n","   - Handles out-of-vocabulary words gracefully\n","   - At test time, unknown words → `<UNK>` token\n","   - Model still generates coherent text even with unknown inputs\n","   - Example: `sample('elephant')` → if 'elephant' not in vocab, uses `<UNK>`\n","\n","**Use Case:**\n","```python\n","# Real-world scenario: User input might contain unseen words\n","user_input = \"quantum physics\"  # 'quantum' might not be in training vocab\n","generated = sample('quantum', n_words=20, stop_at_eos=True)\n","# Model uses <UNK> for 'quantum' but still generates sensible continuation\n","```\n","\n","**Benefits:**\n","- **Robustness**: Handles unexpected inputs\n","- **Control**: Can stop generation naturally\n","- **Flexibility**: Generate from scratch or with seed words"],"metadata":{"id":"A4xx_0PZN_19"}},{"cell_type":"markdown","source":["## **SUMMARY**\n","### **Key Takeaways:**\n","1. **Word Embeddings**: Dense vector representations replace sparse one-hot encoding, enabling efficient learning for large vocabularies\n","2. **Special Tokens**: `<SOS>`, `<EOS>`, and `<UNK>` tokens make the model robust and production-ready\n","3. **Hidden State = Memory**: The hidden state $\\mathbf{h}_t$ encodes all information seen so far across word sequences\n","4. **BPTT with Embeddings**: Backpropagation flows through time and through the embedding layer, updating only relevant word vectors\n","5. **Controlled Generation**: Can start from scratch (`<SOS>`), stop naturally (`<EOS>`), or handle unknown words (`<UNK>`)\n","\n","### **Word-Level vs Character-Level:**\n","| Aspect | Character-Level | Word-Level |\n","|--------|----------------|------------|\n","| **Vocabulary Size** | Small (~50-100) | Large (1,000-100,000+) |\n","| **Input Representation** | One-hot vectors | Learned embeddings |\n","| **Generation Speed** | Slower (more timesteps) | Faster (fewer timesteps) |\n","| **Creativity** | Can create new words | Limited to known words |\n","| **Grammar** | Must learn from scratch | Better structure naturally |\n","| **Memory Efficiency** | Low (small vocab) | High (sparse embedding updates) |\n","| **Special Tokens** | Less critical | Essential for robustness |\n","\n","### **Modern Improvements:**\n","- **LSTMs (Long Short-Term Memory)**: Solve vanishing gradient with gating mechanisms  \n","- **GRUs (Gated Recurrent Units)**: Simpler than LSTM, similar performance\n","- **Pre-trained Embeddings**: Use Word2Vec, GloVe, or FastText for better initialization\n","- **Subword Tokenization**: BPE, WordPiece handle OOV words (BERT, GPT)\n","- **Transformers**: Use attention, fully parallelizable, now dominant in NLP  \n","\n","\n","### **References:**\n","1. Karpathy, A. (2015). *The Unreasonable Effectiveness of RNNs*. [Blog Post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n","2. Elman, J. (1990). *Finding Structure in Time*. Cognitive Science, 14(2), 179-211.\n","3. Mikolov, T., et al. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv:1301.3781.\n","4. Rumelhart, D. E., et al. (1986). *Learning representations by back-propagating errors*. Nature, 323(6088), 533-536.\n","5. Kingma, D. P., & Ba, J. (2014). *Adam: A Method for Stochastic Optimization*. arXiv:1412.6980.\n","\n","### **Further Reading:**\n","1. Karpathy, A. (2015). [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n","2. Mikolov, T., et al. (2013). [Word2Vec Paper](https://arxiv.org/abs/1301.3781)\n","3. Hochreiter & Schmidhuber (1997). Long Short-Term Memory. Neural Computation.\n","4. Cho et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for SMT.\n","5. Vaswani et al. (2017). Attention Is All You Need. NeurIPS.\n","6. Goodfellow et al. (2016). Deep Learning. Chapter 10: Sequence Modeling."],"metadata":{"id":"yWmqjNEnEaB4"}}]}