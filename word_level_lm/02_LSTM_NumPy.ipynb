{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Word-Level LSTM Language Model**\n","\n","## **From Scratch Implementation in NumPy**\n","\n","This notebook implements a **vanilla LSTM** for word-level language modeling with word embeddings.\n","\n","### **What We'll Build:**\n","- **Vanilla LSTM** with three (forget, input, output) gates\n","- **Word embeddings** for efficient vocabulary representation\n","- **Backpropagation Through Time (BPTT)** for gradient computation\n","- **Word-level text generation** from learned patterns\n","\n","---\n","*Notebook by*: Ahmad Raza [@ahmadrazacdx](https://github.com/ahmadrazacdx)<br>\n","*Date: 2025*  \n","*License: MIT*"],"metadata":{"id":"kv99qL16D9Kj"}},{"cell_type":"code","source":["import re\n","import numpy as np\n","np.random.seed(42)"],"metadata":{"id":"SgSsJcK83DfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goJyoKz2zrmy","outputId":"5691f46b-4dd4-4fef-e9c2-32e1c0053b8b","executionInfo":{"status":"ok","timestamp":1763542370146,"user_tz":-300,"elapsed":38,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has 148 words, 90 unique (including special tokens).\n","Special tokens: ['<SOS>', '<EOS>', '<UNK>']\n","Sample words: ['!', ',', '.', 'a', 'after', 'an', 'and', 'at', 'away', 'beak']\n"]}],"source":["data = open('../data/thirsty_crow.txt', 'r').read()\n","words = re.findall(r\"\\w+|[.,!?'\\\";:]\", data.lower())\n","SOS_TOKEN = '<SOS>'  # Start of Sequence\n","EOS_TOKEN = '<EOS>'  # End of Sequence\n","UNK_TOKEN = '<UNK>'  # Unknown word\n","vocab = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(list(set(words)))\n","data_size, vocab_size = len(words), len(vocab)\n","print(f'Data has {data_size} words, {vocab_size} unique (including special tokens).')\n","print(f'Special tokens: {vocab[:3]}')\n","print(f'Sample words: {vocab[3:13]}')"]},{"cell_type":"code","source":["word_to_ix = {w: i for i, w in enumerate(vocab)}"],"metadata":{"id":"xc6oODzj3ijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ix_to_word = {i: w for i, w in enumerate(vocab)}"],"metadata":{"id":"ClysqC-_3wNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __HYPER-PARAMETERS__"],"metadata":{"id":"JhOzRBXz414W"}},{"cell_type":"code","source":["lr = 1e-3 # learning rate\n","seq_len = 25 # times LSTM will be unrolled (Timesteps)\n","hidden_size = 100 # size of hidden units\n","embed_size = 100 # size of word embedding vector"],"metadata":{"id":"InWw7ZqC3xVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __MODEL PARAM INIT__\n","**Initializing weight matrices for the LSTM with word embeddings.**\n","\n","**Embedding Matrix:**\n","- $\\mathbf{W}_{emb} \\in \\mathbb{R}^{V \\times E}$ Word embedding matrix (lookup table)\n","\n","**Gate Weight Matrices (concatenated input $[\\mathbf{h}_{t-1}; \\mathbf{e}_t]$):**\n","- $\\mathbf{W}_f \\in \\mathbb{R}^{H \\times (H+E)}$ Forget gate weights\n","- $\\mathbf{W}_i \\in \\mathbb{R}^{H \\times (H+E)}$ Input gate weights\n","- $\\mathbf{W}_c \\in \\mathbb{R}^{H \\times (H+E)}$ Candidate cell state weights\n","- $\\mathbf{W}_o \\in \\mathbb{R}^{H \\times (H+E)}$ Output gate weights\n","\n","**Output Layer:**\n","- $\\mathbf{W}_y \\in \\mathbb{R}^{V \\times H}$ Hidden-to-output weights\n","\n","**Biases:**\n","- Gate biases: $\\mathbf{b}_f, \\mathbf{b}_i, \\mathbf{b}_c, \\mathbf{b}_o \\in \\mathbb{R}^{H \\times 1}$\n","- Output bias:  $\\mathbf{b}_y \\in \\mathbb{R}^{V \\times 1}$\n","\n","**Total parameters:** $VE + 4H(H+E) + VH + 4H + V$\n","\n","**Where:**  \n","- $V$ = vocabulary size  \n","- $E$ = embedding dimension (100)  \n","- $H$ = hidden size (100)"],"metadata":{"id":"5LRNhINY6CMp"}},{"cell_type":"code","source":["Wemb = np.random.randn(vocab_size, embed_size) * 0.01  # word embeddings (V, E)\n","Wf = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01  # Forget Gate weights (H, H+E)\n","Wi = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01  # Input Gate weights (H, H+E)\n","Wc = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01  # Candidate Cell State weights (H, H+E)\n","Wo = np.random.randn(hidden_size, hidden_size + embed_size) * 0.01  # Output Gate weights (H, H+E)\n","Wy = np.random.randn(vocab_size, hidden_size) * 0.01  # Prediction weights (V, H)\n","bf = np.ones((hidden_size, 1))  # Forget Gate bias (H, 1)\n","bi = np.zeros((hidden_size, 1))  # Input Gate bias (H, 1)\n","bo = np.zeros((hidden_size, 1))  # Output Gate bias (H, 1)\n","bc = np.zeros((hidden_size, 1))  # CCS bias (H, 1)\n","by = np.zeros((vocab_size, 1))  # prediction bias (V, 1)"],"metadata":{"id":"d__udihsZpJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"\n","Wemb: Word embeddings    : {Wemb.shape}\n","Wf: Forget Gate Weights  : {Wf.shape}\n","Wi: Input Gate Weights   : {Wi.shape}\n","Wc: CCS Weights          : {Wc.shape}\n","Wo: Output Gate Weights  : {Wo.shape}\n","Wy: Prediction Weights   : {Wy.shape}\n","bf: Forget Gate bias     : {bf.shape}\n","bi: Input Gate bias      : {bi.shape}\n","bc: CCS bias             : {bc.shape}\n","bo: Output Gate bias     : {bo.shape}\n","by: Prediction bias      : {by.shape}\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UuOwT9cMZoeo","executionInfo":{"status":"ok","timestamp":1763542371564,"user_tz":-300,"elapsed":17,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"04713e2b-bc18-46f1-b48b-e966ca6f7241"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Wemb: Word embeddings    : (90, 100)\n","Wf: Forget Gate Weights  : (100, 200)\n","Wi: Input Gate Weights   : (100, 200)\n","Wc: CCS Weights          : (100, 200)\n","Wo: Output Gate Weights  : (100, 200)\n","Wy: Prediction Weights   : (90, 100)\n","bf: Forget Gate bias     : (100, 1)\n","bi: Input Gate bias      : (100, 1)\n","bc: CCS bias             : (100, 1)\n","bo: Output Gate bias     : (100, 1)\n","by: Prediction bias      : (90, 1)\n","\n"]}]},{"cell_type":"markdown","source":["### __ADAM OPTIMIZER INITIALIZATION__"],"metadata":{"id":"xvWhMsGbeRjl"}},{"cell_type":"code","source":["# Adam hyperparameters\n","beta1 = 0.9\n","beta2 = 0.999\n","epsilon = 1e-8\n","\n","# Adam memory variables (first moment)\n","mWemb = np.zeros_like(Wemb)\n","mWf = np.zeros_like(Wf)\n","mWi = np.zeros_like(Wi)\n","mWc = np.zeros_like(Wc)\n","mWo = np.zeros_like(Wo)\n","mWy = np.zeros_like(Wy)\n","mbf = np.zeros_like(bf)\n","mbi = np.zeros_like(bi)\n","mbc = np.zeros_like(bc)\n","mbo = np.zeros_like(bo)\n","mby = np.zeros_like(by)\n","\n","# Adam memory variables (second moment)\n","vWemb = np.zeros_like(Wemb)\n","vWf = np.zeros_like(Wf)\n","vWi = np.zeros_like(Wi)\n","vWc = np.zeros_like(Wc)\n","vWo = np.zeros_like(Wo)\n","vWy = np.zeros_like(Wy)\n","vbf = np.zeros_like(bf)\n","vbi = np.zeros_like(bi)\n","vbc = np.zeros_like(bc)\n","vbo = np.zeros_like(bo)\n","vby = np.zeros_like(by)\n","# Timestep counter for bias correction\n","t_adam = 0"],"metadata":{"id":"tUZFfUeAePrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","def softmax(z):\n","    exp_z = np.exp(z - np.max(z))\n","    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"],"metadata":{"id":"ehegFzIZmOCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SINGLE LSTM CELL WITH WORD EMBEDDINGS__\n","\n","**LSTM Forward Pass Equations:**\n","\n","$$\n","\\begin{align}\n","\\mathbf{e}_t &= \\mathbf{W}_{emb}[word\\_idx] \\quad &\\text{(word embedding lookup)} \\\\\n","\\mathbf{z}_t &= [\\mathbf{h}_{t-1}; \\mathbf{e}_t] \\quad &\\text{(concatenated input)} \\\\\n","\\mathbf{f}_t &= \\sigma(\\mathbf{W}_f \\mathbf{z}_t + \\mathbf{b}_f) \\quad &\\text{(forget gate)} \\\\\n","\\mathbf{i}_t &= \\sigma(\\mathbf{W}_i \\mathbf{z}_t + \\mathbf{b}_i) \\quad &\\text{(input gate)} \\\\\n","\\tilde{\\mathbf{c}}_t &= \\tanh(\\mathbf{W}_c \\mathbf{z}_t + \\mathbf{b}_c) \\quad &\\text{(candidate cell state)} \\\\\n","\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t \\quad &\\text{(cell state update)} \\\\\n","\\mathbf{o}_t &= \\sigma(\\mathbf{W}_o \\mathbf{z}_t + \\mathbf{b}_o) \\quad &\\text{(output gate)} \\\\\n","\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t) \\quad &\\text{(hidden state)} \\\\\n","\\mathbf{y}_t &= \\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y \\quad &\\text{(output logits)}\n","\\end{align}\n","$$\n","\n","**Where:** $\\sigma$ = sigmoid, $\\odot$ = element-wise multiplication\n","\n","**Reference:** Hochreiter & Schmidhuber (1997) - *Long Short-Term Memory*"],"metadata":{"id":"VkDtomuF8LFZ"}},{"cell_type":"code","source":["def lstm(c_prev, h_prev, word_idx):\n","    \"\"\"\n","    Single LSTM cell with embedding lookup\n","\n","    Inputs:\n","        - c_prev: Previous cell state (H, 1)\n","        - h_prev: Previous hidden state (H, 1)\n","        - word_idx: Integer index of word in vocabulary\n","\n","    Returns:\n","        - ft, it, cct, ot: Gate activations\n","        - ct: Current cell state (H, 1)\n","        - ht: Current hidden state (H, 1)\n","        - yt: Output logits (V, 1)\n","        - et: Word embedding (E, 1)\n","    \"\"\"\n","    et = Wemb[word_idx].reshape(-1, 1)  # (E, 1)\n","    zt = np.concatenate((h_prev, et), axis=0) # (H,1):(E,1)= (H+E,1)\n","    # Forget Gate\n","    zf = np.dot(Wf, zt) + bf # (H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n","    ft = sigmoid(zf) # (H,1)\n","    #Input Gate\n","    zi = np.dot(Wi, zt) + bi # (H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n","    it = sigmoid(zi) # (H,1)\n","    #Candidate Cell State\n","    zc = np.dot(Wc, zt) + bc # (H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n","    cct = np.tanh(zc) # (H,1)\n","    #Cell State\n","    ct = ft * c_prev + it * cct # (H,1)*(H,1)+(H,1)*(H,1)=(H,1)\n","    #Output Gate\n","    zo = np.dot(Wo, zt) + bo # (H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n","    ot = sigmoid(zo) # (H,1)\n","    #Hidden State\n","    ht = ot * np.tanh(ct) # (H,1)*(H,1)=(H,1)\n","    #Output Logits\n","    yt = np.dot(Wy, ht) + by # #(V,H)@(H,1)->(V,1)+(V,1)=(V,1)\n","    return ft, it, cct, ot, ct, ht, yt, et"],"metadata":{"id":"AVvRr4GJ7SmG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding the LSTM Cell with Embeddings:**\n","\n","**What's happening here?**\n","- At each timestep $t$, the LSTM takes three inputs:\n","  1. Current word index → lookup in $\\mathbf{W}_{emb}$ to get dense vector $\\mathbf{e}_t$\n","  2. Previous hidden state $\\mathbf{h}_{t-1}$ (short-term working memory)\n","  3. Previous cell state $\\mathbf{c}_{t-1}$ (long-term memory)\n","  \n","- It outputs:\n","  1. New hidden state $\\mathbf{h}_t$ (short-term working memory)\n","  2. New cell state $\\mathbf{c}_t$ (long-term memory)\n","  3. Output logits $\\mathbf{y}_t$ (before softmax)\n","  4. Embedding $\\mathbf{e}_t$ (needed for backprop!)\n"],"metadata":{"id":"4f5wu1Tj9rbm"}},{"cell_type":"markdown","source":["### __FORWARD PASS__\n"],"metadata":{"id":"JSiqg1u1Fcuo"}},{"cell_type":"code","source":["def forward(inputs, targets, c_prev, h_prev):\n","    \"\"\"\n","    Forward pass through LSTM Cell with word embeddings\n","\n","    Inputs:\n","        - inputs: List of word indices, e.g., [5, 8, 12, 12, 15] for \"once upon a time there\"\n","        - targets: List of target word indices (inputs shifted by 1)\n","        - c_prev: Initial cell state from previous sequence, shape (H, 1)\n","        - h_prev: Initial hidden state from previous sequence, shape (H, 1)\n","\n","    Returns:\n","        - word_indices: Dict of word indices {0: word_0, 1: word_1, ...}\n","        - et: Dict of word embeddings {0: e_0, 1: e_1, ...}\n","        - ht: Dict of hidden states {-1: h_init, 0: h_0, 1: h_1, ...}\n","        - ct: Dict of cell states {-1: c_init, 0: c_0, 1: c_1, ...}\n","        - yt: Dict of output logits {0: y_0, 1: y_1, ...}\n","        - probt: Dict of probability distributions {0: p_0, 1: p_1, ...}\n","        - loss: Total cross-entropy loss across all timesteps (scalar)\n","    \"\"\"\n","    # Initialize storage dictionaries\n","    word_indices = {}  # Store word indices\n","    et = {}  # Store embeddings\n","    ft = {}  # Store forget gate values\n","    it = {}  # Store input gate values\n","    cct = {} # Store ccs values\n","    ot = {}  # Store output gate values\n","    ht = {}  # Store hidden states\n","    ct = {}  # Store cell states\n","    yt = {}  # Store output logits\n","    probt = {}  # Store probability distributions (after softmax)\n","\n","    # Set initial hidden and cell state\n","    ht[-1] = np.copy(h_prev) #(H,1)\n","    ct[-1] = np.copy(c_prev) #(H,1)\n","    loss = 0\n","    # Loop through each timestep in the sequence\n","    for t in range(len(inputs)):\n","        # Step 1: Store word index\n","        word_indices[t] = inputs[t]\n","        # Step 2: Run LSTM cell (forward computation)\n","        ft[t], it[t], cct[t], ot[t], ct[t], ht[t], yt[t], et[t] = lstm(ct[t-1], ht[t-1], word_indices[t])\n","        # Step 3: Apply softmax to get probabilities\n","        probt[t] = softmax(yt[t])  # (V,1)\n","        # Step 4: Compute loss for this timestep\n","        loss += -np.log(probt[t][targets[t], 0]+ epsilon)\n","\n","    cache = (h_prev, c_prev, ft, it, cct, ot)\n","    return word_indices, et, ht, ct, probt, loss, cache"],"metadata":{"id":"ErpAgrEEEpld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __BACKWARD PASS (BPTT FOR LSTM)__\n","**Complete LSTM Backpropagation Through Time Equations**\n","#### **Step 1: Output Layer Gradient (Softmax + Cross-Entropy)**\n","\n","$$\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} = \\mathbf{p}_t - \\mathbf{1}_{y^*_t}$$\n","\n","Where $\\mathbf{p}_t$ is the predicted probability distribution and $\\mathbf{1}_{y^*_t}$ is the one-hot vector of the true label.\n","\n","**Output layer weight gradients:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\mathbf{h}_t^T$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t}$$\n","\n","#### **Step 2: Hidden State Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} = \\mathbf{W}_y^T \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}}$$\n","\n","The gradient flows from two sources:\n","- Current timestep's output loss (first term)\n","- Future timestep's hidden state (second term)\n","\n","\n","#### **Step 3: Output Gate Gradients**\n","\n","Recall: $\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)$\n","\n","**Gradient w.r.t. output gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot \\tanh(\\mathbf{c}_t)$$\n","\n","**Gradient w.r.t. output gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_o} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} \\odot \\mathbf{o}_t \\odot (1 - \\mathbf{o}_t)$$\n","\n","(Using sigmoid derivative: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$)\n","\n","#### **Step 4: Cell State Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot \\mathbf{o}_t \\odot (1 - \\tanh^2(\\mathbf{c}_t)) + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t+1}}$$\n","\n","**Key Insight:** The cell state gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$ flows through forget gates across timesteps, allowing long-range gradient propagation without vanishing!\n","\n","#### **Step 5: Candidate Cell State Gradients**\n","\n","Recall: $\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t$\n","**Gradient w.r.t. candidate cell (after tanh):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{c}}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{i}_t$$\n","**Gradient w.r.t. candidate cell pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_c} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{c}}_t} \\odot (1 - \\tilde{\\mathbf{c}}_t^2)$$\n","\n","(Using tanh derivative: $\\tanh'(z) = 1 - \\tanh^2(z)$)\n","#### **Step 6: Input Gate Gradients**\n","**Gradient w.r.t. input gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{i}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\tilde{\\mathbf{c}}_t$$\n","**Gradient w.r.t. input gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{i}_t} \\odot \\mathbf{i}_t \\odot (1 - \\mathbf{i}_t)$$\n","#### **Step 7: Forget Gate Gradients**\n","**Gradient w.r.t. forget gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{f}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{c}_{t-1}$$\n","**Gradient w.r.t. forget gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_f} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{f}_t} \\odot \\mathbf{f}_t \\odot (1 - \\mathbf{f}_t)$$\n","#### **Step 8: Weight Matrix Gradients (All Gates)**\n","For each gate $g \\in \\{f, i, c, o\\}$:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_g} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_g} \\mathbf{z}_t^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_g} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_g}$$\n","Where $\\mathbf{z}_t = [\\mathbf{h}_{t-1}; \\mathbf{e}_t]$ is the concatenated input (hidden state + word embedding).\n","#### **Step 9: Gradient to Previous Timestep**\n","**Gradient w.r.t. concatenated input:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t} = \\sum_{g \\in \\{f,i,c,o\\}} \\mathbf{W}_g^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_g}$$\n","**Split to get gradient for previous hidden state:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t}[0:H]$$\n","(First $H$ elements of $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_t}$)\n","#### **Step 10: Cell State Gradient to Previous Timestep**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\odot \\mathbf{f}_t$$\n","This gradient flows backward through time via the forget gate, enabling long-term memory.\n","**Implementation Note:** Gradients are accumulated across all timesteps (sums in Step 8), then clipped to prevent exploding gradients before parameter updates.\n","\n","**Notation:**\n","- $\\odot$ = element-wise multiplication\n","- $\\sigma$ = sigmoid function\n","- $T$ = sequence length\n","- $H$ = hidden dimension"],"metadata":{"id":"7CTgWopPOAka"}},{"cell_type":"code","source":["def backward(inputs, targets, word_indices, et, ht, ct, probt, cache):\n","    \"\"\"\n","    Backpropagation Through Time (BPTT) for LSTM with word embeddings\n","\n","    Inputs:\n","        - inputs: List of input word indices\n","        - targets: List of target word indices\n","        - word_indices: Dict of word indices from forward pass\n","        - et: Dict of word embeddings from forward pass\n","        - ht: Dict of hidden states from forward pass\n","        - ct: Dict of cell states from forward pass\n","        - probt: Dict of probability distributions from forward pass\n","        - cache: Tuple (h_prev, c_prev, ft, it, cct, ot) from forward pass\n","\n","    Returns:\n","        - dWemb: Gradient w.r.t. word embeddings\n","        - dWf, dWi, dWc, dWo: Gradients for gate weight matrices\n","        - dWy: Gradient w.r.t. hidden-to-output weights\n","        - dbf, dbi, dbc, dbo: Gradients for gate biases\n","        - dby: Gradient w.r.t. output bias\n","    \"\"\"\n","    h_prev, c_prev, ft, it, cct, ot = cache\n","    dWemb = np.zeros_like(Wemb)  # (V, E)\n","    dWf = np.zeros_like(Wf)  # (H, H+E)\n","    dWi = np.zeros_like(Wi)  # (H, H+E)\n","    dWc = np.zeros_like(Wc)  # (H, H+E)\n","    dWo = np.zeros_like(Wo)  # (H, H+E)\n","    dWy = np.zeros_like(Wy)  # (V, H)\n","\n","    dbf = np.zeros_like(bf)  # (H, 1)\n","    dbi = np.zeros_like(bi)  # (H, 1)\n","    dbc = np.zeros_like(bc)  # (H, 1)\n","    dbo = np.zeros_like(bo)  # (H, 1)\n","    dby = np.zeros_like(by)  # (V, 1)\n","\n","    # Gradient of hidden state and cell state at next timestep (initially zero)\n","    dh_next = np.zeros_like(ht[0])  # (H, 1)\n","    dc_next = np.zeros_like(ct[0])  # (H, 1)\n","\n","    # Backpropagate through time (from last to first timestep)\n","    for t in reversed(range(len(inputs))):\n","        # Step 1: Gradient of loss w.r.t output probabilities\n","        dy = np.copy(probt[t])  # (V, 1)\n","        dy[targets[t]] -= 1  # Subtract 1 from correct class (cross-entropy gradient)\n","\n","        # Step 2: Gradients for output layer (Wy, by)\n","        dWy += np.dot(dy, ht[t].T)  # (V,1)@(1,H) = (V,H)\n","        dby += dy  # (V, 1)\n","\n","        # Step 3: Gradient w.r.t hidden state\n","        # Comes from two sources: current output and next timestep\n","        dh = np.dot(Wy.T, dy) + dh_next  # (H,V)@(V,1) + (H,1) = (H,1)\n","\n","        # Step 4: Gradient w.r.t output gate\n","        tanh_ct = np.tanh(ct[t])  # (H, 1)\n","        do = dh * tanh_ct  # (H,1) * (H,1) = (H,1)\n","        dzo = do * ot[t] * (1 - ot[t])  # (H,1)*(H,1)*(H,1)=(H,1)\n","\n","        # Step 5: Gradient w.r.t cell state\n","        dc = dh * ot[t] * (1 - tanh_ct**2)  # (H,1)*(H,1)*(H,1) = (H,1)\n","        dc = dc + dc_next  # gradient from next timestep will be added\n","\n","        # Step 6: Gradient w.r.t candidate cell state\n","        dcc = dc * it[t]  # (H,1) * (H,1) = (H,1)\n","        dzcc = dcc * (1 - cct[t]**2)  # (H,1)\n","\n","        # Step 7: Gradient w.r.t input gate\n","        di = dc * cct[t]  # (H,1) * (H,1) = (H,1)\n","        dzi = di * it[t] * (1 - it[t])  # (H,1)\n","\n","        # Step 8: Gradient w.r.t forget gate\n","        df = dc * ct[t-1]  # (H,1) * (H,1) = (H,1)\n","        dzf = df * ft[t] * (1 - ft[t])  # (H,1)\n","\n","        # Step 9: Create concatenated input [h_{t-1}, e_t]\n","        zt = np.concatenate((ht[t-1], et[t]), axis=0)  # (H+E, 1)\n","\n","        # Step 10: Gradients for gate weight matrices and biases\n","        dWf += np.dot(dzf, zt.T)  # (H,1)@(1,H+E) = (H,H+E)\n","        dbf += dzf  # (H,1)\n","\n","        dWi += np.dot(dzi, zt.T)  # (H,1)@(1,H+E) = (H,H+E)\n","        dbi += dzi  # (H,1)\n","\n","        dWc += np.dot(dzcc, zt.T)  # (H,1)@(1,H+E) = (H,H+E)\n","        dbc += dzcc  # (H,1)\n","\n","        dWo += np.dot(dzo, zt.T)  # (H,1)@(1,H+E) = (H,H+E)\n","        dbo += dzo  # (H,1)\n","\n","        # Step 11: Gradient w.r.t concatenated input [h_t-1, e_t]\n","        dzt = np.dot(Wf.T, dzf) + np.dot(Wi.T, dzi) + np.dot(Wc.T, dzcc) + np.dot(Wo.T, dzo) #(H+E, 1)\n","\n","        # Step 12: Split gradient and pass to previous timestep\n","        dh_next = dzt[:hidden_size, :]  # First H elements: (H,1)\n","        de = dzt[hidden_size:, :]  # Last E elements: (E,1)\n","\n","        # Step 13: Accumulate gradient for word embedding\n","        # accumulate gradients for the same word across timesteps\n","        dWemb[word_indices[t]] += de.reshape(-1)  # (E,)\n","\n","        # Step 14: Gradient w.r.t previous cell state\n","        dc_next = dc * ft[t]  # (H,1) * (H,1) = (H,1)\n","\n","        # Clip gradients to prevent exploding gradients\n","    for dparam in [dWemb, dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby]:\n","        np.clip(dparam, -5, 5, out=dparam)\n","\n","    return dWemb, dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby"],"metadata":{"id":"07BaraxIyIst"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __UPDATE PARAMS WITH ADAM__"],"metadata":{"id":"whAHx2WTW03k"}},{"cell_type":"code","source":["def update_parameters(dWemb, dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby, learning_rate):\n","    \"\"\"\n","    Update LSTM model parameters using Adam optimizer with bias correction\n","\n","    Inputs:\n","        - dWemb: Gradient for word embeddings (V, E)\n","        - dWf: Gradient for forget gate weights (H, H+E)\n","        - dWi: Gradient for input gate weights (H, H+E)\n","        - dWc: Gradient for cell candidate weights (H, H+E)\n","        - dWo: Gradient for output gate weights (H, H+E)\n","        - dWy: Gradient for prediction weights (V, H)\n","        - dbf, dbi, dbc, dbo: Gradients for gate biases (H, 1)\n","        - dby: Gradient for output bias (V, 1)\n","        - learning_rate: Step size for parameter updates\n","\n","    Returns:\n","        - None (updates global parameters in-place)\n","    \"\"\"\n","    global Wemb, Wf, Wi, Wc, Wo, Wy, bf, bi, bc, bo, by\n","    global mWemb, mWf, mWi, mWc, mWo, mWy, mbf, mbi, mbc, mbo, mby\n","    global vWemb, vWf, vWi, vWc, vWo, vWy, vbf, vbi, vbc, vbo, vby\n","    global t_adam\n","\n","    t_adam += 1\n","    params = [\n","        (Wemb, dWemb, mWemb, vWemb),\n","        (Wf, dWf, mWf, vWf),\n","        (Wi, dWi, mWi, vWi),\n","        (Wc, dWc, mWc, vWc),\n","        (Wo, dWo, mWo, vWo),\n","        (Wy, dWy, mWy, vWy),\n","        (bf, dbf, mbf, vbf),\n","        (bi, dbi, mbi, vbi),\n","        (bc, dbc, mbc, vbc),\n","        (bo, dbo, mbo, vbo),\n","        (by, dby, mby, vby)\n","    ]\n","    updated = []\n","    for param, grad, m, v in params:\n","        m = beta1 * m + (1 - beta1) * grad\n","        v = beta2 * v + (1 - beta2) * (grad ** 2)\n","        # Bias correction\n","        m_corrected = m / (1 - beta1 ** t_adam)\n","        v_corrected = v / (1 - beta2 ** t_adam)\n","        param = param - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n","        updated.append((param, m, v))\n","\n","    (Wemb, mWemb, vWemb), (Wf, mWf, vWf), (Wi, mWi, vWi), (Wc, mWc, vWc), (Wo, mWo, vWo), (Wy, mWy, vWy), \\\n","    (bf, mbf, vbf), (bi, mbi, vbi), (bc, mbc, vbc), (bo, mbo, vbo), (by, mby, vby) = updated"],"metadata":{"id":"wSGIUqZsMU0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __TRAIN MODEL__\n"],"metadata":{"id":"hpHLQqh7XFil"}},{"cell_type":"code","source":["def train(words_list, num_iterations=1000, print_every=100, sample_every=100):\n","    \"\"\"\n","    Train word-level LSTM language model\n","\n","    Inputs:\n","        - words_list: List of words from training text\n","        - num_iterations: Number of training iterations (not full data passes)\n","        - print_every: Print loss every N iterations\n","        - sample_every: Generate sample text every N iterations\n","\n","    Returns:\n","        - smooth_loss: Exponentially smoothed loss value\n","\n","    Note: Both hidden state h_prev AND cell state c_prev are maintained across\n","          sequences for temporal continuity. Only reset when reaching end of\n","          data or on first iteration.\n","    \"\"\"\n","    smooth_loss = -np.log(1.0 / vocab_size) * seq_len\n","    n = 0  # Iterations counter\n","    p = 0  # Data pointer (position in word_list)\n","    h_prev = np.zeros((hidden_size, 1))\n","    c_prev = np.zeros((hidden_size, 1))\n","\n","    while n < num_iterations:\n","        # Reset pointer, hidden state, AND cell state at end of data or first iteration\n","        if p + seq_len + 1 >= len(words_list) or n == 0:\n","            h_prev = np.zeros((hidden_size, 1))  # Fresh start\n","            c_prev = np.zeros((hidden_size, 1))  # Fresh cell state\n","            p = 0  # Go back to beginning\n","\n","        # Input:  words at positions [p, p+1, ..., p+seq_len-1]\n","        # Target: words at positions [p+1, p+2, ..., p+seq_len]\n","        inputs = [word_to_ix[w] for w in words_list[p:p+seq_len]]\n","        targets = [word_to_ix[w] for w in words_list[p+1:p+seq_len+1]]\n","\n","        # Forward pass (pass both hidden state AND cell state!)\n","        word_indices, et, ht, ct, probt, loss, cache = forward(inputs, targets, c_prev, h_prev)\n","\n","        # Update hidden state AND cell state for next sequence\n","        h_prev = np.copy(ht[len(inputs) - 1])\n","        c_prev = np.copy(ct[len(inputs) - 1])\n","\n","        # Backward pass\n","        dWemb, dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby = backward(inputs, targets, word_indices, et, ht, ct, probt, cache)\n","\n","        # Update parameters\n","        update_parameters(dWemb, dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby, lr)\n","\n","        # Update smooth loss\n","        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","\n","        # Print progress\n","        if n % print_every == 0:\n","            print(f\"Iter {n:6d} | Loss: {smooth_loss:.4f}\")\n","        # Generate sample text\n","        if n % sample_every == 0 and n > 0:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"SAMPLE at iteration {n}:\")\n","            print(f\"{'='*60}\")\n","            sample_text = sample(seed_word=None, n_words=50)\n","            print(sample_text)\n","            print(f\"{'='*60}\\n\")\n","        p += seq_len\n","\n","        n += 1\n","    print(\"\\nTraining complete!\")\n","    return smooth_loss"],"metadata":{"id":"HvrBIPp3XAt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SAMPLING FUNCTION (GENERATE TEXT)__"],"metadata":{"id":"j4GOFoOuYzvD"}},{"cell_type":"code","source":["def sample(seed_word=None, n_words=50, use_sos=True, stop_at_eos=True):\n","    \"\"\"\n","    Generate text by sampling from the trained word-level LSTM language model\n","\n","    Inputs:\n","        - seed_word: Starting word for text generation (if None, uses <SOS>)\n","        - n_words: Maximum number of words to generate (default 50)\n","        - use_sos: If True and seed_word is None, start with <SOS> token\n","        - stop_at_eos: If True, stop generation when <EOS> token is sampled\n","\n","    Returns:\n","        - String of generated text\n","    \"\"\"\n","    # Initialize with seed word or SOS token\n","    if seed_word is None:\n","        if use_sos:\n","            word_idx = word_to_ix[SOS_TOKEN]\n","            generated_words = []  # Don't include SOS in output\n","        else:\n","            # Start with first regular word (after special tokens)\n","            word_idx = word_to_ix[vocab[3]]\n","            generated_words = [ix_to_word[word_idx]]\n","    else:\n","        # Handle seed word not in vocabulary\n","        word_idx = word_to_ix.get(seed_word.lower(), word_to_ix[UNK_TOKEN])\n","        if word_idx == word_to_ix[UNK_TOKEN]:\n","            print(f\"Warning: '{seed_word}' not in vocab. Using <UNK> token.\")\n","        generated_words = [ix_to_word[word_idx]]\n","\n","    h = np.zeros((hidden_size, 1))  # Initialize hidden state\n","    c = np.zeros((hidden_size, 1))  # Initialize cell state\n","\n","    for _ in range(n_words):\n","        # Run LSTM cell\n","        _, _, _, _, c, h, y, _ = lstm(c, h, word_idx)\n","        p = softmax(y)\n","        word_idx = np.random.choice(range(vocab_size), p=p.ravel())\n","        word = ix_to_word[word_idx]\n","        if stop_at_eos and word == EOS_TOKEN:\n","            break\n","        # Skip adding special tokens to output\n","        if word not in [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]:\n","            generated_words.append(word)\n","\n","    return ' '.join(generated_words)"],"metadata":{"id":"ldl7q4JsYWJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __RUN TRAINING__"],"metadata":{"id":"1y0M3vMIZcjn"}},{"cell_type":"code","source":["final_loss = train(words, 10000, print_every=500, sample_every=2000)\n","print(f\"\\nFinal smooth loss: {final_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjfsldMiZVna","outputId":"495e624b-b4e5-4c46-b483-5724b79e651b","executionInfo":{"status":"ok","timestamp":1763542501676,"user_tz":-300,"elapsed":123680,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter      0 | Loss: 112.4952\n","Iter    500 | Loss: 101.4359\n","Iter   1000 | Loss: 76.1447\n","Iter   1500 | Loss: 50.9899\n","Iter   2000 | Loss: 32.7823\n","\n","============================================================\n","SAMPLE at iteration 2000:\n","============================================================\n","and flew small small crow up looked small small small them away thirsty by moment . a long time , and a thought in brightly , and but after flying for a long under , the crow finally saw a pitcher a quickly flew down and thought little water at\n","============================================================\n","\n","Iter   2500 | Loss: 20.6978\n","Iter   3000 | Loss: 12.9231\n","Iter   3500 | Loss: 8.0341\n","Iter   4000 | Loss: 4.9949\n","\n","============================================================\n","SAMPLE at iteration 4000:\n","============================================================\n","upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   4500 | Loss: 3.1113\n","Iter   5000 | Loss: 1.9442\n","Iter   5500 | Loss: 1.2203\n","Iter   6000 | Loss: 0.7703\n","\n","============================================================\n","SAMPLE at iteration 6000:\n","============================================================\n","got an picking up small small small\n","============================================================\n","\n","Iter   6500 | Loss: 0.4895\n","Iter   7000 | Loss: 0.3136\n","Iter   7500 | Loss: 0.2028\n","Iter   8000 | Loss: 0.1325\n","\n","============================================================\n","SAMPLE at iteration 8000:\n","============================================================\n","inside , a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher\n","============================================================\n","\n","Iter   8500 | Loss: 0.0877\n","Iter   9000 | Loss: 0.0587\n","Iter   9500 | Loss: 0.0399\n","\n","Training complete!\n","\n","Final smooth loss: 0.0275\n"]}]},{"cell_type":"markdown","source":["### __TEST DIFFERENT SEEDS__"],"metadata":{"id":"jhx3NWCEtl0J"}},{"cell_type":"code","source":["seed_words = ['The', 'he', 'thirsty', 'wood', 'shiny']\n","for word in seed_words:\n","    generated_word = sample(word, n_words=150)\n","    print(word, ':', generated_word)\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_1JWyvyZi7B","outputId":"e62ac003-f7b1-4a69-8031-34071c37a0bb","executionInfo":{"status":"ok","timestamp":1763542501778,"user_tz":-300,"elapsed":84,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The : the search picking up small small pebbles small pebbles small continued water quickly picking up small small pebbles small was time upon upon upon upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came came came\n","------------------------------------------------------------\n","he : he up small small small small small small small small pebbles tired pebbles small small small small small very small small small in he pitcher upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came came\n","------------------------------------------------------------\n","thirsty : thirsty crow there bottom , a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came came came a quickly rise . the bottom , but his beak could not reach it . the crow thought for a\n","------------------------------------------------------------\n","Warning: 'wood' not in vocab. Using <UNK> token.\n","wood : <UNK> happily very . , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came came came came came feeling quickly idea ! water was came came came brightly , and the poor crow was feeling tired and weak .\n","------------------------------------------------------------\n","Warning: 'shiny' not in vocab. Using <UNK> token.\n","shiny : <UNK> under upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a tree . he quickly flew down and looked inside . there was a little water at the bottom , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came came came came bottom , but his beak could not reach it . the crow thought for a moment . then he got an\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Test with SOS token (no seed word)\n","for i in range(3):\n","    generated = sample(seed_word=None, n_words=30, use_sos=True, stop_at_eos=True)\n","    print(f\"Sample {i+1}: {generated}\")\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnJ4WRiCUd_S","executionInfo":{"status":"ok","timestamp":1763542501817,"user_tz":-300,"elapsed":32,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"74acdde2-06f3-44e5-fdcb-941e34e4a8bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample 1: sun small upon upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and\n","------------------------------------------------------------\n","Sample 2: finally an picking up small pebbles small was pitcher high beak up small pitcher finally saw reach upon a time , on a very hot day , a thirsty\n","------------------------------------------------------------\n","Sample 3: thought moment , a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Test with unknown word\n","generated = sample('elephant', n_words=30, stop_at_eos=True)\n","print(f\"elephant: {generated}\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsFe5WPyUhvI","executionInfo":{"status":"ok","timestamp":1763542501848,"user_tz":-300,"elapsed":25,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"096100f8-4bcd-4f0a-e740-1004c784a91c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: 'elephant' not in vocab. Using <UNK> token.\n","elephant: <UNK> of small upon upon upon upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly\n","============================================================\n"]}]},{"cell_type":"markdown","source":["### **References:**\n","1. Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735-1780. [**Original LSTM Paper**]\n","2. Gers, F. A., et al. (2000). *Learning to Forget: Continual Prediction with LSTM*. Neural Computation, 12(10), 2451-2471.\n","\n","### **Further Reading:**\n","1. Hochreiter & Schmidhuber (1997). [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) — Original LSTM paper\n","2. Olah, C. (2015). [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) — Visual explanations\n","3. Cho et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for SMT — Introduced GRUs\n","4. Vaswani et al. (2017). Attention Is All You Need. NeurIPS — Transformers replacing LSTMs\n","5. Goodfellow et al. (2016). Deep Learning. Chapter 10: Sequence Modeling — Comprehensive theory\n","6. Graves, A. (2013). Generating Sequences With Recurrent Neural Networks — Advanced LSTM techniques"],"metadata":{"id":"HhZHDwtPMo4E"}}]}