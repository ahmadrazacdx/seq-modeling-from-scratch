{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Word GRU Language Model**\n","\n","## **From Scratch Implementation in NumPy**\n","\n","This notebook implements a **vanilla GRU** for word-level language modeling.\n","\n","\n","### **What We'll Build:**\n","- **Vanilla GRU** with two (update, reset) gates\n","- **Backpropagation** for gradient computation\n","- **Word-level text generation** from learned patterns\n","---\n","*Notebook by*: Ahmad Raza [@ahmadrazacdx](https://github.com/ahmadrazacdx)<br>\n","*Date: 2025*  \n","*License: MIT*"],"metadata":{"id":"kv99qL16D9Kj"}},{"cell_type":"code","source":["import re\n","import numpy as np\n","np.random.seed(42)"],"metadata":{"id":"SgSsJcK83DfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goJyoKz2zrmy","outputId":"e243ecba-d046-493d-933f-f2eabe224f64","executionInfo":{"status":"ok","timestamp":1763544791569,"user_tz":-300,"elapsed":69,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data has 148 words, 90 unique (including special tokens).\n","Special tokens: ['<SOS>', '<EOS>', '<UNK>']\n","Sample words: ['!', ',', '.', 'a', 'after', 'an', 'and', 'at', 'away', 'beak']\n"]}],"source":["data = open('../data/thirsty_crow.txt', 'r').read()\n","words = re.findall(r\"\\w+|[.,!?'\\\";:]\", data.lower())\n","SOS_TOKEN = '<SOS>'  # Start of Sequence\n","EOS_TOKEN = '<EOS>'  # End of Sequence\n","UNK_TOKEN = '<UNK>'  # Unknown word\n","vocab = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(list(set(words)))\n","data_size, vocab_size = len(words), len(vocab)\n","print(f'Data has {data_size} words, {vocab_size} unique (including special tokens).')\n","print(f'Special tokens: {vocab[:3]}')\n","print(f'Sample words: {vocab[3:13]}')"]},{"cell_type":"code","source":["word_to_ix = {w: i for i, w in enumerate(vocab)}"],"metadata":{"id":"xc6oODzj3ijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ix_to_word = {i: w for i, w in enumerate(vocab)}"],"metadata":{"id":"ClysqC-_3wNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __HYPER-PARAMETERS__"],"metadata":{"id":"JhOzRBXz414W"}},{"cell_type":"code","source":["lr = 1e-3 # learning rate\n","seq_len = 25 # times GRU will be unrolled (Timesteps)\n","hidden_size = 100 # size of hidden units\n","embed_size = 100 # size of word embedding vector"],"metadata":{"id":"InWw7ZqC3xVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __MODEL PARAM INIT__\n","**Initializing weight matrices for the GRU.**\n","\n","**Embedding Matrix:**\n","- $\\mathbf{W}_{emb} \\in \\mathbb{R}^{V \\times E}$ Word embedding matrix (lookup table)\n","\n","\n","**Gate Weight Matrices (concatenated input $[\\mathbf{h}_{t-1}; \\mathbf{x}_t]$):**\n","- $\\mathbf{W}_u \\in \\mathbb{R}^{H \\times (H+E)}$ Update gate weights\n","- $\\mathbf{W}_r \\in \\mathbb{R}^{H \\times (H+E)}$ Reset gate weights\n","- $\\mathbf{W}_h \\in \\mathbb{R}^{H \\times (H+E)}$ Candidate hidden state weights\n","\n","**Output Layer:**\n","- $\\mathbf{W}_y \\in \\mathbb{R}^{V \\times H}$ Hidden-to-output weights\n","\n","**Biases:**\n","- Gate biases: $\\mathbf{b}_u, \\mathbf{b}_r, \\mathbf{b}_h \\in \\mathbb{R}^{H \\times 1}$\n","- Output bias: $\\mathbf{b}_y \\in \\mathbb{R}^{V \\times 1}$\n","\n","**Total parameters:** $VE + 3H(H+E) + VH + 3H + V$ <br>\n","\n","**Where:**  \n","- $V$ = vocabulary size  \n","- $E$ = embedding dimension (100)  \n","- $H$ = hidden size (100)"],"metadata":{"id":"5LRNhINY6CMp"}},{"cell_type":"code","source":["Wemb = np.random.randn(vocab_size, embed_size) * 0.01  # word embeddings (V, E)\n","Wu = np.random.randn(hidden_size, hidden_size+embed_size)*0.01 # Update Gate weights (H, H+E)\n","Wr = np.random.randn(hidden_size, hidden_size+embed_size)*0.01 # Reset Gate weights (H, H+E)\n","Wh = np.random.randn(hidden_size, hidden_size+embed_size)*0.01 # Candidate Hidden State weights (H, H+E)\n","Wy = np.random.randn(vocab_size, hidden_size)*0.01 # Prediction weights (V, H)\n","bu = np.zeros((hidden_size, 1)) # Update Gate bias (H, 1)\n","br = np.zeros((hidden_size, 1)) # Reset Gate bias(H, 1)\n","bh = np.zeros((hidden_size, 1)) # CHS bias (H, 1)\n","by = np.zeros((vocab_size, 1)) # prediction bias (V, 1)"],"metadata":{"id":"NPbbN7zi5jjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"\n","Wemb: Word Embeddings    : {Wemb.shape}\n","Wu: Update Gate Weights  : {Wu.shape}\n","Wr: Reset Gate Weights   : {Wr.shape}\n","Wh: CHS Weights          : {Wh.shape}\n","Wy: Prediction Weights   : {Wy.shape}\n","bu: Update Gate bias     : {bu.shape}\n","br: Reset Gate bias      : {br.shape}\n","bh: CHS bias             : {bh.shape}\n","by: Prediction bias      : {by.shape}\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HGbspWj7RRE","outputId":"5b651755-28d6-41b5-f53a-216c69633bad","executionInfo":{"status":"ok","timestamp":1763544795220,"user_tz":-300,"elapsed":26,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Wemb: Word Embeddings    : (90, 100)\n","Wu: Update Gate Weights  : (100, 200)\n","Wr: Reset Gate Weights   : (100, 200)\n","Wh: CHS Weights          : (100, 200)\n","Wy: Prediction Weights   : (90, 100)\n","bu: Update Gate bias     : (100, 1)\n","br: Reset Gate bias      : (100, 1)\n","bh: CHS bias             : (100, 1)\n","by: Prediction bias      : (90, 1)\n","\n"]}]},{"cell_type":"markdown","source":["### __ADAM OPTIMIZER INITIALIZATION__"],"metadata":{"id":"xvWhMsGbeRjl"}},{"cell_type":"code","source":["# Adam hyperparameters\n","beta1 = 0.9\n","beta2 = 0.999\n","epsilon = 1e-8\n","\n","# Adam memory variables (first moment)\n","mWemb = np.zeros_like(Wemb)\n","mWu = np.zeros_like(Wu)\n","mWr = np.zeros_like(Wr)\n","mWh = np.zeros_like(Wh)\n","mWy = np.zeros_like(Wy)\n","mbu = np.zeros_like(bu)\n","mbr = np.zeros_like(br)\n","mbh = np.zeros_like(bh)\n","mby = np.zeros_like(by)\n","\n","# Adam memory variables (second moment)\n","vWemb = np.zeros_like(Wemb)\n","vWu = np.zeros_like(Wu)\n","vWr = np.zeros_like(Wr)\n","vWh = np.zeros_like(Wh)\n","vWy = np.zeros_like(Wy)\n","vbu = np.zeros_like(bu)\n","vbr = np.zeros_like(br)\n","vbh = np.zeros_like(bh)\n","vby = np.zeros_like(by)\n","# Timestep counter for bias correction\n","t_adam = 0"],"metadata":{"id":"tUZFfUeAePrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","def softmax(z):\n","    exp_z = np.exp(z - np.max(z))\n","    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"],"metadata":{"id":"ehegFzIZmOCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SINGLE GRU CELL__\n","**Note:** This notebook follows PyTorch's GRU equations for implementation.\n","**GRU Forward Pass Equations with Embeddings:**\n","\n","$$\\mathbf{e}_t = \\mathbf{W}_{emb}[word\\_idx] \\quad \\text{(word embedding lookup)}$$\n","\n","**Update Gate:**\n","\n","$$\\mathbf{z}_u = \\mathbf{W}_u[\\mathbf{h}_{t-1}; \\mathbf{e}_t] + \\mathbf{b}_u$$\n","$$\\mathbf{u}_t = \\sigma(\\mathbf{z}_u)$$\n","\n","**Reset Gate:**\n","\n","$$\\mathbf{z}_r = \\mathbf{W}_r[\\mathbf{h}_{t-1}; \\mathbf{e}_t] + \\mathbf{b}_r$$\n","$$\\mathbf{r}_t = \\sigma(\\mathbf{z}_r)$$\n","\n","**Candidate Hidden State:**\n","\n","$$\\mathbf{z}_h =  W_{h,x} x_t  + r_t \\odot ( W_{h,h} h_{t-1}) + b_{h}$$\n","$$\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{z}_h)$$\n","\n","**Hidden State (Interpolation):**\n","\n","$$h_t = (1 - u_t) \\tilde{\\mathbf{h}}_t  + u_t \\odot h_{t-1}$$\n","\n","**Output:**\n","\n","$$\\mathbf{z}_y = \\mathbf{W}_y\\mathbf{h}_t + \\mathbf{b}_y$$\n","$$\\mathbf{p}_t = \\text{softmax}(\\mathbf{z}_y)$$\n","\n","**Where:**  $\\sigma$: sigmoid, $\\odot$: element-wise multiplication\n","\n","**GRU Equations: Paper vs PyTorch**  \n","\n","| Component | Paper (Cho et al., 2014) | PyTorch Implementation |\n","|-----------|---------------------------|----------------------|\n","| **Hidden State**| $$h_t = (1 - u_t) \\odot h_{t-1} + u_t \\tilde{\\mathbf{h}}_t$$ | $$h_t = (1 - u_t) \\tilde{\\mathbf{h}}_t  + u_t \\odot h_{t-1}$$|\n","\n","\n","**References**\n","\n","- Cho et al. *Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*, EMNLP 2014. [https://aclanthology.org/D14-1179/](https://aclanthology.org/D14-1179/)\n","\n","- PyTorch GRUCell documentation. [https://docs.pytorch.org/stable/generated/torch.nn.GRUCell.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n","\n","\n"],"metadata":{"id":"i41ls9kPICdy"}},{"cell_type":"code","source":["def gru(h_prev, word_idx):\n","    \"\"\"\n","    Single GRU cell with embedding lookup\n","\n","    Inputs:\n","        - h_prev: Previous hidden state (H, 1)\n","        - word_idx: Integer index of word in vocabulary\n","\n","    Returns:\n","        - ut, rt, cht: Gate activations\n","        - ht: Current hidden state (H, 1)\n","        - yt: Output logits (V, 1)\n","        - et: Word embedding (E, 1)\n","    \"\"\"\n","    et = Wemb[word_idx].reshape(-1, 1)  # (E, 1)\n","    zt = np.concatenate((h_prev, et), axis=0) #(H+E,1)\n","    # Update Gate\n","    zu = np.dot(Wu, zt) + bu #(H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n","    ut = sigmoid(zu) # (H,1)\n","    #Reset Gate\n","    zr = np.dot(Wr, zt) + br #(H,H+E)@(H+E,1)->(H,1)+(H,1)=(H,1)\n","    rt = sigmoid(zr) # (H,1)\n","    #Candidate Hidden State\n","    #Split Wh into hidden and input parts\n","    Wh_h = Wh[:, :hidden_size]  # (H,H)\n","    Wh_x = Wh[:, hidden_size:]  # (H,E)\n","    zcht = np.dot(Wh_x, et) + rt * np.dot(Wh_h, h_prev) + bh #(H,1)\n","    cht = np.tanh(zcht) # (H,1)\n","    #Hidden State\n","    ht = (1-ut) * cht + ut * h_prev # (H,1)+(H,1)=(H,1)\n","    #Output Logits\n","    yt = np.dot(Wy, ht) + by # #(V,H)@(H,1)->(V,1)+(V,1)=(V,1)\n","    return et, ut, rt, cht, ht, yt"],"metadata":{"id":"AVvRr4GJ7SmG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Understanding the GRU Cell:**\n","\n","**What's happening here?**\n","- At each timestep $t$, the GRU takes two inputs:\n","  1. Current word embedding $\\mathbf{e}_t$ (dense vector from embedding matrix)\n","  2. Previous hidden state $\\mathbf{h}_{t-1}$\n","  \n","- It outputs:\n","  1. New hidden state $\\mathbf{h}_t$ (combines short and long-term memory)\n","  2. Logits $\\mathbf{y}_t$ (output logits)\n","  3. Cache values $(\\mathbf{e}_t, \\mathbf{u}_t, \\mathbf{r}_t, \\mathbf{ch}_t)$"],"metadata":{"id":"4f5wu1Tj9rbm"}},{"cell_type":"markdown","source":["### __FORWARD PASS__\n"],"metadata":{"id":"JSiqg1u1Fcuo"}},{"cell_type":"code","source":["def forward(inputs, targets, h_prev):\n","    \"\"\"\n","    Forward pass through GRU Cell\n","\n","    Inputs:\n","        - inputs: List of word indices, e.g., [23, 145, 67] for \"the cat sat\"\n","        - targets: List of target word indices (inputs shifted by 1)\n","        - h_prev: Initial hidden state from previous sequence, shape (H, 1)\n","\n","    Returns:\n","        - et: Dict of embeddings {0: e_0, 1: e_1, ...}\n","        - ht: Dict of hidden states {-1: h_init, 0: h_0, 1: h_1, ...}\n","        - probt: Dict of probability distributions {0: p_0, 1: p_1, ...}\n","        - loss: Total cross-entropy loss across all timesteps (scalar)\n","    \"\"\"\n","    # Initialize storage dictionaries\n","    et = {}  # Store  word embeddings\n","    ut = {}  # Store update gate values\n","    rt = {}  # Store reset gate values\n","    cht = {}  # Store chs values\n","    ht = {}  # Store hidden states\n","    yt = {}  # Store output logits\n","    probt = {}  # Store probability distributions (after softmax)\n","\n","    # Set initial hidden state\n","    ht[-1] = np.copy(h_prev) #(H,1)\n","    loss = 0\n","    # Loop through each timestep in the sequence\n","    for t in range(len(inputs)):\n","        # Step 1: Get word embedding from embedding matrix\n","        word_idx = inputs[t]\n","        # Step 2: Run GRU cell (forward computation)\n","        et[t], ut[t], rt[t], cht[t], ht[t], yt[t] = gru(ht[t-1], word_idx)\n","        # Step 3: Apply Softmax\n","        probt[t] = softmax(yt[t])\n","        # Step 4: Compute loss for this timestep\n","        # Cross-entropy: -log(probability of correct next word)\n","        loss += -np.log(probt[t][targets[t], 0]+ epsilon)\n","\n","    cache = (h_prev, ut, rt, cht)\n","    return et, ht, probt, loss, cache"],"metadata":{"id":"ErpAgrEEEpld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __BACKWARD PASS (BPTT)__\n","**Complete GRU Backpropagation Through Time Equations**\n","\n","#### **Step 1: Output Layer Gradient (Softmax + Cross-Entropy)**\n","\n","$$\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} = \\mathbf{p}_t - \\mathbf{1}_{y^*_t}$$\n","\n","Where $\\mathbf{p}_t$ is the predicted probability distribution and $\\mathbf{1}_{y^*_t}$ is the one-hot vector of the true label.\n","\n","**Output layer weight gradients:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\mathbf{h}_t^T$$\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_y} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t}$$\n","\n","#### **Step 2: Hidden State Gradient**\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} = \\mathbf{W}_y^T \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t+1}}$$\n","\n","The gradient flows from two sources:\n","- Current timestep's output loss (first term)\n","- Future timestep's hidden state (second term, initialized as zeros for the last timestep)\n","\n","#### **Step 3: Update Gate Gradients**\n","\n","Recall: $\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\tilde{\\mathbf{h}}_t + \\mathbf{u}_t \\odot \\mathbf{h}_{t-1}$\n","\n","**Gradient w.r.t. update gate (after sigmoid):**\n","\n","Taking the derivative:\n","$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{u}_t} = -\\tilde{\\mathbf{h}}_t + \\mathbf{h}_{t-1} = \\mathbf{h}_{t-1} - \\tilde{\\mathbf{h}}_t$$\n","\n","Therefore:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (\\mathbf{h}_{t-1} - \\tilde{\\mathbf{h}}_t)$$\n","\n","**Gradient w.r.t. update gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}_t} \\odot \\mathbf{u}_t \\odot (1 - \\mathbf{u}_t)$$\n","\n","(Using sigmoid derivative: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$)\n","\n","#### **Step 4: Candidate Hidden State Gradients**\n","\n","Recall: $\\mathbf{h}_t = (1 - \\mathbf{u}_t) \\odot \\tilde{\\mathbf{h}}_t + \\mathbf{u}_t \\odot \\mathbf{h}_{t-1}$ **(PyTorch style)**\n","\n","**Gradient w.r.t. candidate hidden state (after tanh):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{h}}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot (1 - \\mathbf{u}_t)$$\n","\n","**Key Insight:** The coefficient of $\\tilde{\\mathbf{h}}_t$ in the hidden state equation is $(1 - \\mathbf{u}_t)$.\n","\n","**Gradient w.r.t. candidate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{\\mathbf{h}}_t} \\odot (1 - \\tilde{\\mathbf{h}}_t^2)$$\n","\n","(Using tanh derivative: $\\tanh'(z) = 1 - \\tanh^2(z)$)\n","\n","#### **Step 5: Reset Gate Gradients**\n","\n","Recall the **candidate computation** with split matrices:\n","$$\\mathbf{z}_h = \\mathbf{W}_{h,x} \\mathbf{e}_t + \\mathbf{r}_t \\odot (\\mathbf{W}_{h,h} \\mathbf{h}_{t-1}) + \\mathbf{b}_h$$\n","\n","Where $\\mathbf{W}_{h,h}$ are the first $H$ columns of $\\mathbf{W}_h$ (hidden part) and $\\mathbf{W}_{h,x}$ are the last $E$ columns (input part).\n","\n","**Gradient w.r.t. reset gate (after sigmoid):**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{r}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\odot (\\mathbf{W}_{h,h} \\mathbf{h}_{t-1})$$\n","\n","**Key Insight:** The reset gate multiplies the hidden contribution $\\mathbf{W}_{h,h} \\mathbf{h}_{t-1}$, so its gradient is the element-wise product with this term.\n","\n","**Gradient w.r.t. reset gate pre-activation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{r}_t} \\odot \\mathbf{r}_t \\odot (1 - \\mathbf{r}_t)$$\n","\n","#### **Step 6: Weight Matrix Gradients**\n","\n","**Update Gate:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_u} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u} [\\mathbf{h}_{t-1}; \\mathbf{e}_t]^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_u} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}$$\n","\n","**Reset Gate:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_r} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r} [\\mathbf{h}_{t-1}; \\mathbf{e}_t]^T$$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_r} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}$$\n","\n","**Candidate Hidden State (Split Computation):**\n","\n","For the hidden part $\\mathbf{W}_{h,h}$:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{h,h}} = \\sum_{t=0}^{T-1} (\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\odot \\mathbf{r}_t) \\mathbf{h}_{t-1}^T$$\n","\n","For the input part $\\mathbf{W}_{h,x}$:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{h,x}} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\mathbf{e}_t^T$$\n","\n","Bias gradient:\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_h} = \\sum_{t=0}^{T-1} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}$$\n","\n","**Embedding Matrix:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{emb}[i]} = \\sum_{t: w_t=i} \\left[\\mathbf{W}_u^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}\\right]_{[H:]} + \\left[\\mathbf{W}_r^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}\\right]_{[H:]} + \\mathbf{W}_{h,x}^T\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h}$$\n","\n","#### **Step 7: Gradient to Previous Hidden State**\n","\n","The gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}$ flows through **four pathways**:\n","\n","**1. Through Update Gate Input:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(u)} = \\left[\\mathbf{W}_u^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_u}\\right]_{[0:H]}$$\n","\n","**2. Through Reset Gate Input:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(r)} = \\left[\\mathbf{W}_r^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_r}\\right]_{[0:H]}$$\n","\n","**3. Through Candidate Computation:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(h)} = \\mathbf{W}_{h,h}^T \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}_h} \\odot \\mathbf{r}_t\\right)$$\n","\n","**Key Insight:** The reset gate is applied **after** the matrix multiplication $\\mathbf{W}_{h,h} \\mathbf{h}_{t-1}$, so gradient flows through $\\mathbf{W}_{h,h}^T$ with element-wise multiplication by $\\mathbf{r}_t$.\n","\n","**4. Direct path through hidden state update:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(\\text{direct})} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\odot \\mathbf{u}_t$$\n","\n","**Note:** When $\\mathbf{u}_t \\approx 1$, the old hidden state is preserved (coefficient is $\\mathbf{u}_t$).\n","\n","**Total gradient to previous hidden state:**\n","$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(u)} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(r)} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(h)} + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}}^{(\\text{direct})}$$\n","\n","**Key Insight:** The direct pathway $\\mathbf{u}_t \\odot \\mathbf{h}_{t-1}$ allows gradients to flow unimpeded when the update gate is open ($\\mathbf{u}_t \\approx 1$), enabling long-term memory and mitigating vanishing gradients!\n","\n","#### **Step 8: Gradient Clipping**\n","\n","To prevent exploding gradients, clip all parameter gradients:\n","\n","$$\\text{clip}(\\nabla \\theta, -\\tau, \\tau)$$\n","\n","Common clipping threshold: $\\tau = 5$\n","\n","**Notation:**\n","- $\\odot$ = element-wise multiplication\n","- $\\sigma$ = sigmoid function\n","- $T$ = sequence length\n","- $H$ = hidden dimension\n","- $E$ = embedding dimension\n","- $V$ = vocabulary size\n","- $\\mathbf{W}_{h,h}$ = hidden part of $\\mathbf{W}_h$ (first $H$ columns)\n","- $\\mathbf{W}_{h,x}$ = input part of $\\mathbf{W}_h$ (last $E$ columns)\n","- $[\\cdot; \\cdot]$ = concatenation along dimension 0"],"metadata":{"id":"DjyNlnLGZPlJ"}},{"cell_type":"code","source":["def backward(inputs, targets, et, ht, probt, cache):\n","    \"\"\"\n","    Backpropagation Through Time (BPTT) for GRU\n","\n","    Inputs:\n","        - inputs: List of input word indices\n","        - targets: List of target word indices\n","        - et: Dict of word embeddings from forward pass\n","        - ht: Dict of hidden states from forward pass\n","        - probt: Dict of probability distributions from forward pass\n","        - cache: Tuple (h_prev, ut, rt, cht) from forward pass\n","\n","    Returns:\n","        - dWemb: Gradient w.r.t. embedding matrix\n","        - dWu, dWr, dWh: Gradients for gate weight matrices\n","        - dWy: Gradient w.r.t. hidden-to-output weights\n","        - dbu, dbr, dbh: Gradients for gate biases\n","        - dby: Gradient w.r.t. output bias\n","    \"\"\"\n","    h_prev, ut, rt, cht = cache\n","    dWemb = np.zeros_like(Wemb)  # (V, E)\n","    dWu = np.zeros_like(Wu)  # (H, H+E)\n","    dWr = np.zeros_like(Wr)  # (H, H+E)\n","    dWh = np.zeros_like(Wh)  # (H, H+E)\n","    dWy = np.zeros_like(Wy)  # (V, H)\n","\n","    dbu = np.zeros_like(bu)  # (H, 1)\n","    dbr = np.zeros_like(br)  # (H, 1)\n","    dbh = np.zeros_like(bh)  # (H, 1)\n","    dby = np.zeros_like(by)  # (V, 1)\n","\n","    # Gradient of hidden state at next timestep (initially zero)\n","    dh_next = np.zeros_like(ht[0])  # (H, 1)\n","\n","    # Backpropagate through time (from last to first timestep)\n","    for t in reversed(range(len(inputs))):\n","        # Step 1: Gradient of loss w.r.t output probabilities\n","        dy = np.copy(probt[t])  # (V, 1)\n","        dy[targets[t]] -= 1  # Subtract 1 from correct class (cross-entropy gradient)\n","        # Gradients for output layer\n","        dWy += np.dot(dy, ht[t].T)  # (V,1)@(1,H) = (V,H)\n","        dby += dy  # (V, 1)\n","\n","        # Step 2: Gradient w.r.t hidden state\n","        # Comes from two sources: current output and next timestep\n","        dh = np.dot(Wy.T, dy) + dh_next  # (H,V)@(V,1) + (H,1) = (H,1)\n","\n","        # Step 3: Update Gate Gradients\n","        du = dh * (ht[t-1] - cht[t])  # (H,1)\n","        dzu = du * ut[t] * (1 - ut[t])\n","\n","        # Gradient for Wu and bu\n","        zt = np.concatenate((ht[t-1], et[t]), axis=0)  # (H+E, 1)\n","        dWu += np.dot(dzu, zt.T)  # (H,1)@(1,H+E) = (H,H+E)\n","        dbu += dzu  # (H,1)\n","\n","        # Step 4: Gradient w.r.t candidate hidden state\n","        dcht = dh * (1 - ut[t])  # (H,1) * (H,1) = (H,1)\n","        dzh = dcht * (1 - cht[t]**2)  # (H,1)\n","\n","        # Step 5: Gradient w.r.t reset gate\n","        # Split Wh into hidden and input parts\n","        Wh_h = Wh[:, :hidden_size]  # (H, H)\n","        Wh_x = Wh[:, hidden_size:]  # (H, E)\n","        dr = dzh * np.dot(Wh_h, ht[t-1])  # (H,1) * (H,1) = (H,1)\n","        dzr = dr * rt[t] * (1 - rt[t]) # (H,1)\n","\n","        # Gradient for Wr and br\n","        dWr += np.dot(dzr, zt.T)  # (H,1)@(1,H+E) = (H,H+E)\n","        dbr += dzr  # (H,1)\n","\n","        # Step 6: Weight Matrix Wh Gradients\n","        dWh_h = np.dot(dzh * rt[t], ht[t-1].T)  # (H,1) @ (1,H) = (H,H)\n","        dWh_x = np.dot(dzh, et[t].T)  # (H,1) @ (1,E) = (H,E)\n","        # Combine into full Wh gradient\n","        dWh[:, :hidden_size] += dWh_h  # Update hidden part\n","        dWh[:, hidden_size:] += dWh_x  # Update input part\n","        dbh += dzh  # (H,1)\n","\n","        # Step 7: Embedding Gradients\n","        # Gradients flow to embedding from three gates\n","        # From update gate\n","        de_from_zu = np.dot(Wu.T, dzu)[hidden_size:, :]  # (H+E,H)@(H,1)->(E,1)\n","        # From reset gate\n","        de_from_zr = np.dot(Wr.T, dzr)[hidden_size:, :]  # (E,1)\n","        # From candidate (through Wh_x)\n","        de_from_zh = np.dot(Wh_x.T, dzh)  # (E,H)@(H,1) = (E,1)\n","        # Total embedding gradient\n","        de = de_from_zu + de_from_zr + de_from_zh  # (E,1)\n","        # Accumulate gradient for this word's embedding\n","        word_idx = inputs[t]\n","        dWemb[word_idx] += de.ravel()\n","\n","        # Step 8: Gradient to Previous Hidden State\n","        # Four pathways:\n","        # (1) Through update gate\n","        dh_from_zu = np.dot(Wu.T, dzu)[:hidden_size, :]  # (H,1)\n","        # (2) Through reset gate\n","        dh_from_zr = np.dot(Wr.T, dzr)[:hidden_size, :]  # (H,1)\n","        # (3) Through candidate (rt * Wh_h @ h_prev)\n","        dh_from_zh = np.dot(Wh_h.T, dzh * rt[t])  # (H,H)@(H,1) = (H,1)\n","        # (4) Direct path (ht = ... + ut * h_prev)\n","        dh_from_ht = dh * ut[t]  # (H,1)\n","        # Total gradient to previous hidden state\n","        dh_next = dh_from_zu + dh_from_zr + dh_from_zh + dh_from_ht  # (H,1)\n","\n","    for dparam in [dWemb, dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby]:\n","        np.clip(dparam, -5, 5, out=dparam)\n","\n","    return dWemb, dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby"],"metadata":{"id":"07BaraxIyIst"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __UPDATE PARAMS WITH ADAM__"],"metadata":{"id":"whAHx2WTW03k"}},{"cell_type":"code","source":["def update_parameters(dWemb, dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby, learning_rate):\n","    \"\"\"\n","    Update GRU model parameters using Adam optimizer with bias correction\n","\n","    Inputs:\n","        - dWemb: Gradient for embedding matrix (V, E)\n","        - dWu: Gradient for Update gate weights (H, H+E)\n","        - dWr: Gradient for Reset gate weights (H, H+E)\n","        - dWh: Gradient for cell Hidden weights (H, H+E)\n","        - dWy: Gradient for prediction weights (V, H)\n","        - dbu, dbr, dbh: Gradients for gate biases (H, 1)\n","        - dby: Gradient for output bias (V, 1)\n","        - learning_rate: Step size for parameter updates\n","\n","    Returns:\n","        - None (updates global parameters in-place)\n","    \"\"\"\n","    global Wemb, Wu, Wr, Wh, Wy, bu, br, bh, by\n","    global mWemb, mWu, mWr, mWh, mWy, mbu, mbr, mbh, mby\n","    global vWemb, vWu, vWr, vWh, vWy, vbu, vbr, vbh, vby\n","    global t_adam\n","\n","    t_adam += 1\n","    params = [\n","        (Wemb, dWemb, mWemb, vWemb),\n","        (Wu, dWu, mWu, vWu),\n","        (Wr, dWr, mWr, vWr),\n","        (Wh, dWh, mWh, vWh),\n","        (Wy, dWy, mWy, vWy),\n","        (bu, dbu, mbu, vbu),\n","        (br, dbr, mbr, vbr),\n","        (bh, dbh, mbh, vbh),\n","        (by, dby, mby, vby)\n","    ]\n","    updated = []\n","    for param, grad, m, v in params:\n","        m = beta1 * m + (1 - beta1) * grad\n","        v = beta2 * v + (1 - beta2) * (grad ** 2)\n","        # Bias correction\n","        m_corrected = m / (1 - beta1 ** t_adam)\n","        v_corrected = v / (1 - beta2 ** t_adam)\n","        param = param - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n","        updated.append((param, m, v))\n","\n","    (Wemb, mWemb, vWemb), (Wu, mWu, vWu), (Wr, mWr, vWr), (Wh, mWh, vWh), (Wy, mWy, vWy),\\\n","    (bu, mbu, vbu), (br, mbr, vbr), (bh, mbh, vbh), (by, mby, vby) = updated"],"metadata":{"id":"wSGIUqZsMU0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __TRAIN MODEL__\n"],"metadata":{"id":"hpHLQqh7XFil"}},{"cell_type":"code","source":["def train(words_list, num_iterations=1000, print_every=100, sample_every=100):\n","    \"\"\"\n","    Train GRU language model\n","\n","    Inputs:\n","        - words_list: List of words for training\n","        - num_iterations: Number of training iterations (not full data passes)\n","        - print_every: Print loss every N iterations\n","        - sample_every: Generate sample text every N iterations\n","\n","    Returns:\n","        - smooth_loss: Exponentially smoothed loss value\n","\n","    Note: Hidden state h_prev is maintained across sequences for temporal\n","        continuity. Only reset when reaching end of data or on first iteration.\n","    \"\"\"\n","    smooth_loss = -np.log(1.0 / vocab_size) * seq_len\n","    n = 0  # Iterations counter\n","    p = 0  # Data pointer (position in word list)\n","    h_prev = np.zeros((hidden_size, 1))\n","\n","    while n < num_iterations:\n","        # Reset pointer and hidden state at end of data or first iteration\n","        if p + seq_len + 1 >= len(words_list) or n == 0:\n","            h_prev = np.zeros((hidden_size, 1))  # Fresh start\n","            p = 0  # Go back to beginning\n","\n","        # Input:  words at positions [p, p+1, ..., p+seq_len-1]\n","        # Target: words at positions [p+1, p+2, ..., p+seq_len]\n","        inputs = [word_to_ix[w] for w in words_list[p:p+seq_len]]\n","        targets = [word_to_ix[w] for w in words_list[p+1:p+seq_len+1]]\n","\n","        # Forward pass\n","        et, ht, probt, loss, cache = forward(inputs, targets, h_prev)\n","        # Update hidden state for next sequence\n","        h_prev = np.copy(ht[len(inputs) - 1])\n","        # Backward pass\n","        dWemb, dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby = backward(inputs, targets, et, ht, probt, cache)\n","        # Update parameters\n","        update_parameters(dWemb, dWu, dWr, dWh, dWy, dbu, dbr, dbh, dby, lr)\n","        # Update smooth loss\n","        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","        # Print progress\n","        if n % print_every == 0:\n","            print(f\"Iter {n:6d} | Loss: {smooth_loss:.4f}\")\n","        # Generate sample text\n","        if n % sample_every == 0 and n > 0:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"SAMPLE at iteration {n}:\")\n","            print(f\"{'='*60}\")\n","            sample_text = sample(words_list[0], n_words=50)\n","            print(sample_text)\n","            print(f\"{'='*60}\\n\")\n","        p += seq_len\n","        n += 1\n","\n","    print(\"\\nTraining complete!\")\n","    return smooth_loss"],"metadata":{"id":"HvrBIPp3XAt-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __SAMPLING FUNCTION (GENERATE TEXT)__"],"metadata":{"id":"j4GOFoOuYzvD"}},{"cell_type":"code","source":["def sample(seed_word=None, n_words=50, use_sos=True, stop_at_eos=True):\n","    \"\"\"\n","    Generate text by sampling from the trained word-level GRU language model\n","\n","    Inputs:\n","        - seed_word: Starting word for text generation (if None, uses <SOS>)\n","        - n_words: Maximum number of words to generate (default 50)\n","        - use_sos: If True and seed_word is None, start with <SOS> token\n","        - stop_at_eos: If True, stop generation when <EOS> token is sampled\n","\n","    Returns:\n","        - String of generated text\n","    \"\"\"\n","    # Initialize with seed word or SOS token\n","    if seed_word is None:\n","        if use_sos:\n","            word_idx = word_to_ix[SOS_TOKEN]\n","            generated_words = []  # Don't include SOS in output\n","        else:\n","            # Start with first regular word (after special tokens)\n","            word_idx = word_to_ix[vocab[3]]\n","            generated_words = [ix_to_word[word_idx]]\n","    else:\n","        # Handle seed word not in vocabulary\n","        word_idx = word_to_ix.get(seed_word.lower(), word_to_ix[UNK_TOKEN])\n","        if word_idx == word_to_ix[UNK_TOKEN]:\n","            print(f\"Warning: '{seed_word}' not in vocab. Using <UNK> token.\")\n","        generated_words = [ix_to_word[word_idx]]\n","\n","    h = np.zeros((hidden_size, 1))  # Initialize hidden state\n","\n","    for _ in range(n_words):\n","        _, _, _, _, h, y = gru(h, word_idx)\n","        p = softmax(y)\n","        word_idx = np.random.choice(range(vocab_size), p=p.ravel())\n","        word = ix_to_word[word_idx]\n","        if stop_at_eos and word == EOS_TOKEN:\n","            break\n","        # Skip adding special tokens to output\n","        if word not in [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]:\n","            generated_words.append(word)\n","\n","    return ' '.join(generated_words)"],"metadata":{"id":"ldl7q4JsYWJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __RUN TRAINING__"],"metadata":{"id":"1y0M3vMIZcjn"}},{"cell_type":"code","source":["final_loss = train(words, num_iterations=10000, print_every=500, sample_every=1000)\n","print(f\"\\nFinal smooth loss: {final_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjfsldMiZVna","outputId":"e12b3224-0059-4676-9def-c69a01bd41fd","executionInfo":{"status":"ok","timestamp":1763544937472,"user_tz":-300,"elapsed":131703,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter      0 | Loss: 112.4952\n","Iter    500 | Loss: 94.9219\n","Iter   1000 | Loss: 62.7837\n","\n","============================================================\n","SAMPLE at iteration 1000:\n","============================================================\n","once upon a moment crow was feeling started tired and weak . after flying for a long time , the crow thirsty , but his got stones and looked inside . there was a little water at the bottom , slowly a moment a moment . then he got an idea\n","============================================================\n","\n","Iter   1500 | Loss: 39.4343\n","Iter   2000 | Loss: 24.4665\n","\n","============================================================\n","SAMPLE at iteration 2000:\n","============================================================\n","once upon a time , on , the water began to rise . the crow continued dropping stones until the water came a little water at the bottom , the water began to rise . but his could not reach it . the crow thought for a long time , the\n","============================================================\n","\n","Iter   2500 | Loss: 15.1062\n","Iter   3000 | Loss: 9.3178\n","\n","============================================================\n","SAMPLE at iteration 3000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   3500 | Loss: 5.7515\n","Iter   4000 | Loss: 3.5550\n","\n","============================================================\n","SAMPLE at iteration 4000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   4500 | Loss: 2.2017\n","Iter   5000 | Loss: 1.3670\n","\n","============================================================\n","SAMPLE at iteration 5000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   5500 | Loss: 0.8516\n","Iter   6000 | Loss: 0.5328\n","\n","============================================================\n","SAMPLE at iteration 6000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a was lying under a tree water inside . tree .\n","============================================================\n","\n","Iter   6500 | Loss: 0.3350\n","Iter   7000 | Loss: 0.2120\n","\n","============================================================\n","SAMPLE at iteration 7000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   7500 | Loss: 0.1350\n","Iter   8000 | Loss: 0.0868\n","\n","============================================================\n","SAMPLE at iteration 8000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   8500 | Loss: 0.0563\n","Iter   9000 | Loss: 0.0370\n","\n","============================================================\n","SAMPLE at iteration 9000:\n","============================================================\n","once upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","============================================================\n","\n","Iter   9500 | Loss: 0.0246\n","\n","Training complete!\n","\n","Final smooth loss: 0.0166\n"]}]},{"cell_type":"markdown","source":["### __TEST DIFFERENT SEEDS__"],"metadata":{"id":"jhx3NWCEtl0J"}},{"cell_type":"code","source":["seed_words = ['the', 'and', 'he', 'was', 'i']\n","for word in seed_words:\n","    generated = sample(word, n_words=50)\n","    print(word, ':', generated)\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_1JWyvyZi7B","outputId":"9f3a1491-3f36-46ff-9bf1-1960ad982c7e","executionInfo":{"status":"ok","timestamp":1763544937505,"user_tz":-300,"elapsed":28,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the : the up small pebbles by water started picking up small pebbles one by one upon upon a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a\n","------------------------------------------------------------\n","and : and upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying\n","------------------------------------------------------------\n","he : he up small pebbles into into the water began to rise . the crow continued dropping stones until the water came a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow was feeling tired and weak\n","------------------------------------------------------------\n","was : was small water pitcher one and the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor crow\n","------------------------------------------------------------\n","Warning: 'i' not in vocab. Using <UNK> token.\n","i : <UNK> small , the crow continued dropping stones until the water came , but his beak could not reach it . the crow thought for a moment . then he got an idea ! he started picking up small pebbles one by one and dropped them into the pitcher . slowly\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Test with SOS token (no seed word)\n","for i in range(3):\n","    generated = sample(seed_word=None, n_words=30, use_sos=True, stop_at_eos=True)\n","    print(f\"Sample {i+1}: {generated}\")\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G26qew26bnpk","executionInfo":{"status":"ok","timestamp":1763544937532,"user_tz":-300,"elapsed":22,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"13db3b93-7826-493e-eaff-8f455d379f2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample 1: by upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor\n","------------------------------------------------------------\n","Sample 2: it upon a time , on a very hot day , a thirsty crow was flying in search of water . the sun was shining brightly , and the poor\n","------------------------------------------------------------\n","Sample 3: crow up small pebbles one upon the water came feeling tired and weak . after flying for a long time , the crow finally saw a pitcher lying under a\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Test with unknown word\n","generated = sample('elephant', n_words=30, stop_at_eos=True)\n","print(f\"elephant: {generated}\")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MogJp0OboVM","executionInfo":{"status":"ok","timestamp":1763544937559,"user_tz":-300,"elapsed":24,"user":{"displayName":"Ahmad Raza","userId":"04590257032587795730"}},"outputId":"7043dd51-6f30-43a6-ae3d-9512c2145b54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: 'elephant' not in vocab. Using <UNK> token.\n","elephant: <UNK> at he up small pebbles by into into the pitcher . slowly , the water began to rise . the crow continued dropping stones until the water came a very\n","============================================================\n"]}]},{"cell_type":"markdown","source":["## **SUMMARY**\n","\n","\n","### **Further Reading:**\n","1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) — Olah (2015) - Visual explanations (covers GRU too)\n","2. [Empirical Evaluation of Gated RNNs](https://arxiv.org/abs/1412.3555) — Chung et al. (2014) - GRU vs LSTM comparison\n","3. [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) — Karpathy (2015) - Character-level modeling\n","4. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) — Vaswani et al. (2017) - Transformers replacing RNNs\n","5. Deep Learning Book, Chapter 10 — Goodfellow et al. (2016) - Comprehensive RNN theory\n","6. [GRU vs LSTM: A Comparison](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) — Practical guide with visualizations\n"],"metadata":{"id":"HhZHDwtPMo4E"}}]}